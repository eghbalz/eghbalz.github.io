<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    
        <script>
            L_NO_TOUCH = false;
            L_DISABLE_3D = false;
        </script>
    
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.6.0/dist/leaflet.js"></script>
    <script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.6.0/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
            <meta name="viewport" content="width=device-width,
                initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
            <style>
                #map_acddb7aff0574da8923bcc405bcf02f6 {
                    position: relative;
                    width: 100.0%;
                    height: 100.0%;
                    left: 0.0%;
                    top: 0.0%;
                }
            </style>
        
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/leaflet.markercluster.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.Default.css"/>
</head>
<body>    
    
            <div class="folium-map" id="map_acddb7aff0574da8923bcc405bcf02f6" ></div>
        
</body>
<script>    
    
            var map_acddb7aff0574da8923bcc405bcf02f6 = L.map(
                "map_acddb7aff0574da8923bcc405bcf02f6",
                {
                    center: [0, 0],
                    crs: L.CRS.EPSG3857,
                    zoom: 1,
                    zoomControl: true,
                    preferCanvas: false,
                }
            );

            

        
    
            var tile_layer_7525adad272146e5b88812da97ba198b = L.tileLayer(
                "https://cartodb-basemaps-{s}.global.ssl.fastly.net/light_all/{z}/{x}/{y}.png",
                {"attribution": "\u0026copy; \u003ca href=\"http://www.openstreetmap.org/copyright\"\u003eOpenStreetMap\u003c/a\u003e contributors \u0026copy; \u003ca href=\"http://cartodb.com/attributions\"\u003eCartoDB\u003c/a\u003e, CartoDB \u003ca href =\"http://cartodb.com/attributions\"\u003eattributions\u003c/a\u003e", "detectRetina": false, "maxNativeZoom": 18, "maxZoom": 18, "minZoom": 0, "noWrap": false, "opacity": 1, "subdomains": "abc", "tms": false}
            ).addTo(map_acddb7aff0574da8923bcc405bcf02f6);
        
    
            var marker_cluster_fed02479aba5438f9de49785c4283dfe = L.markerClusterGroup(
                {}
            );
            map_acddb7aff0574da8923bcc405bcf02f6.addLayer(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
            var circle_marker_c0f3cad64b83420582aea1d3e620f183 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ec74ee4ab5524693bda4acc6a5085ae9 = L.popup({"maxWidth": "100%"});

        
            var html_5e635b92da114d2b8cf4425d3681fdd6 = $(`<div id="html_5e635b92da114d2b8cf4425d3681fdd6" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/2008.03350">A Joint Framework for Audio Tagging and Weakly Supervised Acoustic Event Detection Using DenseNet with Global Average Pooling</a><br></div>`)[0];
            popup_ec74ee4ab5524693bda4acc6a5085ae9.setContent(html_5e635b92da114d2b8cf4425d3681fdd6);
        

        circle_marker_c0f3cad64b83420582aea1d3e620f183.bindPopup(popup_ec74ee4ab5524693bda4acc6a5085ae9)
        ;

        
    
    
            var circle_marker_d384ecb64236433b95d0c1d0ff2aca3c = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_bc0377d5e35e49d78ca189513fa437e7 = L.popup({"maxWidth": "100%"});

        
            var html_2324808bb4724b1d8960329e59b2e7cb = $(`<div id="html_2324808bb4724b1d8960329e59b2e7cb" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1903.07714">A RAD approach to deep mixture models</a><br></div>`)[0];
            popup_bc0377d5e35e49d78ca189513fa437e7.setContent(html_2324808bb4724b1d8960329e59b2e7cb);
        

        circle_marker_d384ecb64236433b95d0c1d0ff2aca3c.bindPopup(popup_bc0377d5e35e49d78ca189513fa437e7)
        ;

        
    
    
            var circle_marker_16d0ebf6de3242a589eb2cd93b122528 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ccb231412c824fc8b4790c485c82f8d1 = L.popup({"maxWidth": "100%"});

        
            var html_96a9f7b8d1394ac18548c13e5812b197 = $(`<div id="html_96a9f7b8d1394ac18548c13e5812b197" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7952131/">A comparison of deep learning methods for environmental sound detection</a><br></div>`)[0];
            popup_ccb231412c824fc8b4790c485c82f8d1.setContent(html_96a9f7b8d1394ac18548c13e5812b197);
        

        circle_marker_16d0ebf6de3242a589eb2cd93b122528.bindPopup(popup_ccb231412c824fc8b4790c485c82f8d1)
        ;

        
    
    
            var circle_marker_3751a935ae594e7a8da0da4db86d2be7 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_59a79d1ff92a449bb5eb4922efe1d21c = L.popup({"maxWidth": "100%"});

        
            var html_f76362daae4543c792f72a68df0f9b5c = $(`<div id="html_f76362daae4543c792f72a68df0f9b5c" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8883750/">A low-cost driver and passenger activity detection system based on deep learning and multiple sensor fusion</a><br></div>`)[0];
            popup_59a79d1ff92a449bb5eb4922efe1d21c.setContent(html_f76362daae4543c792f72a68df0f9b5c);
        

        circle_marker_3751a935ae594e7a8da0da4db86d2be7.bindPopup(popup_59a79d1ff92a449bb5eb4922efe1d21c)
        ;

        
    
    
            var circle_marker_310d2f2d05a74ea5ae0b9671a68dea6b = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f675ddec65b34a488cf1c38a573617d7 = L.popup({"maxWidth": "100%"});

        
            var html_11e35e39a5ce49b9b3dbd6434930a591 = $(`<div id="html_11e35e39a5ce49b9b3dbd6434930a591" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://search.ieice.org/bin/summary.php?id=e100-d_12_3041">A novel discriminative feature extraction for acoustic scene classification using rnn based source separation</a><br></div>`)[0];
            popup_f675ddec65b34a488cf1c38a573617d7.setContent(html_11e35e39a5ce49b9b3dbd6434930a591);
        

        circle_marker_310d2f2d05a74ea5ae0b9671a68dea6b.bindPopup(popup_f675ddec65b34a488cf1c38a573617d7)
        ;

        
    
    
            var circle_marker_6608b4845f5a4fdd8496c671ad8d3efe = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_61293a28c1ea424cbf3938d3363edbb8 = L.popup({"maxWidth": "100%"});

        
            var html_6531cbe9e30a4c4eb0aa11702a64317b = $(`<div id="html_6531cbe9e30a4c4eb0aa11702a64317b" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Zhao_161.pdf">ADSC submission for DCASE 2017: Acoustic scene classification using deep residual convolutional neural networks</a><br></div>`)[0];
            popup_61293a28c1ea424cbf3938d3363edbb8.setContent(html_6531cbe9e30a4c4eb0aa11702a64317b);
        

        circle_marker_6608b4845f5a4fdd8496c671ad8d3efe.bindPopup(popup_61293a28c1ea424cbf3938d3363edbb8)
        ;

        
    
    
            var circle_marker_983b8c9e531d40bdb87bc98dd59652b8 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_96fa494dc90a49e68654c0855e352b56 = L.popup({"maxWidth": "100%"});

        
            var html_363a8bce2f2541b29d8b534f242484c8 = $(`<div id="html_363a8bce2f2541b29d8b534f242484c8" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1811.06669">Aclnet: efficient end-to-end audio classification cnn</a><br></div>`)[0];
            popup_96fa494dc90a49e68654c0855e352b56.setContent(html_363a8bce2f2541b29d8b534f242484c8);
        

        circle_marker_983b8c9e531d40bdb87bc98dd59652b8.bindPopup(popup_96fa494dc90a49e68654c0855e352b56)
        ;

        
    
    
            var circle_marker_9b48d70c370e4910af69e966f01a6723 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d8bf16f581894867b517109db30d3db8 = L.popup({"maxWidth": "100%"});

        
            var html_ec9be4928615467e9df520ed07320c0e = $(`<div id="html_ec9be4928615467e9df520ed07320c0e" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.researchgate.net/profile/Rakib_Hyder/publication/319185344_Acoustic_Scene_Classification_Using_a_CNN-SuperVector_System_Trained_with_Auditory_and_Spectrogram_Image_Features/links/5a0210e84585155c96cb478d/Acoustic-Scene-Classification-Using-a-CNN-SuperVector-System-Trained-with-Auditory-and-Spectrogram-Image-Features.pdf">Acoustic Scene Classification Using a CNN-SuperVector System Trained with Auditory and Spectrogram Image Features.</a><br></div>`)[0];
            popup_d8bf16f581894867b517109db30d3db8.setContent(html_ec9be4928615467e9df520ed07320c0e);
        

        circle_marker_9b48d70c370e4910af69e966f01a6723.bindPopup(popup_d8bf16f581894867b517109db30d3db8)
        ;

        
    
    
            var circle_marker_6160fc0656fe4f2f9d742d097aece007 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f825eeb7994b4decb8d0b513c31c0be9 = L.popup({"maxWidth": "100%"});

        
            var html_13a5e6ab1383463ba9c5e1af1274fec7 = $(`<div id="html_13a5e6ab1383463ba9c5e1af1274fec7" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8897625/">Adaptive multi-scale detection of acoustic events</a><br></div>`)[0];
            popup_f825eeb7994b4decb8d0b513c31c0be9.setContent(html_13a5e6ab1383463ba9c5e1af1274fec7);
        

        circle_marker_6160fc0656fe4f2f9d742d097aece007.bindPopup(popup_f825eeb7994b4decb8d0b513c31c0be9)
        ;

        
    
    
            var circle_marker_1cbe2217456942eca826d5cc65900caa = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9d71ead48c294f4cae8ed1896f05d274 = L.popup({"maxWidth": "100%"});

        
            var html_4a42b3cf9b2d41c7b4d0823a90ec5de9 = $(`<div id="html_4a42b3cf9b2d41c7b4d0823a90ec5de9" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3331184.3331234">Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continuation</a><br></div>`)[0];
            popup_9d71ead48c294f4cae8ed1896f05d274.setContent(html_4a42b3cf9b2d41c7b4d0823a90ec5de9);
        

        circle_marker_1cbe2217456942eca826d5cc65900caa.bindPopup(popup_9d71ead48c294f4cae8ed1896f05d274)
        ;

        
    
    
            var circle_marker_3e1c8eb1ad5846eead258245af2f197c = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f4c57b01ef354f3ea37f6833bc21346a = L.popup({"maxWidth": "100%"});

        
            var html_a731ce6a76a4453e825b28b8d5fa72ee = $(`<div id="html_a731ce6a76a4453e825b28b8d5fa72ee" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://tomcollinsresearch.net/pdf/dasEtAlSMC2018.pdf">Analyzing and Classifying Guitarists from Rock Guitar Solo Tablature</a><br></div>`)[0];
            popup_f4c57b01ef354f3ea37f6833bc21346a.setContent(html_a731ce6a76a4453e825b28b8d5fa72ee);
        

        circle_marker_3e1c8eb1ad5846eead258245af2f197c.bindPopup(popup_f4c57b01ef354f3ea37f6833bc21346a)
        ;

        
    
    
            var circle_marker_cc36c5c17b3e4db4a25ac3486cb905b9 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6ee283fb7da74ccca7f083e9efa8e68c = L.popup({"maxWidth": "100%"});

        
            var html_a497c1d987884f5cb37fecc7b4831aa1 = $(`<div id="html_a497c1d987884f5cb37fecc7b4831aa1" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0440.PDF">Attention Based CLDNNs for Short-Duration Acoustic Scene Classification.</a><br></div>`)[0];
            popup_6ee283fb7da74ccca7f083e9efa8e68c.setContent(html_a497c1d987884f5cb37fecc7b4831aa1);
        

        circle_marker_cc36c5c17b3e4db4a25ac3486cb905b9.bindPopup(popup_6ee283fb7da74ccca7f083e9efa8e68c)
        ;

        
    
    
            var circle_marker_31737e95d9c84158957c4135808f2c46 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_544a55bc3adb4fec991136376cd119ca = L.popup({"maxWidth": "100%"});

        
            var html_0f2ea2afaf664d40a4ccc05693631d20 = $(`<div id="html_0f2ea2afaf664d40a4ccc05693631d20" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1906.02975">Audio tagging with noisy labels and minimal supervision</a><br></div>`)[0];
            popup_544a55bc3adb4fec991136376cd119ca.setContent(html_0f2ea2afaf664d40a4ccc05693631d20);
        

        circle_marker_31737e95d9c84158957c4135808f2c46.bindPopup(popup_544a55bc3adb4fec991136376cd119ca)
        ;

        
    
    
            var circle_marker_8c9d25fd42304d7cb3388e0c01d6a09b = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2df8be87cd1f44e39fa81ed0db4ff518 = L.popup({"maxWidth": "100%"});

        
            var html_32fc52f53fdc4d5f92272fde748339bc = $(`<div id="html_32fc52f53fdc4d5f92272fde748339bc" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://dcase.community/documents/challenge2017/technical_reports/DCASE2017_Hasan_167.pdf">Buet Bosch consortium (B2C) acoustic scene classification systems for DCASE 2017 challenge</a><br></div>`)[0];
            popup_2df8be87cd1f44e39fa81ed0db4ff518.setContent(html_32fc52f53fdc4d5f92272fde748339bc);
        

        circle_marker_8c9d25fd42304d7cb3388e0c01d6a09b.bindPopup(popup_2df8be87cd1f44e39fa81ed0db4ff518)
        ;

        
    
    
            var circle_marker_b0717afb0ce047849ec0c03700dd4905 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_db0eef37925a4312ad442e4e944cd21c = L.popup({"maxWidth": "100%"});

        
            var html_979e3813b04e493797466837478f1ab0 = $(`<div id="html_979e3813b04e493797466837478f1ab0" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://jit.ndhu.edu.tw/article/view/2329">ChoseAmobile: A Web-based Recommendation System for Mobile Phone Products</a><br></div>`)[0];
            popup_db0eef37925a4312ad442e4e944cd21c.setContent(html_979e3813b04e493797466837478f1ab0);
        

        circle_marker_b0717afb0ce047849ec0c03700dd4905.bindPopup(popup_db0eef37925a4312ad442e4e944cd21c)
        ;

        
    
    
            var circle_marker_d88ab8428aaa4a45ab672340f7d420ed = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3cd1c0a3e2754a5681f3c54a8f018561 = L.popup({"maxWidth": "100%"});

        
            var html_4eef34867ea94e00baccf9c4292f7d85 = $(`<div id="html_4eef34867ea94e00baccf9c4292f7d85" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1901.06125">Cold-start playlist recommendation with multitask learning</a><br></div>`)[0];
            popup_3cd1c0a3e2754a5681f3c54a8f018561.setContent(html_4eef34867ea94e00baccf9c4292f7d85);
        

        circle_marker_d88ab8428aaa4a45ab672340f7d420ed.bindPopup(popup_3cd1c0a3e2754a5681f3c54a8f018561)
        ;

        
    
    
            var circle_marker_27ecb40c4f844d6da6e33e86079dfcfc = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_896a042ead364c518f8caa5a6d183bd1 = L.popup({"maxWidth": "100%"});

        
            var html_59d6e327666b40c7b9a5b4f348b94996 = $(`<div id="html_59d6e327666b40c7b9a5b4f348b94996" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7966291/">Convolutional gated recurrent neural network incorporating spatial features for audio tagging</a><br></div>`)[0];
            popup_896a042ead364c518f8caa5a6d183bd1.setContent(html_59d6e327666b40c7b9a5b4f348b94996);
        

        circle_marker_27ecb40c4f844d6da6e33e86079dfcfc.bindPopup(popup_896a042ead364c518f8caa5a6d183bd1)
        ;

        
    
    
            var circle_marker_6a5390f46fd140f4867396177647273f = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9e641c31ef6947ec9059190928718ba9 = L.popup({"maxWidth": "100%"});

        
            var html_6228ab3eb81849d3b6f301ab2dcc7ffd = $(`<div id="html_6228ab3eb81849d3b6f301ab2dcc7ffd" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://tel.archives-ouvertes.fr/tel-01559667/">Convolutional operators in the time-frequency domain</a><br></div>`)[0];
            popup_9e641c31ef6947ec9059190928718ba9.setContent(html_6228ab3eb81849d3b6f301ab2dcc7ffd);
        

        circle_marker_6a5390f46fd140f4867396177647273f.bindPopup(popup_9e641c31ef6947ec9059190928718ba9)
        ;

        
    
    
            var circle_marker_0df61f6b62b4453699ef71cfd9468e81 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1c28bc5b119e49beaef6e16cbd0cbc4a = L.popup({"maxWidth": "100%"});

        
            var html_e23dd4729ab449deb53c3508ef5d1a52 = $(`<div id="html_e23dd4729ab449deb53c3508ef5d1a52" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s13735-018-0154-2">Current challenges and visions in music recommender systems research</a><br></div>`)[0];
            popup_1c28bc5b119e49beaef6e16cbd0cbc4a.setContent(html_e23dd4729ab449deb53c3508ef5d1a52);
        

        circle_marker_0df61f6b62b4453699ef71cfd9468e81.bindPopup(popup_1c28bc5b119e49beaef6e16cbd0cbc4a)
        ;

        
    
    
            var circle_marker_13e1744ca1bd4e2d9cc57f715eb516b0 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d7dc55611ff14cd6b6e0ccc4135cb684 = L.popup({"maxWidth": "100%"});

        
            var html_7ed67e12232e4ccdb474ee1017a503d3 = $(`<div id="html_7ed67e12232e4ccdb474ee1017a503d3" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7927482/">DCAR: A discriminative and compact audio representation for audio processing</a><br></div>`)[0];
            popup_d7dc55611ff14cd6b6e0ccc4135cb684.setContent(html_7ed67e12232e4ccdb474ee1017a503d3);
        

        circle_marker_13e1744ca1bd4e2d9cc57f715eb516b0.bindPopup(popup_d7dc55611ff14cd6b6e0ccc4135cb684)
        ;

        
    
    
            var circle_marker_9e28f2e4a9ac4d7ab7232e0f119b5e96 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_cbabee1f74c04f4bb0f7f45189e1de6b = L.popup({"maxWidth": "100%"});

        
            var html_91406837df8b4ed88c9e59b1c8a5f8f0 = $(`<div id="html_91406837df8b4ed88c9e59b1c8a5f8f0" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1808.00773">DCASE 2018 challenge surrey cross-task convolutional neural network baseline</a><br></div>`)[0];
            popup_cbabee1f74c04f4bb0f7f45189e1de6b.setContent(html_91406837df8b4ed88c9e59b1c8a5f8f0);
        

        circle_marker_9e28f2e4a9ac4d7ab7232e0f119b5e96.bindPopup(popup_cbabee1f74c04f4bb0f7f45189e1de6b)
        ;

        
    
    
            var circle_marker_0ce9f87d23234941aef66feddc760a81 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8b7f73fb8f224801b042ffe803b43044 = L.popup({"maxWidth": "100%"});

        
            var html_0437ed7ee8d84e44945326be64785e6a = $(`<div id="html_0437ed7ee8d84e44945326be64785e6a" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Diverse_Image_Generation_via_Self-Conditioned_GANs_CVPR_2020_paper.html">Diverse Image Generation via Self-Conditioned GANs</a><br></div>`)[0];
            popup_8b7f73fb8f224801b042ffe803b43044.setContent(html_0437ed7ee8d84e44945326be64785e6a);
        

        circle_marker_0ce9f87d23234941aef66feddc760a81.bindPopup(popup_8b7f73fb8f224801b042ffe803b43044)
        ;

        
    
    
            var circle_marker_8e0a0f1cccc84c0395b6881f0005236f = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7c364a1e6037427bb7b9435413dfdab7 = L.popup({"maxWidth": "100%"});

        
            var html_fca31272557646678dd4d60adfb311a8 = $(`<div id="html_fca31272557646678dd4d60adfb311a8" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683514/">Domain mismatch robust acoustic scene classification using channel information conversion</a><br></div>`)[0];
            popup_7c364a1e6037427bb7b9435413dfdab7.setContent(html_fca31272557646678dd4d60adfb311a8);
        

        circle_marker_8e0a0f1cccc84c0395b6881f0005236f.bindPopup(popup_7c364a1e6037427bb7b9435413dfdab7)
        ;

        
    
    
            var circle_marker_17c7701afcf94720836a0949b3e1ec1c = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7e2c75bb0e964b919d5c5f8a48b93e3b = L.popup({"maxWidth": "100%"});

        
            var html_7ec97d07da124d22baf263b3d8d588bb = $(`<div id="html_7ec97d07da124d22baf263b3d8d588bb" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1807.09902">General-purpose tagging of freesound audio with audioset labels: Task description, dataset, and baseline</a><br></div>`)[0];
            popup_7e2c75bb0e964b919d5c5f8a48b93e3b.setContent(html_7ec97d07da124d22baf263b3d8d588bb);
        

        circle_marker_17c7701afcf94720836a0949b3e1ec1c.bindPopup(popup_7e2c75bb0e964b919d5c5f8a48b93e3b)
        ;

        
    
    
            var circle_marker_bf6da3eb295c46c7b8b087715f213d2e = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8e581fb56363461fa6783200d1de0d5d = L.popup({"maxWidth": "100%"});

        
            var html_5d4eeaf025f84503b546c6c6e4df7f83 = $(`<div id="html_5d4eeaf025f84503b546c6c6e4df7f83" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://archives.ismir.net/ismir2019/paper/000104.pdf">Generating Structured Drum Pattern Using Variational Autoencoder and Self-similarity Matrix.</a><br></div>`)[0];
            popup_8e581fb56363461fa6783200d1de0d5d.setContent(html_5d4eeaf025f84503b546c6c6e4df7f83);
        

        circle_marker_bf6da3eb295c46c7b8b087715f213d2e.bindPopup(popup_8e581fb56363461fa6783200d1de0d5d)
        ;

        
    
    
            var circle_marker_cf591b53049a44ee8aeea74390ac4814 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_606f81c7104e4a8f90450911c3601587 = L.popup({"maxWidth": "100%"});

        
            var html_b49cb74c0330459ab4d334010ab9599e = $(`<div id="html_b49cb74c0330459ab4d334010ab9599e" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Mun_213.pdf">Generative adversarial network based acoustic scene training set augmentation and selection using SVM hyper-plane</a><br></div>`)[0];
            popup_606f81c7104e4a8f90450911c3601587.setContent(html_b49cb74c0330459ab4d334010ab9599e);
        

        circle_marker_cf591b53049a44ee8aeea74390ac4814.bindPopup(popup_606f81c7104e4a8f90450911c3601587)
        ;

        
    
    
            var circle_marker_ae7aaee710fb401d9b3ca60aad75fefe = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_235eeff4e72a4b3bb1f7516b35599c32 = L.popup({"maxWidth": "100%"});

        
            var html_9685853181bf4997a8bd3a12a76e1285 = $(`<div id="html_9685853181bf4997a8bd3a12a76e1285" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1808.05340">Genre-agnostic key classification with convolutional neural networks</a><br></div>`)[0];
            popup_235eeff4e72a4b3bb1f7516b35599c32.setContent(html_9685853181bf4997a8bd3a12a76e1285);
        

        circle_marker_ae7aaee710fb401d9b3ca60aad75fefe.bindPopup(popup_235eeff4e72a4b3bb1f7516b35599c32)
        ;

        
    
    
            var circle_marker_bde6fa264fcf438d89223fd73e3d3fba = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_521b205f4fa147bca3655a49beb8a1a5 = L.popup({"maxWidth": "100%"});

        
            var html_8fc31b35d89c4bd098a2bb32aecec9ca = $(`<div id="html_8fc31b35d89c4bd098a2bb32aecec9ca" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.researchgate.net/profile/Filip_Korzeniowski/publication/331276600_Harmonic_Analysis_of_Musical_Audio_using_Deep_Neural_Networks/links/5c6fcc2fa6fdcc471591c504/Harmonic-Analysis-of-Musical-Audio-using-Deep-Neural-Networks.pdf">Harmonic Analysis of Musical Audio using Deep Neural Networks</a><br></div>`)[0];
            popup_521b205f4fa147bca3655a49beb8a1a5.setContent(html_8fc31b35d89c4bd098a2bb32aecec9ca);
        

        circle_marker_bde6fa264fcf438d89223fd73e3d3fba.bindPopup(popup_521b205f4fa147bca3655a49beb8a1a5)
        ;

        
    
    
            var circle_marker_87ccb0abf01444dfa6f9467d506e7a48 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_589eb6675b8f4618a18804f597d89a2b = L.popup({"maxWidth": "100%"});

        
            var html_a984d6e13e3748d4b374c3869049f1c3 = $(`<div id="html_a984d6e13e3748d4b374c3869049f1c3" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://ieomsociety.org/toronto2019/papers/361.pdf">Humans' Perceptions of Handwritten Digits Generated by a Generative Adversarial Network</a><br></div>`)[0];
            popup_589eb6675b8f4618a18804f597d89a2b.setContent(html_a984d6e13e3748d4b374c3869049f1c3);
        

        circle_marker_87ccb0abf01444dfa6f9467d506e7a48.bindPopup(popup_589eb6675b8f4618a18804f597d89a2b)
        ;

        
    
    
            var circle_marker_1bfcba90f4d24c14a0a6166068debb54 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_da54119ee54443aeb18d77376b6d0225 = L.popup({"maxWidth": "100%"});

        
            var html_7b420213bfda41818548f1888496696e = $(`<div id="html_7b420213bfda41818548f1888496696e" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0957417419301794">Hybrid feature-based analysis of video's affective content using protagonist detection</a><br></div>`)[0];
            popup_da54119ee54443aeb18d77376b6d0225.setContent(html_7b420213bfda41818548f1888496696e);
        

        circle_marker_1bfcba90f4d24c14a0a6166068debb54.bindPopup(popup_da54119ee54443aeb18d77376b6d0225)
        ;

        
    
    
            var circle_marker_0d5e8dac74224042a5939d29f2fa8138 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_500ab6c5ea4e4f4dbf457bb18f73df2f = L.popup({"maxWidth": "100%"});

        
            var html_a5ff463fca874561a54e4c47f9b97fa8 = $(`<div id="html_a5ff463fca874561a54e4c47f9b97fa8" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Kothinti_79.pdf">INTEGRATED BOTTOM-UP AND TOP-DOWN INFERENCE FOR SOUND EVENT DETECTION Technical Report</a><br></div>`)[0];
            popup_500ab6c5ea4e4f4dbf457bb18f73df2f.setContent(html_a5ff463fca874561a54e4c47f9b97fa8);
        

        circle_marker_0d5e8dac74224042a5939d29f2fa8138.bindPopup(popup_500ab6c5ea4e4f4dbf457bb18f73df2f)
        ;

        
    
    
            var circle_marker_5a749e47045e4ab49ea8ca88114f5a55 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b0ba5db26459471fbbdc8ea85aec792c = L.popup({"maxWidth": "100%"});

        
            var html_d6c095d1ec0c4ff9be4ca3e84b9826ae = $(`<div id="html_d6c095d1ec0c4ff9be4ca3e84b9826ae" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://journals.sagepub.com/doi/abs/10.1177/1475921720923147">If structure can exclaim: a novel robotic-assisted percussion method for spatial bolt-ball joint looseness detection</a><br></div>`)[0];
            popup_b0ba5db26459471fbbdc8ea85aec792c.setContent(html_d6c095d1ec0c4ff9be4ca3e84b9826ae);
        

        circle_marker_5a749e47045e4ab49ea8ca88114f5a55.bindPopup(popup_b0ba5db26459471fbbdc8ea85aec792c)
        ;

        
    
    
            var circle_marker_73163b578a294b8091a0fe63ea6c3313 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_877a27e2b75e49dabb1e2c64670fe28a = L.popup({"maxWidth": "100%"});

        
            var html_9f3822837f9245018346a9b37638ab88 = $(`<div id="html_9f3822837f9245018346a9b37638ab88" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/2009.01759">Intra-Utterance Similarity Preserving Knowledge Distillation for Audio Tagging</a><br></div>`)[0];
            popup_877a27e2b75e49dabb1e2c64670fe28a.setContent(html_9f3822837f9245018346a9b37638ab88);
        

        circle_marker_73163b578a294b8091a0fe63ea6c3313.bindPopup(popup_877a27e2b75e49dabb1e2c64670fe28a)
        ;

        
    
    
            var circle_marker_962c544d62984050b4db8fd2cc793fcf = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_67f6ba2c1f42414b8abbfa7f9a62be8e = L.popup({"maxWidth": "100%"});

        
            var html_c7cf5970db3d4d5bb8a7b98731335d41 = $(`<div id="html_c7cf5970db3d4d5bb8a7b98731335d41" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Kothinti_90.pdf">JOINT ACOUSTIC AND CLASS INFERENCE FOR WEAKLY SUPERVISED SOUND EVENT DETECTION Technical Report Sandeep Kothinti1, Keisuke Imoto2 …</a><br></div>`)[0];
            popup_67f6ba2c1f42414b8abbfa7f9a62be8e.setContent(html_c7cf5970db3d4d5bb8a7b98731335d41);
        

        circle_marker_962c544d62984050b4db8fd2cc793fcf.bindPopup(popup_67f6ba2c1f42414b8abbfa7f9a62be8e)
        ;

        
    
    
            var circle_marker_10e32af2136d4cf2aa3cb7f62c189871 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8b9a117ecc3f44a88f142957ff27f15c = L.popup({"maxWidth": "100%"});

        
            var html_3ddaf5e3090242c19d9f7f10d9e936d6 = $(`<div id="html_3ddaf5e3090242c19d9f7f10d9e936d6" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682772/">Joint acoustic and class inference for weakly supervised sound event detection</a><br></div>`)[0];
            popup_8b9a117ecc3f44a88f142957ff27f15c.setContent(html_3ddaf5e3090242c19d9f7f10d9e936d6);
        

        circle_marker_10e32af2136d4cf2aa3cb7f62c189871.bindPopup(popup_8b9a117ecc3f44a88f142957ff27f15c)
        ;

        
    
    
            var circle_marker_fba68d416df84705b1cf48d84e24b779 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f6355d8b96d24ff4b152f489a2ab6f2b = L.popup({"maxWidth": "100%"});

        
            var html_f17358ede57a4a15a8cb36091d30725b = $(`<div id="html_f17358ede57a4a15a8cb36091d30725b" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ccrma.stanford.edu/~urinieto/MARL/publications/ISMIR2020_MoodPrediction.pdf">MOOD CLASSIFICATION USING LISTENING DATA</a><br></div>`)[0];
            popup_f6355d8b96d24ff4b152f489a2ab6f2b.setContent(html_f17358ede57a4a15a8cb36091d30725b);
        

        circle_marker_fba68d416df84705b1cf48d84e24b779.bindPopup(popup_f6355d8b96d24ff4b152f489a2ab6f2b)
        ;

        
    
    
            var circle_marker_08579cc135cc4b96be9a87be7e0d4d0d = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_50fb3192a880445ba64aeef1842e69c3 = L.popup({"maxWidth": "100%"});

        
            var html_f5a091f2fef946779346fe2f9020d16e = $(`<div id="html_f5a091f2fef946779346fe2f9020d16e" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8925176/">Multi-scale convolutional recurrent neural network with ensemble method for weakly labeled sound event detection</a><br></div>`)[0];
            popup_50fb3192a880445ba64aeef1842e69c3.setContent(html_f5a091f2fef946779346fe2f9020d16e);
        

        circle_marker_08579cc135cc4b96be9a87be7e0d4d0d.bindPopup(popup_50fb3192a880445ba64aeef1842e69c3)
        ;

        
    
    
            var circle_marker_bd157f0c615a4848b7a585438117f12a = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_4cbe9ccf938e472798f468585f2714c2 = L.popup({"maxWidth": "100%"});

        
            var html_96e759fd21774119a7a415e2656a0bed = $(`<div id="html_96e759fd21774119a7a415e2656a0bed" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://escholarship.org/uc/item/6mm160gq">Neural network based representation learning and modeling for speech and speaker recognition</a><br></div>`)[0];
            popup_4cbe9ccf938e472798f468585f2714c2.setContent(html_96e759fd21774119a7a415e2656a0bed);
        

        circle_marker_bd157f0c615a4848b7a585438117f12a.bindPopup(popup_4cbe9ccf938e472798f468585f2714c2)
        ;

        
    
    
            var circle_marker_ad4916ad5c104b80aa4147d586241b64 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_104f1be1cbfc4d128b39a9f0b2098f5f = L.popup({"maxWidth": "100%"});

        
            var html_2808ccb599614e0ebc82b1939ad87137 = $(`<div id="html_2808ccb599614e0ebc82b1939ad87137" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/2007.09284">Optimal Bayesian estimation of Gaussian mixtures with growing number of components</a><br></div>`)[0];
            popup_104f1be1cbfc4d128b39a9f0b2098f5f.setContent(html_2808ccb599614e0ebc82b1939ad87137);
        

        circle_marker_ad4916ad5c104b80aa4147d586241b64.bindPopup(popup_104f1be1cbfc4d128b39a9f0b2098f5f)
        ;

        
    
    
            var circle_marker_4898acafc68d4ed5ac1f08a472d6cc35 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b1e1e8a0deb240418419cd2552ba0463 = L.popup({"maxWidth": "100%"});

        
            var html_548c9d39255f457896f24673bc46e984 = $(`<div id="html_548c9d39255f457896f24673bc46e984" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.theses.fr/2017PSLEE012">Opérateurs convolutionnels dans le plan temps-fréquence</a><br></div>`)[0];
            popup_b1e1e8a0deb240418419cd2552ba0463.setContent(html_548c9d39255f457896f24673bc46e984);
        

        circle_marker_4898acafc68d4ed5ac1f08a472d6cc35.bindPopup(popup_b1e1e8a0deb240418419cd2552ba0463)
        ;

        
    
    
            var circle_marker_2848cc7437134130ac27b88bbc128cfb = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_84a031fd7bb94dfb911c06fea556f139 = L.popup({"maxWidth": "100%"});

        
            var html_8928e051a92b4ce5a6a2b6ffd7f6eaf3 = $(`<div id="html_8928e051a92b4ce5a6a2b6ffd7f6eaf3" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="">Polyphonic sound event detection with weak labeling</a><br></div>`)[0];
            popup_84a031fd7bb94dfb911c06fea556f139.setContent(html_8928e051a92b4ce5a6a2b6ffd7f6eaf3);
        

        circle_marker_2848cc7437134130ac27b88bbc128cfb.bindPopup(popup_84a031fd7bb94dfb911c06fea556f139)
        ;

        
    
    
            var circle_marker_70a94e758ef947ef9f8db94c9a9a9844 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1bf2d0d0165e4d06ac8f47f5e668ff09 = L.popup({"maxWidth": "100%"});

        
            var html_58417bcf23b24f58afccfe5de2acaa10 = $(`<div id="html_58417bcf23b24f58afccfe5de2acaa10" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1908.06708">Recommender Systems Fairness Evaluation via Generalized Cross Entropy</a><br></div>`)[0];
            popup_1bf2d0d0165e4d06ac8f47f5e668ff09.setContent(html_58417bcf23b24f58afccfe5de2acaa10);
        

        circle_marker_70a94e758ef947ef9f8db94c9a9a9844.bindPopup(popup_1bf2d0d0165e4d06ac8f47f5e668ff09)
        ;

        
    
    
            var circle_marker_5f548c28733c4b92b5e9ff762c04795c = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9d0286ea806f4d918a68eeb49af2244f = L.popup({"maxWidth": "100%"});

        
            var html_e4801a7f9d6645f3a3aa5e5669f0e880 = $(`<div id="html_e4801a7f9d6645f3a3aa5e5669f0e880" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.aclweb.org/anthology/2020.ecnlp-1.7/">Semi-Supervised Iterative Approach for Domain-Specific Complaint Detection in Social Media</a><br></div>`)[0];
            popup_9d0286ea806f4d918a68eeb49af2244f.setContent(html_e4801a7f9d6645f3a3aa5e5669f0e880);
        

        circle_marker_5f548c28733c4b92b5e9ff762c04795c.bindPopup(popup_9d0286ea806f4d918a68eeb49af2244f)
        ;

        
    
    
            var circle_marker_7a4705d0e6f24297920613608fb01486 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2eb7093bf42745f4abf192e399bced86 = L.popup({"maxWidth": "100%"});

        
            var html_aa6e2b62c91047868c0d927ef131b97c = $(`<div id="html_aa6e2b62c91047868c0d927ef131b97c" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-319-63450-0_13">Sound analysis in smart cities</a><br></div>`)[0];
            popup_2eb7093bf42745f4abf192e399bced86.setContent(html_aa6e2b62c91047868c0d927ef131b97c);
        

        circle_marker_7a4705d0e6f24297920613608fb01486.bindPopup(popup_2eb7093bf42745f4abf192e399bced86)
        ;

        
    
    
            var circle_marker_16515a9a3bc64b7e88daf08a9b0ff971 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_450c774fb7e64908a9740660e2c11a31 = L.popup({"maxWidth": "100%"});

        
            var html_ffd5aca9b1344211bf057f6c4e63d5f8 = $(`<div id="html_ffd5aca9b1344211bf057f6c4e63d5f8" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://openreview.net/forum?id=HkGv2NMTjQ">Sound event classification using ontology-based neural networks</a><br></div>`)[0];
            popup_450c774fb7e64908a9740660e2c11a31.setContent(html_ffd5aca9b1344211bf057f6c4e63d5f8);
        

        circle_marker_16515a9a3bc64b7e88daf08a9b0ff971.bindPopup(popup_450c774fb7e64908a9740660e2c11a31)
        ;

        
    
    
            var circle_marker_27d6687e63844aafa09212db8589cf51 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_44227fbb25994497a947bb09bc47fe08 = L.popup({"maxWidth": "100%"});

        
            var html_6bb234befafa47c9b6abe449cebf1745 = $(`<div id="html_6bb234befafa47c9b6abe449cebf1745" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60771">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</a><br></div>`)[0];
            popup_44227fbb25994497a947bb09bc47fe08.setContent(html_6bb234befafa47c9b6abe449cebf1745);
        

        circle_marker_27d6687e63844aafa09212db8589cf51.bindPopup(popup_44227fbb25994497a947bb09bc47fe08)
        ;

        
    
    
            var circle_marker_cd745f5efae548d682a7a5c4d212f75e = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e26b08529f04425181482950ed351d26 = L.popup({"maxWidth": "100%"});

        
            var html_5d8513604c944473bb02938391df219a = $(`<div id="html_5d8513604c944473bb02938391df219a" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9054478/">Sound event detection in synthetic domestic environments</a><br></div>`)[0];
            popup_e26b08529f04425181482950ed351d26.setContent(html_5d8513604c944473bb02938391df219a);
        

        circle_marker_cd745f5efae548d682a7a5c4d212f75e.bindPopup(popup_e26b08529f04425181482950ed351d26)
        ;

        
    
    
            var circle_marker_ceedef57c8924bfea69e158e667fdc4c = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ba546dd2c026473ab32e973b902f6e1d = L.popup({"maxWidth": "100%"});

        
            var html_da88e0a14197485f93e0777c10e41bc6 = $(`<div id="html_da88e0a14197485f93e0777c10e41bc6" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8673582/">Sound event detection in the DCASE 2017 challenge</a><br></div>`)[0];
            popup_ba546dd2c026473ab32e973b902f6e1d.setContent(html_da88e0a14197485f93e0777c10e41bc6);
        

        circle_marker_ceedef57c8924bfea69e158e667fdc4c.bindPopup(popup_ba546dd2c026473ab32e973b902f6e1d)
        ;

        
    
    
            var circle_marker_67f38217f1a44ac18f4a246d1f7873fe = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_75a1c932260c42bdb6f5f160a426f729 = L.popup({"maxWidth": "100%"});

        
            var html_2ee213389168405eaedc7326a8cbdab7 = $(`<div id="html_2ee213389168405eaedc7326a8cbdab7" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9165887/">Sound event detection of weakly labelled data with CNN-transformer and automatic threshold optimization</a><br></div>`)[0];
            popup_75a1c932260c42bdb6f5f160a426f729.setContent(html_2ee213389168405eaedc7326a8cbdab7);
        

        circle_marker_67f38217f1a44ac18f4a246d1f7873fe.bindPopup(popup_75a1c932260c42bdb6f5f160a426f729)
        ;

        
    
    
            var circle_marker_f7b41454767d4951a86fe1ffc5d12051 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6de994ba6dd34e16affacd2ac4bf837a = L.popup({"maxWidth": "100%"});

        
            var html_dea8eabc2ad44549aa86b7bb1b919c93 = $(`<div id="html_dea8eabc2ad44549aa86b7bb1b919c93" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1803.01164">The history began from alexnet: A comprehensive survey on deep learning approaches</a><br></div>`)[0];
            popup_6de994ba6dd34e16affacd2ac4bf837a.setContent(html_dea8eabc2ad44549aa86b7bb1b919c93);
        

        circle_marker_f7b41454767d4951a86fe1ffc5d12051.bindPopup(popup_6de994ba6dd34e16affacd2ac4bf837a)
        ;

        
    
    
            var circle_marker_a0b9c2164e7940dc890af31dc15513f2 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b5e1107229db4d3a8f076968cb2504af = L.popup({"maxWidth": "100%"});

        
            var html_40732b4c25c24d68b98c9840677766a2 = $(`<div id="html_40732b4c25c24d68b98c9840677766a2" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="">Uncertainty aware multimodal activity recognition with Bayesian inference.</a><br></div>`)[0];
            popup_b5e1107229db4d3a8f076968cb2504af.setContent(html_40732b4c25c24d68b98c9840677766a2);
        

        circle_marker_a0b9c2164e7940dc890af31dc15513f2.bindPopup(popup_b5e1107229db4d3a8f076968cb2504af)
        ;

        
    
    
            var circle_marker_521cfc31081545df85fba3220046fad3 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2a937db094b140858d7010e3a1eff09d = L.popup({"maxWidth": "100%"});

        
            var html_c6c605f913a24f379f22631f6e3658a6 = $(`<div id="html_c6c605f913a24f379f22631f6e3658a6" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Subedar_Uncertainty-Aware_Audiovisual_Activity_Recognition_Using_Deep_Bayesian_Variational_Inference_ICCV_2019_paper.html">Uncertainty-aware audiovisual activity recognition using deep bayesian variational inference</a><br></div>`)[0];
            popup_2a937db094b140858d7010e3a1eff09d.setContent(html_c6c605f913a24f379f22631f6e3658a6);
        

        circle_marker_521cfc31081545df85fba3220046fad3.bindPopup(popup_2a937db094b140858d7010e3a1eff09d)
        ;

        
    
    
            var circle_marker_949ac392f06646bcb1dab1ec41c1dace = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7e82e403f7b44e9cbfe27b7d625247b8 = L.popup({"maxWidth": "100%"});

        
            var html_c108d2e5a34f4aad815d1567736c2669 = $(`<div id="html_c108d2e5a34f4aad815d1567736c2669" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8937231/">Unsupervised adversarial domain adaptation based on the wasserstein distance for acoustic scene classification</a><br></div>`)[0];
            popup_7e82e403f7b44e9cbfe27b7d625247b8.setContent(html_c108d2e5a34f4aad815d1567736c2669);
        

        circle_marker_949ac392f06646bcb1dab1ec41c1dace.bindPopup(popup_7e82e403f7b44e9cbfe27b7d625247b8)
        ;

        
    
    
            var circle_marker_dfe141cde11f4f3c9d87f3c8c26b500b = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a2364212559d48489a08968937ba62b9 = L.popup({"maxWidth": "100%"});

        
            var html_3f52766135c9458091a06b6ea17a8705 = $(`<div id="html_3f52766135c9458091a06b6ea17a8705" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7952190/">Very deep convolutional neural networks for raw waveforms</a><br></div>`)[0];
            popup_a2364212559d48489a08968937ba62b9.setContent(html_3f52766135c9458091a06b6ea17a8705);
        

        circle_marker_dfe141cde11f4f3c9d87f3c8c26b500b.bindPopup(popup_a2364212559d48489a08968937ba62b9)
        ;

        
    
    
            var circle_marker_0eee9787f78e4c3d9508848f78b016c2 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9ec7a0befe694518a973838eb0ac2bd4 = L.popup({"maxWidth": "100%"});

        
            var html_c86137d915144fcf8945fa43aa3ba387 = $(`<div id="html_c86137d915144fcf8945fa43aa3ba387" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://deepai.org/publication/weakly-labelled-audioset-classification-with-attention-neural-networks">Weakly labelled audioset classification with attention neural networks</a><br></div>`)[0];
            popup_9ec7a0befe694518a973838eb0ac2bd4.setContent(html_c86137d915144fcf8945fa43aa3ba387);
        

        circle_marker_0eee9787f78e4c3d9508848f78b016c2.bindPopup(popup_9ec7a0befe694518a973838eb0ac2bd4)
        ;

        
    
    
            var circle_marker_55765e086d0e479f92168afd76823ead = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9f97a76df311457a97b3bb86d3c861d9 = L.popup({"maxWidth": "100%"});

        
            var html_25ca0fed5d004774847bed14f3b7a958 = $(`<div id="html_25ca0fed5d004774847bed14f3b7a958" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8777125/">Weakly labelled audioset tagging with attention neural networks</a><br></div>`)[0];
            popup_9f97a76df311457a97b3bb86d3c861d9.setContent(html_25ca0fed5d004774847bed14f3b7a958);
        

        circle_marker_55765e086d0e479f92168afd76823ead.bindPopup(popup_9f97a76df311457a97b3bb86d3c861d9)
        ;

        
    
    
            var circle_marker_8b9beae2107f43d5b7668f5a376bf79e = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1b824c7e9775490cbdec950a52ef479a = L.popup({"maxWidth": "100%"});

        
            var html_af85f7d115354d69ba5caef0264c3ba4 = $(`<div id="html_af85f7d115354d69ba5caef0264c3ba4" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7966035/">A convolutional neural network approach for acoustic scene classification</a><br></div>`)[0];
            popup_1b824c7e9775490cbdec950a52ef479a.setContent(html_af85f7d115354d69ba5caef0264c3ba4);
        

        circle_marker_8b9beae2107f43d5b7668f5a376bf79e.bindPopup(popup_1b824c7e9775490cbdec950a52ef479a)
        ;

        
    
    
            var circle_marker_07b7c94366ee4747be6c586c89538a9c = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e7e081fefd29424793d08436897ee4c1 = L.popup({"maxWidth": "100%"});

        
            var html_6d3af3e7b1ed4012a95a1becc1944e3a = $(`<div id="html_6d3af3e7b1ed4012a95a1becc1944e3a" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="http://archive.nyu.edu/handle/2451/60751">Acoustic scene classification in DCASE 2019 challenge: Closed and open set classification and data mismatch setups</a><br></div>`)[0];
            popup_e7e081fefd29424793d08436897ee4c1.setContent(html_6d3af3e7b1ed4012a95a1becc1944e3a);
        

        circle_marker_07b7c94366ee4747be6c586c89538a9c.bindPopup(popup_e7e081fefd29424793d08436897ee4c1)
        ;

        
    
    
            var circle_marker_e0bc8c075c774b9d8ba6618d0b64c1d2 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e17644b46b834cd78192463705194a0c = L.popup({"maxWidth": "100%"});

        
            var html_e401d0963668401ea9e32070ddc1736d = $(`<div id="html_e401d0963668401ea9e32070ddc1736d" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="http://dcase.community/documents/challenge2017/technical_reports/DCASE2017_Jallet_140.pdf">Acoustic scene classification using convolutional recurrent neural networks</a><br></div>`)[0];
            popup_e17644b46b834cd78192463705194a0c.setContent(html_e401d0963668401ea9e32070ddc1736d);
        

        circle_marker_e0bc8c075c774b9d8ba6618d0b64c1d2.bindPopup(popup_e17644b46b834cd78192463705194a0c)
        ;

        
    
    
            var circle_marker_4b72c543ade04668a417e029c0c14278 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_63195536073744d59318abdce6418310 = L.popup({"maxWidth": "100%"});

        
            var html_d8ce7a9b74df4ea0a1fef9031776c243 = $(`<div id="html_d8ce7a9b74df4ea0a1fef9031776c243" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8521242/">Acoustic scene classification: an overview of DCASE 2017 challenge entries</a><br></div>`)[0];
            popup_63195536073744d59318abdce6418310.setContent(html_d8ce7a9b74df4ea0a1fef9031776c243);
        

        circle_marker_4b72c543ade04668a417e029c0c14278.bindPopup(popup_63195536073744d59318abdce6418310)
        ;

        
    
    
            var circle_marker_649c691174fd4089b3171337a3dea107 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f7a8e8600c2c48fdbf5e6227f8f8b5f2 = L.popup({"maxWidth": "100%"});

        
            var html_a4a2b8bc678943bea138f141984c10d6 = $(`<div id="html_a4a2b8bc678943bea138f141984c10d6" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://arxiv.org/abs/2002.05033">Active Learning for Sound Event Detection</a><br></div>`)[0];
            popup_f7a8e8600c2c48fdbf5e6227f8f8b5f2.setContent(html_a4a2b8bc678943bea138f141984c10d6);
        

        circle_marker_649c691174fd4089b3171337a3dea107.bindPopup(popup_f7a8e8600c2c48fdbf5e6227f8f8b5f2)
        ;

        
    
    
            var circle_marker_6c3d8ec8eb8e403e8eb4e682ff06d339 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3f07e0cfcc174f08ac41fe3f32862be1 = L.popup({"maxWidth": "100%"});

        
            var html_a319c4e08cba4ff3a514b3110dbeb385 = $(`<div id="html_a319c4e08cba4ff3a514b3110dbeb385" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8170047/">Assessment of human and machine performance in acoustic scene classification: DCASE 2016 case study</a><br></div>`)[0];
            popup_3f07e0cfcc174f08ac41fe3f32862be1.setContent(html_a319c4e08cba4ff3a514b3110dbeb385);
        

        circle_marker_6c3d8ec8eb8e403e8eb4e682ff06d339.bindPopup(popup_3f07e0cfcc174f08ac41fe3f32862be1)
        ;

        
    
    
            var circle_marker_ae1e9bcf645047c9917c02e11cfa2664 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e58e6337534948bdb7297740b8dde246 = L.popup({"maxWidth": "100%"});

        
            var html_8cbb521403904148b0ce901985acf9a7 = $(`<div id="html_8cbb521403904148b0ce901985acf9a7" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8123864/">Detection and classification of acoustic scenes and events: Outcome of the DCASE 2016 challenge</a><br></div>`)[0];
            popup_e58e6337534948bdb7297740b8dde246.setContent(html_8cbb521403904148b0ce901985acf9a7);
        

        circle_marker_ae1e9bcf645047c9917c02e11cfa2664.bindPopup(popup_e58e6337534948bdb7297740b8dde246)
        ;

        
    
    
            var circle_marker_42788eb4cc964ccc842cd6b4444dba4f = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e36db74d4dcc42e6984c779b94a71fb9 = L.popup({"maxWidth": "100%"});

        
            var html_96b7b3704af54d3581281014d64f0907 = $(`<div id="html_96b7b3704af54d3581281014d64f0907" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9057638/">Glottal Source Information for Pathological Voice Detection</a><br></div>`)[0];
            popup_e36db74d4dcc42e6984c779b94a71fb9.setContent(html_96b7b3704af54d3581281014d64f0907);
        

        circle_marker_42788eb4cc964ccc842cd6b4444dba4f.bindPopup(popup_e36db74d4dcc42e6984c779b94a71fb9)
        ;

        
    
    
            var circle_marker_06c882969a2c401a9ce333476a249b91 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c4fa693d9a874b8ebbeb758a44c62e24 = L.popup({"maxWidth": "100%"});

        
            var html_da3b980ad0d848bb934b9d8b22d25272 = $(`<div id="html_da3b980ad0d848bb934b9d8b22d25272" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8673582/">Sound event detection in the DCASE 2017 challenge</a><br></div>`)[0];
            popup_c4fa693d9a874b8ebbeb758a44c62e24.setContent(html_da3b980ad0d848bb934b9d8b22d25272);
        

        circle_marker_06c882969a2c401a9ce333476a249b91.bindPopup(popup_c4fa693d9a874b8ebbeb758a44c62e24)
        ;

        
    
    
            var circle_marker_4616586c7ecb45e988445c04eaad88c6 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_aff7cbd2f3d34547be9dc7c8647790bd = L.popup({"maxWidth": "100%"});

        
            var html_041c03ea65b6474fb2eb44d0404bddb8 = $(`<div id="html_041c03ea65b6474fb2eb44d0404bddb8" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682858/">Sound event envelope estimation in polyphonic mixtures</a><br></div>`)[0];
            popup_aff7cbd2f3d34547be9dc7c8647790bd.setContent(html_041c03ea65b6474fb2eb44d0404bddb8);
        

        circle_marker_4616586c7ecb45e988445c04eaad88c6.bindPopup(popup_aff7cbd2f3d34547be9dc7c8647790bd)
        ;

        
    
    
            var circle_marker_80817fa783f943c5967248bc0b45dbe9 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_649d6ae740044e1f9093500c991e6b7f = L.popup({"maxWidth": "100%"});

        
            var html_f3afec40619542a9b2810632794136a0 = $(`<div id="html_f3afec40619542a9b2810632794136a0" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8937231/">Unsupervised adversarial domain adaptation based on the wasserstein distance for acoustic scene classification</a><br></div>`)[0];
            popup_649d6ae740044e1f9093500c991e6b7f.setContent(html_f3afec40619542a9b2810632794136a0);
        

        circle_marker_80817fa783f943c5967248bc0b45dbe9.bindPopup(popup_649d6ae740044e1f9093500c991e6b7f)
        ;

        
    
    
            var circle_marker_82cddd266fa648fc8df9add9eff32872 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a26d89a466b64740ba6cdcbc8aa1ad62 = L.popup({"maxWidth": "100%"});

        
            var html_3f0a58fd49094445b5495f05bd01d7fc = $(`<div id="html_3f0a58fd49094445b5495f05bd01d7fc" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://arxiv.org/abs/1808.05777">Unsupervised adversarial domain adaptation for acoustic scene classification</a><br></div>`)[0];
            popup_a26d89a466b64740ba6cdcbc8aa1ad62.setContent(html_3f0a58fd49094445b5495f05bd01d7fc);
        

        circle_marker_82cddd266fa648fc8df9add9eff32872.bindPopup(popup_a26d89a466b64740ba6cdcbc8aa1ad62)
        ;

        
    
    
            var circle_marker_fa893eb9248f459e8d25c65a2af381be = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b2451a30366f440da9e00f533e0203fb = L.popup({"maxWidth": "100%"});

        
            var html_f938273901ef45768ef0fac601f441cb = $(`<div id="html_f938273901ef45768ef0fac601f441cb" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://opus.bibliothek.uni-augsburg.de/opus4/files/45063/DCASE_2017+-+Sequence.pdf">Workshop (DCASE2017)</a><br></div>`)[0];
            popup_b2451a30366f440da9e00f533e0203fb.setContent(html_f938273901ef45768ef0fac601f441cb);
        

        circle_marker_fa893eb9248f459e8d25c65a2af381be.bindPopup(popup_b2451a30366f440da9e00f533e0203fb)
        ;

        
    
    
            var circle_marker_719198ffaebd4cbc9a74829ca5257a85 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c9318175a61844fc96cc534d5c02c03d = L.popup({"maxWidth": "100%"});

        
            var html_cd35afa919e24e82ad460d807cd6d587 = $(`<div id="html_cd35afa919e24e82ad460d807cd6d587" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-981-13-8707-4_8">A Comparison of Attention Mechanisms of Convolutional Neural Network in Weakly Labeled Audio Tagging</a><br></div>`)[0];
            popup_c9318175a61844fc96cc534d5c02c03d.setContent(html_cd35afa919e24e82ad460d807cd6d587);
        

        circle_marker_719198ffaebd4cbc9a74829ca5257a85.bindPopup(popup_c9318175a61844fc96cc534d5c02c03d)
        ;

        
    
    
            var circle_marker_ef4314d9f78842749fadb99943860481 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e9e964d685ae4b6fa29344fba7335765 = L.popup({"maxWidth": "100%"});

        
            var html_96211252e4da49d98a297875c9e91d71 = $(`<div id="html_96211252e4da49d98a297875c9e91d71" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.researchgate.net/profile/Lam_Pham4/publication/335829391_A_Robust_Framework_for_Acoustic_Scene_Classification/links/5dc03975a6fdcc2128011ee7/A-Robust-Framework-for-Acoustic-Scene-Classification.pdf">A Robust Framework for Acoustic Scene Classification.</a><br></div>`)[0];
            popup_e9e964d685ae4b6fa29344fba7335765.setContent(html_96211252e4da49d98a297875c9e91d71);
        

        circle_marker_ef4314d9f78842749fadb99943860481.bindPopup(popup_e9e964d685ae4b6fa29344fba7335765)
        ;

        
    
    
            var circle_marker_62fe17d9165d4bd98d440d4b7c79f858 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_51a12e36add042f9b1280a08a96af12e = L.popup({"maxWidth": "100%"});

        
            var html_15f316c6b022400eacc0cedfd974b1e9 = $(`<div id="html_15f316c6b022400eacc0cedfd974b1e9" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9052995/">A framework for the robust evaluation of sound event detection</a><br></div>`)[0];
            popup_51a12e36add042f9b1280a08a96af12e.setContent(html_15f316c6b022400eacc0cedfd974b1e9);
        

        circle_marker_62fe17d9165d4bd98d440d4b7c79f858.bindPopup(popup_51a12e36add042f9b1280a08a96af12e)
        ;

        
    
    
            var circle_marker_aa7f64c223dc42cb8ce1f7f956c2aee0 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b4f06e560ff0444889eea60d6d202557 = L.popup({"maxWidth": "100%"});

        
            var html_66d580aa07fd46abbeeb75df334799a9 = $(`<div id="html_66d580aa07fd46abbeeb75df334799a9" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553225/">A fusion of deep convolutional generative adversarial networks and sequence to sequence autoencoders for acoustic scene classification</a><br></div>`)[0];
            popup_b4f06e560ff0444889eea60d6d202557.setContent(html_66d580aa07fd46abbeeb75df334799a9);
        

        circle_marker_aa7f64c223dc42cb8ce1f7f956c2aee0.bindPopup(popup_b4f06e560ff0444889eea60d6d202557)
        ;

        
    
    
            var circle_marker_c7657c7e649a4da194536ff406b10b8c = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8ac385115a5b4c8b93d1de5d78d5497e = L.popup({"maxWidth": "100%"});

        
            var html_216fe7ac37c44566b6c669d1e7939cba = $(`<div id="html_216fe7ac37c44566b6c669d1e7939cba" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1481.pdf">ASe: Acoustic Scene Embedding Using Deep Archetypal Analysis and GMM.</a><br></div>`)[0];
            popup_8ac385115a5b4c8b93d1de5d78d5497e.setContent(html_216fe7ac37c44566b6c669d1e7939cba);
        

        circle_marker_c7657c7e649a4da194536ff406b10b8c.bindPopup(popup_8ac385115a5b4c8b93d1de5d78d5497e)
        ;

        
    
    
            var circle_marker_55bc969495714a4ba0090991a57c79d8 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7779c19141134be1acbfaa36859dec76 = L.popup({"maxWidth": "100%"});

        
            var html_d335d81d812e4a90b354fe32e8a56b89 = $(`<div id="html_d335d81d812e4a90b354fe32e8a56b89" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/2007.12391">Artificial Intelligence in the Creative Industries: A Review</a><br></div>`)[0];
            popup_7779c19141134be1acbfaa36859dec76.setContent(html_d335d81d812e4a90b354fe32e8a56b89);
        

        circle_marker_55bc969495714a4ba0090991a57c79d8.bindPopup(popup_7779c19141134be1acbfaa36859dec76)
        ;

        
    
    
            var circle_marker_c1a2c0672493492685b1bf1552a1ce5b = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_4b8b67a90ca341fc8188db5cd93d79c7 = L.popup({"maxWidth": "100%"});

        
            var html_26bb19421b534649b3e78f28b309fe08 = $(`<div id="html_26bb19421b534649b3e78f28b309fe08" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/1703.04770">Audio scene classification with deep recurrent neural networks</a><br></div>`)[0];
            popup_4b8b67a90ca341fc8188db5cd93d79c7.setContent(html_26bb19421b534649b3e78f28b309fe08);
        

        circle_marker_c1a2c0672493492685b1bf1552a1ce5b.bindPopup(popup_4b8b67a90ca341fc8188db5cd93d79c7)
        ;

        
    
    
            var circle_marker_16ce9761bca74010812ba0af6df809b6 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0b17263d0b4b43dba16ac6329db5cf0a = L.popup({"maxWidth": "100%"});

        
            var html_2ee36776c8804e01b61ce1f265705452 = $(`<div id="html_2ee36776c8804e01b61ce1f265705452" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/1811.01095">Beyond equal-length snippets: How long is sufficient to recognize an audio scene?</a><br></div>`)[0];
            popup_0b17263d0b4b43dba16ac6329db5cf0a.setContent(html_2ee36776c8804e01b61ce1f265705452);
        

        circle_marker_16ce9761bca74010812ba0af6df809b6.bindPopup(popup_0b17263d0b4b43dba16ac6329db5cf0a)
        ;

        
    
    
            var circle_marker_106a3e67cd3547f8969752d8f46bcd98 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_83903cccff5845a8ad3a563be28b89c9 = L.popup({"maxWidth": "100%"});

        
            var html_c017be6deb694cb5bb30d246b9e2c505 = $(`<div id="html_c017be6deb694cb5bb30d246b9e2c505" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7966291/">Convolutional gated recurrent neural network incorporating spatial features for audio tagging</a><br></div>`)[0];
            popup_83903cccff5845a8ad3a563be28b89c9.setContent(html_c017be6deb694cb5bb30d246b9e2c505);
        

        circle_marker_106a3e67cd3547f8969752d8f46bcd98.bindPopup(popup_83903cccff5845a8ad3a563be28b89c9)
        ;

        
    
    
            var circle_marker_1789b1e562b045028b2e79548f7d72bd = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6731e9279adf4385849927d1aee00f2c = L.popup({"maxWidth": "100%"});

        
            var html_8ac11113d842441795756ce953b94672 = $(`<div id="html_8ac11113d842441795756ce953b94672" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/1808.00773">DCASE 2018 challenge surrey cross-task convolutional neural network baseline</a><br></div>`)[0];
            popup_6731e9279adf4385849927d1aee00f2c.setContent(html_8ac11113d842441795756ce953b94672);
        

        circle_marker_1789b1e562b045028b2e79548f7d72bd.bindPopup(popup_6731e9279adf4385849927d1aee00f2c)
        ;

        
    
    
            var circle_marker_78020e7db7a2439490101f578bfd9a14 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_176914ed45f3499f9de5838f4bdbe61f = L.popup({"maxWidth": "100%"});

        
            var html_883e1b131ab64a53bd55eea21d15e3ad = $(`<div id="html_883e1b131ab64a53bd55eea21d15e3ad" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/2007.12864">DD-CNN: Depthwise Disout Convolutional Neural Network for Low-complexity Acoustic Scene Classification</a><br></div>`)[0];
            popup_176914ed45f3499f9de5838f4bdbe61f.setContent(html_883e1b131ab64a53bd55eea21d15e3ad);
        

        circle_marker_78020e7db7a2439490101f578bfd9a14.bindPopup(popup_176914ed45f3499f9de5838f4bdbe61f)
        ;

        
    
    
            var circle_marker_9e9a44ddb6be4991be46ee6821c5dcdc = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7e0f8f42e0574c07a3abb93df274d0a4 = L.popup({"maxWidth": "100%"});

        
            var html_c53d797927524d448ed159ab893e4443 = $(`<div id="html_c53d797927524d448ed159ab893e4443" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8123864/">Detection and classification of acoustic scenes and events: Outcome of the DCASE 2016 challenge</a><br></div>`)[0];
            popup_7e0f8f42e0574c07a3abb93df274d0a4.setContent(html_c53d797927524d448ed159ab893e4443);
        

        circle_marker_9e9a44ddb6be4991be46ee6821c5dcdc.bindPopup(popup_7e0f8f42e0574c07a3abb93df274d0a4)
        ;

        
    
    
            var circle_marker_c67ffec72e224bf4a293a534f2b2a762 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_188725423155401e8978b081e5ec6167 = L.popup({"maxWidth": "100%"});

        
            var html_1bff7b73ce3e427d8d77f567d170c281 = $(`<div id="html_1bff7b73ce3e427d8d77f567d170c281" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.mdpi.com/239784">EigenScape: A database of spatial acoustic scene recordings</a><br></div>`)[0];
            popup_188725423155401e8978b081e5ec6167.setContent(html_1bff7b73ce3e427d8d77f567d170c281);
        

        circle_marker_c67ffec72e224bf4a293a534f2b2a762.bindPopup(popup_188725423155401e8978b081e5ec6167)
        ;

        
    
    
            var circle_marker_cf4e6b228de3476ab22dd4d1100b54db = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_fb4a737ee76140faafe3af69bcdf5ce5 = L.popup({"maxWidth": "100%"});

        
            var html_61417b173799409f96d696d08322de77 = $(`<div id="html_61417b173799409f96d696d08322de77" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="http://web.pkusz.edu.cn/adsp/files/2020/08/Interspeech2020_%E7%8E%8B%E8%B5%AB%E9%BA%9F_Environmental-Sound-Classification-with-Parallel-Temporal-spectral-Attention.pdf">Environmental sound classification with parallel temporal-spectral attention</a><br></div>`)[0];
            popup_fb4a737ee76140faafe3af69bcdf5ce5.setContent(html_61417b173799409f96d696d08322de77);
        

        circle_marker_cf4e6b228de3476ab22dd4d1100b54db.bindPopup(popup_fb4a737ee76140faafe3af69bcdf5ce5)
        ;

        
    
    
            var circle_marker_6016c983937540b5baf76c5202de92db = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f597657883d24277b6adf7534ee1befb = L.popup({"maxWidth": "100%"});

        
            var html_cf93c65ec5864030a0c38407c2e42943 = $(`<div id="html_cf93c65ec5864030a0c38407c2e42943" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://asa.scitation.org/doi/abs/10.1121/1.5111059">General audio tagging with ensembling convolutional neural networks and statistical features</a><br></div>`)[0];
            popup_f597657883d24277b6adf7534ee1befb.setContent(html_cf93c65ec5864030a0c38407c2e42943);
        

        circle_marker_6016c983937540b5baf76c5202de92db.bindPopup(popup_f597657883d24277b6adf7534ee1befb)
        ;

        
    
    
            var circle_marker_92abcb232ad2408d9dae2cec86a616dc = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_050d885d3bc34cc7a3b5a3d63af07076 = L.popup({"maxWidth": "100%"});

        
            var html_2253e32d617b45c9907daa276fe7cf4f = $(`<div id="html_2253e32d617b45c9907daa276fe7cf4f" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="http://epubs.surrey.ac.uk/849923/">General-purpose audio tagging from noisy labels using convolutional neural networks</a><br></div>`)[0];
            popup_050d885d3bc34cc7a3b5a3d63af07076.setContent(html_2253e32d617b45c9907daa276fe7cf4f);
        

        circle_marker_92abcb232ad2408d9dae2cec86a616dc.bindPopup(popup_050d885d3bc34cc7a3b5a3d63af07076)
        ;

        
    
    
            var circle_marker_51c5320d27d540ddad9622e28f07ffb1 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1d21ba2e910845efa6d5bdd13fc7f557 = L.popup({"maxWidth": "100%"});

        
            var html_a94cb740d1a44a2b86fe7f732e4faca8 = $(`<div id="html_a94cb740d1a44a2b86fe7f732e4faca8" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933052/">Improved audio scene classification based on label-tree embeddings and convolutional neural networks</a><br></div>`)[0];
            popup_1d21ba2e910845efa6d5bdd13fc7f557.setContent(html_a94cb740d1a44a2b86fe7f732e4faca8);
        

        circle_marker_51c5320d27d540ddad9622e28f07ffb1.bindPopup(popup_1d21ba2e910845efa6d5bdd13fc7f557)
        ;

        
    
    
            var circle_marker_18a9d863a3a8495ba66377a408466970 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e4922aa5bab24326bd624517c33b430a = L.popup({"maxWidth": "100%"});

        
            var html_194ad47b359547a1b3ea18d60a1f99fc = $(`<div id="html_194ad47b359547a1b3ea18d60a1f99fc" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9115871/">Learning Hierarchy Aware Embedding From Raw Audio for Acoustic Scene Classification</a><br></div>`)[0];
            popup_e4922aa5bab24326bd624517c33b430a.setContent(html_194ad47b359547a1b3ea18d60a1f99fc);
        

        circle_marker_18a9d863a3a8495ba66377a408466970.bindPopup(popup_e4922aa5bab24326bd624517c33b430a)
        ;

        
    
    
            var circle_marker_4b177f3b26e34bd4bea1d8c571648ead = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_86f6e36d75cd40c5848debf25908ef1a = L.popup({"maxWidth": "100%"});

        
            var html_c9150f5c0c99454c8c67e074147b717c = $(`<div id="html_c9150f5c0c99454c8c67e074147b717c" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="">Linking open public datasets for multifaceted music discovery</a><br></div>`)[0];
            popup_86f6e36d75cd40c5848debf25908ef1a.setContent(html_c9150f5c0c99454c8c67e074147b717c);
        

        circle_marker_4b177f3b26e34bd4bea1d8c571648ead.bindPopup(popup_86f6e36d75cd40c5848debf25908ef1a)
        ;

        
    
    
            var circle_marker_3d21f110c18540c59844332bdc3e735e = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_82af4cc4dec847b1b9ec21db7d0e5e78 = L.popup({"maxWidth": "100%"});

        
            var html_20ff07adcc4747f29f9dbb1250489888 = $(`<div id="html_20ff07adcc4747f29f9dbb1250489888" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/1707.04678">Lyrics-based music genre classification using a hierarchical attention network</a><br></div>`)[0];
            popup_82af4cc4dec847b1b9ec21db7d0e5e78.setContent(html_20ff07adcc4747f29f9dbb1250489888);
        

        circle_marker_3d21f110c18540c59844332bdc3e735e.bindPopup(popup_82af4cc4dec847b1b9ec21db7d0e5e78)
        ;

        
    
    
            var circle_marker_4333629a97e84ccebc7c33d5bfbc98bc = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b340270ce0a047039428c65f7fa78dcc = L.popup({"maxWidth": "100%"});

        
            var html_d15a9f7b2d7746a3a9c3d511492cfea4 = $(`<div id="html_d15a9f7b2d7746a3a9c3d511492cfea4" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3356590.3356611">Modelling Musical Similarity for Drum Patterns: A Perceptual Evaluation</a><br></div>`)[0];
            popup_b340270ce0a047039428c65f7fa78dcc.setContent(html_d15a9f7b2d7746a3a9c3d511492cfea4);
        

        circle_marker_4333629a97e84ccebc7c33d5bfbc98bc.bindPopup(popup_b340270ce0a047039428c65f7fa78dcc)
        ;

        
    
    
            var circle_marker_ce6a0c0cb79e44318784f4163d946fe5 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8670103fd68e4b94b10c1d252ab873c1 = L.popup({"maxWidth": "100%"});

        
            var html_17d4deff126940b484e793f212b3225e = $(`<div id="html_17d4deff126940b484e793f212b3225e" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.jstage.jst.go.jp/article/transinf/E102.D/10/E102.D_2019EDL8062/_article/-char/ja/">Multi Model-Based Distillation for Sound Event Detection</a><br></div>`)[0];
            popup_8670103fd68e4b94b10c1d252ab873c1.setContent(html_17d4deff126940b484e793f212b3225e);
        

        circle_marker_ce6a0c0cb79e44318784f4163d946fe5.bindPopup(popup_8670103fd68e4b94b10c1d252ab873c1)
        ;

        
    
    
            var circle_marker_14ebf39bf7d042058693891523532eb0 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_39837886184e46108012de54821081be = L.popup({"maxWidth": "100%"});

        
            var html_c02721d8f5914141a5e1f6e1d7776457 = $(`<div id="html_c02721d8f5914141a5e1f6e1d7776457" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/2002.04502">Robust Acoustic Scene Classification using a Multi-Spectrogram Encoder-Decoder Framework</a><br></div>`)[0];
            popup_39837886184e46108012de54821081be.setContent(html_c02721d8f5914141a5e1f6e1d7776457);
        

        circle_marker_14ebf39bf7d042058693891523532eb0.bindPopup(popup_39837886184e46108012de54821081be)
        ;

        
    
    
            var circle_marker_b9b0fb1d6b9643c3a6be1c07ac0c3198 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a7a2d9e3ef15462ea3ceb87253527878 = L.popup({"maxWidth": "100%"});

        
            var html_511b9f7e4a454f0bb6a4b364018de715 = $(`<div id="html_511b9f7e4a454f0bb6a4b364018de715" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Amiriparian_173.pdf">Sequence to sequence autoencoders for unsupervised representation learning from audio</a><br></div>`)[0];
            popup_a7a2d9e3ef15462ea3ceb87253527878.setContent(html_511b9f7e4a454f0bb6a4b364018de715);
        

        circle_marker_b9b0fb1d6b9643c3a6be1c07ac0c3198.bindPopup(popup_a7a2d9e3ef15462ea3ceb87253527878)
        ;

        
    
    
            var circle_marker_3dbe0318c60d4d628130f0c03f0f15f0 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_bc9537b329ce4fda9ea6f8338c2d7321 = L.popup({"maxWidth": "100%"});

        
            var html_a3aff9f4c21c4987b9a95f9007e9beeb = $(`<div id="html_a3aff9f4c21c4987b9a95f9007e9beeb" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.academia.edu/download/56171528/Amiriparian17-SSC.PDF">Snore Sound Classification Using Image-Based Deep Spectrum Features.</a><br></div>`)[0];
            popup_bc9537b329ce4fda9ea6f8338c2d7321.setContent(html_a3aff9f4c21c4987b9a95f9007e9beeb);
        

        circle_marker_3dbe0318c60d4d628130f0c03f0f15f0.bindPopup(popup_bc9537b329ce4fda9ea6f8338c2d7321)
        ;

        
    
    
            var circle_marker_cac1c6ae66164e6ebd070b6cda9daec5 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_cb8c4766f87649cbb5b7505134b91e83 = L.popup({"maxWidth": "100%"});

        
            var html_17a9f84b1b544574bd8d30e40febf00a = $(`<div id="html_17a9f84b1b544574bd8d30e40febf00a" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9165887/">Sound event detection of weakly labelled data with CNN-transformer and automatic threshold optimization</a><br></div>`)[0];
            popup_cb8c4766f87649cbb5b7505134b91e83.setContent(html_17a9f84b1b544574bd8d30e40febf00a);
        

        circle_marker_cac1c6ae66164e6ebd070b6cda9daec5.bindPopup(popup_cb8c4766f87649cbb5b7505134b91e83)
        ;

        
    
    
            var circle_marker_341fb55d357e493297a3889929c4df67 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c736084f344f43fc96e0656001a9b47c = L.popup({"maxWidth": "100%"});

        
            var html_d594c5e74dcf4d54a304e781cbc4a059 = $(`<div id="html_d594c5e74dcf4d54a304e781cbc4a059" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="http://epubs.surrey.ac.uk/853328/">Sound event detection with weakly labelled data</a><br></div>`)[0];
            popup_c736084f344f43fc96e0656001a9b47c.setContent(html_d594c5e74dcf4d54a304e781cbc4a059);
        

        circle_marker_341fb55d357e493297a3889929c4df67.bindPopup(popup_c736084f344f43fc96e0656001a9b47c)
        ;

        
    
    
            var circle_marker_8c0fb09c0f8f4945946ab9fec396aadf = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_aed867b7820c49c293705d8aea7fdb6f = L.popup({"maxWidth": "100%"});

        
            var html_38cccf3aeae24ce8acf2bbb376b02229 = $(`<div id="html_38cccf3aeae24ce8acf2bbb376b02229" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8492416/">Transfer learning for wearable long-term social speech evaluations</a><br></div>`)[0];
            popup_aed867b7820c49c293705d8aea7fdb6f.setContent(html_38cccf3aeae24ce8acf2bbb376b02229);
        

        circle_marker_8c0fb09c0f8f4945946ab9fec396aadf.bindPopup(popup_aed867b7820c49c293705d8aea7fdb6f)
        ;

        
    
    
            var circle_marker_6b0c350c46ab4804956d67a4b5be8f9a = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_dcda02e9da7c42388a2918855603e432 = L.popup({"maxWidth": "100%"});

        
            var html_a95554c6cbc74699bbf483bc7de9939d = $(`<div id="html_a95554c6cbc74699bbf483bc7de9939d" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://deepai.org/publication/weakly-labelled-audioset-classification-with-attention-neural-networks">Weakly labelled audioset classification with attention neural networks</a><br></div>`)[0];
            popup_dcda02e9da7c42388a2918855603e432.setContent(html_a95554c6cbc74699bbf483bc7de9939d);
        

        circle_marker_6b0c350c46ab4804956d67a4b5be8f9a.bindPopup(popup_dcda02e9da7c42388a2918855603e432)
        ;

        
    
    
            var circle_marker_87462034f66146dfa912e84087d5f9f5 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0475d5487ee74e0bab9afc959a8d6e9d = L.popup({"maxWidth": "100%"});

        
            var html_6053133c146e415f8a0ff80b1d5e1593 = $(`<div id="html_6053133c146e415f8a0ff80b1d5e1593" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8777125/">Weakly labelled audioset tagging with attention neural networks</a><br></div>`)[0];
            popup_0475d5487ee74e0bab9afc959a8d6e9d.setContent(html_6053133c146e415f8a0ff80b1d5e1593);
        

        circle_marker_87462034f66146dfa912e84087d5f9f5.bindPopup(popup_0475d5487ee74e0bab9afc959a8d6e9d)
        ;

        
    
    
            var circle_marker_e160e2329bbd4d9888ae84f95475e9f6 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_60c730c31ae645539b29ed56f388404d = L.popup({"maxWidth": "100%"});

        
            var html_ed5aac01ea6343c1be332acc9221e462 = $(`<div id="html_ed5aac01ea6343c1be332acc9221e462" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1807.09208">A hybrid of deep audio feature and i-vector for artist recognition</a><br></div>`)[0];
            popup_60c730c31ae645539b29ed56f388404d.setContent(html_ed5aac01ea6343c1be332acc9221e462);
        

        circle_marker_e160e2329bbd4d9888ae84f95475e9f6.bindPopup(popup_60c730c31ae645539b29ed56f388404d)
        ;

        
    
    
            var circle_marker_b5d33e751c2248cd8a0e2fcc3b7c7570 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3ed199945f3340a794a9428be446cff9 = L.popup({"maxWidth": "100%"});

        
            var html_e5ad4ffeeadd43139c1576365002e577 = $(`<div id="html_e5ad4ffeeadd43139c1576365002e577" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://search.ieice.org/bin/summary.php?id=e100-d_12_3041">A novel discriminative feature extraction for acoustic scene classification using rnn based source separation</a><br></div>`)[0];
            popup_3ed199945f3340a794a9428be446cff9.setContent(html_e5ad4ffeeadd43139c1576365002e577);
        

        circle_marker_b5d33e751c2248cd8a0e2fcc3b7c7570.bindPopup(popup_3ed199945f3340a794a9428be446cff9)
        ;

        
    
    
            var circle_marker_adda8b84370245f79a2c1d0b819254c3 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_4cdbba63471b4d98a361cd1eea1b05a3 = L.popup({"maxWidth": "100%"});

        
            var html_37a008a2449b42d68630725e11d38589 = $(`<div id="html_37a008a2449b42d68630725e11d38589" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1910.06784">Acoustic Scene Classification Based on a Large-margin Factorized CNN</a><br></div>`)[0];
            popup_4cdbba63471b4d98a361cd1eea1b05a3.setContent(html_37a008a2449b42d68630725e11d38589);
        

        circle_marker_adda8b84370245f79a2c1d0b819254c3.bindPopup(popup_4cdbba63471b4d98a361cd1eea1b05a3)
        ;

        
    
    
            var circle_marker_6ff0e17f275f4da8b7f6d4645fbe23be = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1da5c637be4b496fadae980024419e19 = L.popup({"maxWidth": "100%"});

        
            var html_67c0b78e0b874a4992ad60632c8b8990 = $(`<div id="html_67c0b78e0b874a4992ad60632c8b8990" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/2003.09164">Acoustic Scene Classification using Audio Tagging</a><br></div>`)[0];
            popup_1da5c637be4b496fadae980024419e19.setContent(html_67c0b78e0b874a4992ad60632c8b8990);
        

        circle_marker_6ff0e17f275f4da8b7f6d4645fbe23be.bindPopup(popup_1da5c637be4b496fadae980024419e19)
        ;

        
    
    
            var circle_marker_8b69c3da3d6e407796aa3489188dfbdf = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8b8b696b4ba849598088525b8b7c72d8 = L.popup({"maxWidth": "100%"});

        
            var html_8acbef07ba124a4ea37d2e9630965c7f = $(`<div id="html_8acbef07ba124a4ea37d2e9630965c7f" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1904.10135">Acoustic scene classification using teacher-student learning with soft-labels</a><br></div>`)[0];
            popup_8b8b696b4ba849598088525b8b7c72d8.setContent(html_8acbef07ba124a4ea37d2e9630965c7f);
        

        circle_marker_8b69c3da3d6e407796aa3489188dfbdf.bindPopup(popup_8b8b696b4ba849598088525b8b7c72d8)
        ;

        
    
    
            var circle_marker_9d81a7cbf0814147a1a4e1c699623e25 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_47e96103f6c14cacb9758f191f49a7b3 = L.popup({"maxWidth": "100%"});

        
            var html_85330b9defd8456386a745caabbf8c3e = $(`<div id="html_85330b9defd8456386a745caabbf8c3e" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8282240/">An acoustic monitoring system and its field trials</a><br></div>`)[0];
            popup_47e96103f6c14cacb9758f191f49a7b3.setContent(html_85330b9defd8456386a745caabbf8c3e);
        

        circle_marker_9d81a7cbf0814147a1a4e1c699623e25.bindPopup(popup_47e96103f6c14cacb9758f191f49a7b3)
        ;

        
    
    
            var circle_marker_8d07df3ce5a341b8b36ea334126d7689 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9e56d6aa8f674e278fe6056523ca377d = L.popup({"maxWidth": "100%"});

        
            var html_84c9a93cf33640989e14e62875d8ff74 = $(`<div id="html_84c9a93cf33640989e14e62875d8ff74" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553423/">Anomalous sound event detection based on wavenet</a><br></div>`)[0];
            popup_9e56d6aa8f674e278fe6056523ca377d.setContent(html_84c9a93cf33640989e14e62875d8ff74);
        

        circle_marker_8d07df3ce5a341b8b36ea334126d7689.bindPopup(popup_9e56d6aa8f674e278fe6056523ca377d)
        ;

        
    
    
            var circle_marker_6a6adb0eb0ca4b4796a7d0692c3aa23e = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b797fc7cae0c4dfc87b38dc644992a11 = L.popup({"maxWidth": "100%"});

        
            var html_7f129327c7b24f8393aed551f87cfa3d = $(`<div id="html_7f129327c7b24f8393aed551f87cfa3d" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-319-93554-6_66">Complex Activity Recognition Using Polyphonic Sound Event Detection</a><br></div>`)[0];
            popup_b797fc7cae0c4dfc87b38dc644992a11.setContent(html_7f129327c7b24f8393aed551f87cfa3d);
        

        circle_marker_6a6adb0eb0ca4b4796a7d0692c3aa23e.bindPopup(popup_b797fc7cae0c4dfc87b38dc644992a11)
        ;

        
    
    
            var circle_marker_8dd298a99cf64d678d5594a615bca29d = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1c600eb207f4472daac747af92be516a = L.popup({"maxWidth": "100%"});

        
            var html_7e271a8b2ed14e0a881375ae96276e17 = $(`<div id="html_7e271a8b2ed14e0a881375ae96276e17" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Han_206.pdf">Convolutional neural networks with binaural representations and background subtraction for acoustic scene classification</a><br></div>`)[0];
            popup_1c600eb207f4472daac747af92be516a.setContent(html_7e271a8b2ed14e0a881375ae96276e17);
        

        circle_marker_8dd298a99cf64d678d5594a615bca29d.bindPopup(popup_1c600eb207f4472daac747af92be516a)
        ;

        
    
    
            var circle_marker_8dbeeae0f10f428cb18e010a4c0a00c2 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_88fb4cfa5de8497189d18bb0b7bf58d1 = L.popup({"maxWidth": "100%"});

        
            var html_4a12f79c025143a4a3262f650258e9b9 = $(`<div id="html_4a12f79c025143a4a3262f650258e9b9" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/2009.09642">DCASENET: A joint pre-trained deep neural network for detecting and classifying acoustic scenes and events</a><br></div>`)[0];
            popup_88fb4cfa5de8497189d18bb0b7bf58d1.setContent(html_4a12f79c025143a4a3262f650258e9b9);
        

        circle_marker_8dbeeae0f10f428cb18e010a4c0a00c2.bindPopup(popup_88fb4cfa5de8497189d18bb0b7bf58d1)
        ;

        
    
    
            var circle_marker_7bb3d1e51e5d42d99afcd80e19e8ff7d = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f1bf09522b1647a080047c6e28940bdd = L.popup({"maxWidth": "100%"});

        
            var html_da703cb9c14145cd935941d2b41ee38e = $(`<div id="html_da703cb9c14145cd935941d2b41ee38e" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Jung_187.pdf">DNN-based audio scene classification for DCASE 2017: dual input features, balancing cost, and stochastic data duplication</a><br></div>`)[0];
            popup_f1bf09522b1647a080047c6e28940bdd.setContent(html_da703cb9c14145cd935941d2b41ee38e);
        

        circle_marker_7bb3d1e51e5d42d99afcd80e19e8ff7d.bindPopup(popup_f1bf09522b1647a080047c6e28940bdd)
        ;

        
    
    
            var circle_marker_0adaa88399d243458508c2cfe9b9c6c2 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_18e01b31eda24aa596c46bda90debd72 = L.popup({"maxWidth": "100%"});

        
            var html_03aee7ad99024001a2db230745e1be13 = $(`<div id="html_03aee7ad99024001a2db230745e1be13" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Suh_101.pdf">Designing Acoustic Scene Classification Models with CNN Variants</a><br></div>`)[0];
            popup_18e01b31eda24aa596c46bda90debd72.setContent(html_03aee7ad99024001a2db230745e1be13);
        

        circle_marker_0adaa88399d243458508c2cfe9b9c6c2.bindPopup(popup_18e01b31eda24aa596c46bda90debd72)
        ;

        
    
    
            var circle_marker_1976cab56ac64c49a8b5da6a11a00366 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_94655c4170d5451abec9e98bfb990a0b = L.popup({"maxWidth": "100%"});

        
            var html_cc12b8e9aa6242ada05a1481bd70423f = $(`<div id="html_cc12b8e9aa6242ada05a1481bd70423f" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7952181/">Detection of anomaly acoustic scenes based on a temporal dissimilarity model</a><br></div>`)[0];
            popup_94655c4170d5451abec9e98bfb990a0b.setContent(html_cc12b8e9aa6242ada05a1481bd70423f);
        

        circle_marker_1976cab56ac64c49a8b5da6a11a00366.bindPopup(popup_94655c4170d5451abec9e98bfb990a0b)
        ;

        
    
    
            var circle_marker_bbd9208bab99405f9fd682dba98e6868 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9795dff281584e7cbaeb5d8ccf3a7eca = L.popup({"maxWidth": "100%"});

        
            var html_d7440f2fb64942958e784e4db4a16704 = $(`<div id="html_d7440f2fb64942958e784e4db4a16704" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683514/">Domain mismatch robust acoustic scene classification using channel information conversion</a><br></div>`)[0];
            popup_9795dff281584e7cbaeb5d8ccf3a7eca.setContent(html_d7440f2fb64942958e784e4db4a16704);
        

        circle_marker_bbd9208bab99405f9fd682dba98e6868.bindPopup(popup_9795dff281584e7cbaeb5d8ccf3a7eca)
        ;

        
    
    
            var circle_marker_50f7f9d946b44db9aebd71c9f80c58aa = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1d69d67f8dab4069b3d1ae549d7d26f8 = L.popup({"maxWidth": "100%"});

        
            var html_639dc347bdbf428eb26287c52c8a5636 = $(`<div id="html_639dc347bdbf428eb26287c52c8a5636" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Kim_21.pdf">GIST_WISENETAI AUDIO TAGGER BASED ON CONCATENATED RESIDUAL NETWORK FOR DCASE 2018 CHALLENGE TASK 2</a><br></div>`)[0];
            popup_1d69d67f8dab4069b3d1ae549d7d26f8.setContent(html_639dc347bdbf428eb26287c52c8a5636);
        

        circle_marker_50f7f9d946b44db9aebd71c9f80c58aa.bindPopup(popup_1d69d67f8dab4069b3d1ae549d7d26f8)
        ;

        
    
    
            var circle_marker_5a529183991545dea5bb3b25fba249b5 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_44348a10285e4517b26991bd68980067 = L.popup({"maxWidth": "100%"});

        
            var html_b1c631e8a1bf4b84a5a10f0ef9985059 = $(`<div id="html_b1c631e8a1bf4b84a5a10f0ef9985059" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Mun_213.pdf">Generative adversarial network based acoustic scene training set augmentation and selection using SVM hyper-plane</a><br></div>`)[0];
            popup_44348a10285e4517b26991bd68980067.setContent(html_b1c631e8a1bf4b84a5a10f0ef9985059);
        

        circle_marker_5a529183991545dea5bb3b25fba249b5.bindPopup(popup_44348a10285e4517b26991bd68980067)
        ;

        
    
    
            var circle_marker_f31ff13d2bc7461b932eeda20dc03a8d = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b65edfe051b24ae6889be5bc51e6df0c = L.popup({"maxWidth": "100%"});

        
            var html_04edb81270ec4f53890760087142a5a0 = $(`<div id="html_04edb81270ec4f53890760087142a5a0" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://www.jstage.jst.go.jp/article/transinf/E101.D/12/E101.D_2018EDP7140/_article/-char/ja/">Hidden singer: Distinguishing imitation singers based on training with only the original song</a><br></div>`)[0];
            popup_b65edfe051b24ae6889be5bc51e6df0c.setContent(html_04edb81270ec4f53890760087142a5a0);
        

        circle_marker_f31ff13d2bc7461b932eeda20dc03a8d.bindPopup(popup_b65edfe051b24ae6889be5bc51e6df0c)
        ;

        
    
    
            var circle_marker_38cee658ff2f4465b3c98b7494a0ce35 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_526b2c31260349a2a905f3324af63929 = L.popup({"maxWidth": "100%"});

        
            var html_3dcc3971d267448ca9583d5a0a8286f9 = $(`<div id="html_3dcc3971d267448ca9583d5a0a8286f9" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9186616/">Knowledge Distillation in Acoustic Scene Classification</a><br></div>`)[0];
            popup_526b2c31260349a2a905f3324af63929.setContent(html_3dcc3971d267448ca9583d5a0a8286f9);
        

        circle_marker_38cee658ff2f4465b3c98b7494a0ce35.bindPopup(popup_526b2c31260349a2a905f3324af63929)
        ;

        
    
    
            var circle_marker_828b9e6c74cc4da9889b24fec3a0fc59 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8981f24062cd44f6aa7947bbec3511c9 = L.popup({"maxWidth": "100%"});

        
            var html_766d379669e34ba68ced1ad0daa5ad9b = $(`<div id="html_766d379669e34ba68ced1ad0daa5ad9b" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Kim_87.pdf">MULTI-CHANNEL FEATURE USING INTER-CLASS AND INTER-DEVICE STANDARD DEVIATIONS FOR ACOUSTIC SCENE CLASSIFICATION</a><br></div>`)[0];
            popup_8981f24062cd44f6aa7947bbec3511c9.setContent(html_766d379669e34ba68ced1ad0daa5ad9b);
        

        circle_marker_828b9e6c74cc4da9889b24fec3a0fc59.bindPopup(popup_8981f24062cd44f6aa7947bbec3511c9)
        ;

        
    
    
            var circle_marker_3de0fd7c0f7a4cf7a06cacf39b11756a = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e35d99cc40664d2f8c1cbbd5c79f7471 = L.popup({"maxWidth": "100%"});

        
            var html_565ce0da5a8f42068e6c460f5707f114 = $(`<div id="html_565ce0da5a8f42068e6c460f5707f114" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/2007.05191">Overcoming label noise in audio event detection using sequential labeling</a><br></div>`)[0];
            popup_e35d99cc40664d2f8c1cbbd5c79f7471.setContent(html_565ce0da5a8f42068e6c460f5707f114);
        

        circle_marker_3de0fd7c0f7a4cf7a06cacf39b11756a.bindPopup(popup_e35d99cc40664d2f8c1cbbd5c79f7471)
        ;

        
    
    
            var circle_marker_64906d0d9d85452fb6e25744fe7b4025 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_54bacb3981314d02a97a7ec99e9b7510 = L.popup({"maxWidth": "100%"});

        
            var html_23f55fde08cb4936a9d98492894c326f = $(`<div id="html_23f55fde08cb4936a9d98492894c326f" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1710.06648">Representation learning of music using artist labels</a><br></div>`)[0];
            popup_54bacb3981314d02a97a7ec99e9b7510.setContent(html_23f55fde08cb4936a9d98492894c326f);
        

        circle_marker_64906d0d9d85452fb6e25744fe7b4025.bindPopup(popup_54bacb3981314d02a97a7ec99e9b7510)
        ;

        
    
    
            var circle_marker_b211ee33c96d4f1a8339843200037cf2 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ed220e40efe5459fb0eadc40bf08c8e2 = L.popup({"maxWidth": "100%"});

        
            var html_f0baa390861a4b74ad49b8bef9e54977 = $(`<div id="html_f0baa390861a4b74ad49b8bef9e54977" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683068/">Scene-dependent Anomalous Acoustic-event Detection Based on Conditional Wavenet and I-vector</a><br></div>`)[0];
            popup_ed220e40efe5459fb0eadc40bf08c8e2.setContent(html_f0baa390861a4b74ad49b8bef9e54977);
        

        circle_marker_b211ee33c96d4f1a8339843200037cf2.bindPopup(popup_ed220e40efe5459fb0eadc40bf08c8e2)
        ;

        
    
    
            var circle_marker_02aa977fe37f42f29510d7a8797e1aa2 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_aa2d8331bb5b4f0087697854b0b2c8fa = L.popup({"maxWidth": "100%"});

        
            var html_d1f17863c52c456fbe94e2fb93048a96 = $(`<div id="html_d1f17863c52c456fbe94e2fb93048a96" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Lim_77.pdf">Sound event detection in domestic environments using ensemble of convolutional recurrent neural networks</a><br></div>`)[0];
            popup_aa2d8331bb5b4f0087697854b0b2c8fa.setContent(html_d1f17863c52c456fbe94e2fb93048a96);
        

        circle_marker_02aa977fe37f42f29510d7a8797e1aa2.bindPopup(popup_aa2d8331bb5b4f0087697854b0b2c8fa)
        ;

        
    
    
            var circle_marker_0eba7db5b78c4672ba528a412dccc86a = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_edf1bd37ebc542c29664a4f218ccf534 = L.popup({"maxWidth": "100%"});

        
            var html_7b9810c171324604a570620fe1de3ac7 = $(`<div id="html_7b9810c171324604a570620fe1de3ac7" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60743">SpecAugment for sound event detection in domestic environments using ensemble of convolutional recurrent neural networks</a><br></div>`)[0];
            popup_edf1bd37ebc542c29664a4f218ccf534.setContent(html_7b9810c171324604a570620fe1de3ac7);
        

        circle_marker_0eba7db5b78c4672ba528a412dccc86a.bindPopup(popup_edf1bd37ebc542c29664a4f218ccf534)
        ;

        
    
    
            var circle_marker_01440a88a1c442e5b1a50a9cd895caa9 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_58ae17d0f6ed4249a234e9d6a558beff = L.popup({"maxWidth": "100%"});

        
            var html_018fd55bd34243bcb6fff3ef3122cd4e = $(`<div id="html_018fd55bd34243bcb6fff3ef3122cd4e" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1910.06790">Weakly Labeled Sound Event Detection using Tri-training and Adversarial Learning</a><br></div>`)[0];
            popup_58ae17d0f6ed4249a234e9d6a558beff.setContent(html_018fd55bd34243bcb6fff3ef3122cd4e);
        

        circle_marker_01440a88a1c442e5b1a50a9cd895caa9.bindPopup(popup_58ae17d0f6ed4249a234e9d6a558beff)
        ;

        
    
    
            var circle_marker_7c21438de1a140a49952a99a00138dd0 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a4f2b72642e34689b799bcc917612a0a = L.popup({"maxWidth": "100%"});

        
            var html_980fa07c80de43c3b602fb24c2a65662 = $(`<div id="html_980fa07c80de43c3b602fb24c2a65662" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Lim_115.pdf">Weakly labeled semi-supervised sound event detection using crnn with inception module</a><br></div>`)[0];
            popup_a4f2b72642e34689b799bcc917612a0a.setContent(html_980fa07c80de43c3b602fb24c2a65662);
        

        circle_marker_7c21438de1a140a49952a99a00138dd0.bindPopup(popup_a4f2b72642e34689b799bcc917612a0a)
        ;

        
    
    
            var circle_marker_f75c3a0ac2124baa92cb4f3ae8b1d75a = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_aabdc4876a054fddaa14b43a91d922e7 = L.popup({"maxWidth": "100%"});

        
            var html_1959f2f9b1574c7cbe52b7ceea6620f7 = $(`<div id="html_1959f2f9b1574c7cbe52b7ceea6620f7" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/1909.12598">" Best-of-Many-Samples" Distribution Matching</a><br></div>`)[0];
            popup_aabdc4876a054fddaa14b43a91d922e7.setContent(html_1959f2f9b1574c7cbe52b7ceea6620f7);
        

        circle_marker_f75c3a0ac2124baa92cb4f3ae8b1d75a.bindPopup(popup_aabdc4876a054fddaa14b43a91d922e7)
        ;

        
    
    
            var circle_marker_dadadbd7aede4a2b8ab7b1e2ab4080bd = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e5dfff48720f4882aba5e6624c059476 = L.popup({"maxWidth": "100%"});

        
            var html_076723fea75e464bb026bf3ea722c146 = $(`<div id="html_076723fea75e464bb026bf3ea722c146" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553225/">A fusion of deep convolutional generative adversarial networks and sequence to sequence autoencoders for acoustic scene classification</a><br></div>`)[0];
            popup_e5dfff48720f4882aba5e6624c059476.setContent(html_076723fea75e464bb026bf3ea722c146);
        

        circle_marker_dadadbd7aede4a2b8ab7b1e2ab4080bd.bindPopup(popup_e5dfff48720f4882aba5e6624c059476)
        ;

        
    
    
            var circle_marker_17928ca9207642e0b25a22bbce772815 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f9e20b077cbb4311a5d73b11c90256d4 = L.popup({"maxWidth": "100%"});

        
            var html_2c96f6e8f33b48f5a5d9b9fdb4a00d6f = $(`<div id="html_2c96f6e8f33b48f5a5d9b9fdb4a00d6f" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://www.mdpi.com/2076-3417/10/6/2020">A review of deep learning based methods for acoustic scene classification</a><br></div>`)[0];
            popup_f9e20b077cbb4311a5d73b11c90256d4.setContent(html_2c96f6e8f33b48f5a5d9b9fdb4a00d6f);
        

        circle_marker_17928ca9207642e0b25a22bbce772815.bindPopup(popup_f9e20b077cbb4311a5d73b11c90256d4)
        ;

        
    
    
            var circle_marker_0fa0b7cda958471e9197a544357f10f6 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_22b8c02c3b014cf8ae966f383db38151 = L.popup({"maxWidth": "100%"});

        
            var html_afb23f68f4694dcb9a4e3b599f8bffb4 = $(`<div id="html_afb23f68f4694dcb9a4e3b599f8bffb4" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/1703.04770">Audio scene classification with deep recurrent neural networks</a><br></div>`)[0];
            popup_22b8c02c3b014cf8ae966f383db38151.setContent(html_afb23f68f4694dcb9a4e3b599f8bffb4);
        

        circle_marker_0fa0b7cda958471e9197a544357f10f6.bindPopup(popup_22b8c02c3b014cf8ae966f383db38151)
        ;

        
    
    
            var circle_marker_1a891630ae07492ca5fc4b7729732860 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_30735c44dcc4437d9db57b9799660d1b = L.popup({"maxWidth": "100%"});

        
            var html_db79ce20bc2041a98c6fe38c88b97a14 = $(`<div id="html_db79ce20bc2041a98c6fe38c88b97a14" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/1811.01095">Beyond equal-length snippets: How long is sufficient to recognize an audio scene?</a><br></div>`)[0];
            popup_30735c44dcc4437d9db57b9799660d1b.setContent(html_db79ce20bc2041a98c6fe38c88b97a14);
        

        circle_marker_1a891630ae07492ca5fc4b7729732860.bindPopup(popup_30735c44dcc4437d9db57b9799660d1b)
        ;

        
    
    
            var circle_marker_3397bf3439704091b6358705b97bd492 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_760d734cf2be486c850e93015d34e314 = L.popup({"maxWidth": "100%"});

        
            var html_e619a91a61b24311800ca7034e195b50 = $(`<div id="html_e619a91a61b24311800ca7034e195b50" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9052950/">Beyond the Dcase 2017 Challenge on Rare Sound Event Detection: A Proposal for a More Realistic Training and Test Framework</a><br></div>`)[0];
            popup_760d734cf2be486c850e93015d34e314.setContent(html_e619a91a61b24311800ca7034e195b50);
        

        circle_marker_3397bf3439704091b6358705b97bd492.bindPopup(popup_760d734cf2be486c850e93015d34e314)
        ;

        
    
    
            var circle_marker_06cea35949c1483092c0c85281458266 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0e7d0fb2a18c48a19228ca95056e132d = L.popup({"maxWidth": "100%"});

        
            var html_70bdd1a8cb294b37893db52a72279444 = $(`<div id="html_70bdd1a8cb294b37893db52a72279444" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://mediatum.ub.tum.de/1463108">Deep representation learning techniques for audio signal processing</a><br></div>`)[0];
            popup_0e7d0fb2a18c48a19228ca95056e132d.setContent(html_70bdd1a8cb294b37893db52a72279444);
        

        circle_marker_06cea35949c1483092c0c85281458266.bindPopup(popup_0e7d0fb2a18c48a19228ca95056e132d)
        ;

        
    
    
            var circle_marker_48f85ccc4b6744d39af8993da59fe7a1 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a9dc2f3aa2314997bde288c84e3363f7 = L.popup({"maxWidth": "100%"});

        
            var html_64aa7de0e55a4413934284a116faf6f7 = $(`<div id="html_64aa7de0e55a4413934284a116faf6f7" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933052/">Improved audio scene classification based on label-tree embeddings and convolutional neural networks</a><br></div>`)[0];
            popup_a9dc2f3aa2314997bde288c84e3363f7.setContent(html_64aa7de0e55a4413934284a116faf6f7);
        

        circle_marker_48f85ccc4b6744d39af8993da59fe7a1.bindPopup(popup_a9dc2f3aa2314997bde288c84e3363f7)
        ;

        
    
    
            var circle_marker_515ade20f8c141e59d392ddd9d362d72 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_91323a1207514eaa9f01969a55522d3c = L.popup({"maxWidth": "100%"});

        
            var html_fb0ca03f0aea4b08bf4f09f16582fbdf = $(`<div id="html_fb0ca03f0aea4b08bf4f09f16582fbdf" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60772">Open-set acoustic scene classification with deep convolutional autoencoders</a><br></div>`)[0];
            popup_91323a1207514eaa9f01969a55522d3c.setContent(html_fb0ca03f0aea4b08bf4f09f16582fbdf);
        

        circle_marker_515ade20f8c141e59d392ddd9d362d72.bindPopup(popup_91323a1207514eaa9f01969a55522d3c)
        ;

        
    
    
            var circle_marker_02a2e5a93fa74af4ab1f58d98c33be32 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a43076e1c5c64a4ab486b7a26bdc8bfd = L.popup({"maxWidth": "100%"});

        
            var html_51a16fc5f0424c3884fdabf61fab2cbe = $(`<div id="html_51a16fc5f0424c3884fdabf61fab2cbe" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7952262/">Reverberation-based feature extraction for acoustic scene classification</a><br></div>`)[0];
            popup_a43076e1c5c64a4ab486b7a26bdc8bfd.setContent(html_51a16fc5f0424c3884fdabf61fab2cbe);
        

        circle_marker_02a2e5a93fa74af4ab1f58d98c33be32.bindPopup(popup_a43076e1c5c64a4ab486b7a26bdc8bfd)
        ;

        
    
    
            var circle_marker_073dc1807dd84e588771c676f77eef12 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_cfed4c32f0134246921aeb9756ba56da = L.popup({"maxWidth": "100%"});

        
            var html_10309cdb18da4c45a9b201aa5528e72a = $(`<div id="html_10309cdb18da4c45a9b201aa5528e72a" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933059/">Robust detection of environmental sounds in binaural auditory scenes</a><br></div>`)[0];
            popup_cfed4c32f0134246921aeb9756ba56da.setContent(html_10309cdb18da4c45a9b201aa5528e72a);
        

        circle_marker_073dc1807dd84e588771c676f77eef12.bindPopup(popup_cfed4c32f0134246921aeb9756ba56da)
        ;

        
    
    
            var circle_marker_63027d5d991d4fa1938cacc310b1bb2e = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3d06280d9df048dba6e55d97055b1e49 = L.popup({"maxWidth": "100%"});

        
            var html_5f55057c2d074b6f9fb98429077175d6 = $(`<div id="html_5f55057c2d074b6f9fb98429077175d6" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Amiriparian_173.pdf">Sequence to sequence autoencoders for unsupervised representation learning from audio</a><br></div>`)[0];
            popup_3d06280d9df048dba6e55d97055b1e49.setContent(html_5f55057c2d074b6f9fb98429077175d6);
        

        circle_marker_63027d5d991d4fa1938cacc310b1bb2e.bindPopup(popup_3d06280d9df048dba6e55d97055b1e49)
        ;

        
    
    
            var circle_marker_185822fd2ca64a158b048e00fffb9661 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_87d85bcb4aed4a808e7c1d207d4c9573 = L.popup({"maxWidth": "100%"});

        
            var html_853cdb6b686548658b0f21fe313d80d2 = $(`<div id="html_853cdb6b686548658b0f21fe313d80d2" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://www.academia.edu/download/56171528/Amiriparian17-SSC.PDF">Snore Sound Classification Using Image-Based Deep Spectrum Features.</a><br></div>`)[0];
            popup_87d85bcb4aed4a808e7c1d207d4c9573.setContent(html_853cdb6b686548658b0f21fe313d80d2);
        

        circle_marker_185822fd2ca64a158b048e00fffb9661.bindPopup(popup_87d85bcb4aed4a808e7c1d207d4c9573)
        ;

        
    
    
            var circle_marker_8dec4ff4025d42c1b1cf63b8e3295a9f = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_20452ca81dc24b6eb7b5eb57341813c6 = L.popup({"maxWidth": "100%"});

        
            var html_50ba72233b164d54884bbff6c97b4393 = $(`<div id="html_50ba72233b164d54884bbff6c97b4393" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://dl.gi.de/handle/20.500.12116/4113">Soundslike</a><br></div>`)[0];
            popup_20452ca81dc24b6eb7b5eb57341813c6.setContent(html_50ba72233b164d54884bbff6c97b4393);
        

        circle_marker_8dec4ff4025d42c1b1cf63b8e3295a9f.bindPopup(popup_20452ca81dc24b6eb7b5eb57341813c6)
        ;

        
    
    
            var circle_marker_fc5d9841f6e74f3c9730ee46ad7bc87b = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_bc299dba07544ea7afcdff7d4036234a = L.popup({"maxWidth": "100%"});

        
            var html_d06fd2964a444c428d5b3dfcad2a11ab = $(`<div id="html_d06fd2964a444c428d5b3dfcad2a11ab" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/2008.04590">Surgical Mask Detection with Convolutional Neural Networks and Data Augmentations on Spectrograms</a><br></div>`)[0];
            popup_bc299dba07544ea7afcdff7d4036234a.setContent(html_d06fd2964a444c428d5b3dfcad2a11ab);
        

        circle_marker_fc5d9841f6e74f3c9730ee46ad7bc87b.bindPopup(popup_bc299dba07544ea7afcdff7d4036234a)
        ;

        
    
    
            var circle_marker_2f7c4d10a51447d4b5137da5cc27ace6 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1d2e445d33b645458a35e23e73da23bb = L.popup({"maxWidth": "100%"});

        
            var html_0fc824e1e4e24f7b871cecde48f5da6b = $(`<div id="html_0fc824e1e4e24f7b871cecde48f5da6b" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/334548980_The_2019_Multimedia_for_Recommender_System_Task_MovieREC_and_NewsREEL_at_MediaEval/links/5d30c3ac299bf1547cc258c1/The-2019-Multimedia-for-Recommender-System-Task-MovieREC-and-NewsREEL-at-MediaEval.pdf">The 2019 Multimedia for Recommender System Task: MovieREC and NewsREEL at MediaEval</a><br></div>`)[0];
            popup_1d2e445d33b645458a35e23e73da23bb.setContent(html_0fc824e1e4e24f7b871cecde48f5da6b);
        

        circle_marker_2f7c4d10a51447d4b5137da5cc27ace6.bindPopup(popup_1d2e445d33b645458a35e23e73da23bb)
        ;

        
    
    
            var circle_marker_be705f486821498db6a99a273d06c102 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_99910b51dacb4be4806d510a4b43d182 = L.popup({"maxWidth": "100%"});

        
            var html_30a220afafb94508af809dfcbba9488f = $(`<div id="html_30a220afafb94508af809dfcbba9488f" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/2005.00145">Unsupervised Domain Adaptation for Acoustic Scene Classification Using Band-Wise Statistics Matching</a><br></div>`)[0];
            popup_99910b51dacb4be4806d510a4b43d182.setContent(html_30a220afafb94508af809dfcbba9488f);
        

        circle_marker_be705f486821498db6a99a273d06c102.bindPopup(popup_99910b51dacb4be4806d510a4b43d182)
        ;

        
    
    
            var circle_marker_c4f1552b9b20461993500abce508987b = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a44910a95e0c4d849954933d9eeb369c = L.popup({"maxWidth": "100%"});

        
            var html_d6ab9556d1614f4191f5a45933243479 = $(`<div id="html_d6ab9556d1614f4191f5a45933243479" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://arxiv.org/abs/1806.07506">A simple fusion of deep and shallow learning for acoustic scene classification</a><br></div>`)[0];
            popup_a44910a95e0c4d849954933d9eeb369c.setContent(html_d6ab9556d1614f4191f5a45933243479);
        

        circle_marker_c4f1552b9b20461993500abce508987b.bindPopup(popup_a44910a95e0c4d849954933d9eeb369c)
        ;

        
    
    
            var circle_marker_f33dcdfb98834f5c99dd25a259b5456e = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a84c08f8eaac4d01b58232dd5a66e857 = L.popup({"maxWidth": "100%"});

        
            var html_7085784243054721b79e6b66d7ef8305 = $(`<div id="html_7085784243054721b79e6b66d7ef8305" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://repositori.upf.edu/handle/10230/33454">Acoustic scene classification by ensembling gradient boosting machine and convolutional neural networks</a><br></div>`)[0];
            popup_a84c08f8eaac4d01b58232dd5a66e857.setContent(html_7085784243054721b79e6b66d7ef8305);
        

        circle_marker_f33dcdfb98834f5c99dd25a259b5456e.bindPopup(popup_a84c08f8eaac4d01b58232dd5a66e857)
        ;

        
    
    
            var circle_marker_dcd163dc7fba4413bb067307674808a1 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_11aeef0595cd4d9fbe04d50b39a06f9c = L.popup({"maxWidth": "100%"});

        
            var html_f7a02571bdc347018286c5436589d730 = $(`<div id="html_f7a02571bdc347018286c5436589d730" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Gong_189.pdf">Acoustic scene classification by fusing LightGBM and VGG-net multichannel predictions</a><br></div>`)[0];
            popup_11aeef0595cd4d9fbe04d50b39a06f9c.setContent(html_f7a02571bdc347018286c5436589d730);
        

        circle_marker_dcd163dc7fba4413bb067307674808a1.bindPopup(popup_11aeef0595cd4d9fbe04d50b39a06f9c)
        ;

        
    
    
            var circle_marker_7f4fef64527c4dfdbf693d8745a00681 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ff702d179b5b4c1ba088b66521017fa8 = L.popup({"maxWidth": "100%"});

        
            var html_ae62c42129134f74b08e11334a686b5d = $(`<div id="html_ae62c42129134f74b08e11334a686b5d" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://arxiv.org/abs/1906.02975">Audio tagging with noisy labels and minimal supervision</a><br></div>`)[0];
            popup_ff702d179b5b4c1ba088b66521017fa8.setContent(html_ae62c42129134f74b08e11334a686b5d);
        

        circle_marker_7f4fef64527c4dfdbf693d8745a00681.bindPopup(popup_ff702d179b5b4c1ba088b66521017fa8)
        ;

        
    
    
            var circle_marker_89fdadeb6ff545eca2c8134cfec8381e = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0a88dba845f043a19d68ef2970106312 = L.popup({"maxWidth": "100%"});

        
            var html_c93e44f140b343b28b2e27fcb2b12be6 = $(`<div id="html_c93e44f140b343b28b2e27fcb2b12be6" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://e-archivo.uc3m.es/handle/10016/29786">Bayesian and echoic log-surprise for auditory saliency detection</a><br></div>`)[0];
            popup_0a88dba845f043a19d68ef2970106312.setContent(html_c93e44f140b343b28b2e27fcb2b12be6);
        

        circle_marker_89fdadeb6ff545eca2c8134cfec8381e.bindPopup(popup_0a88dba845f043a19d68ef2970106312)
        ;

        
    
    
            var circle_marker_1eae82e3f7fe4d35b1b5138393a18947 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ac5bd2a1415b4430becc9fea1c1a9f7b = L.popup({"maxWidth": "100%"});

        
            var html_5bfeaf45f9ec4c53acd163c7ccfb4cae = $(`<div id="html_5bfeaf45f9ec4c53acd163c7ccfb4cae" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="">DJ-Running: An Emotion-based System for Recommending Spotify Songs to Runners.</a><br></div>`)[0];
            popup_ac5bd2a1415b4430becc9fea1c1a9f7b.setContent(html_5bfeaf45f9ec4c53acd163c7ccfb4cae);
        

        circle_marker_1eae82e3f7fe4d35b1b5138393a18947.bindPopup(popup_ac5bd2a1415b4430becc9fea1c1a9f7b)
        ;

        
    
    
            var circle_marker_6129fdbbe45a413e9d9bbc4abf01c722 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_afbccd67ceab476f81019194ce573331 = L.popup({"maxWidth": "100%"});

        
            var html_f026215b9fd9436d9b0742565dccfb04 = $(`<div id="html_f026215b9fd9436d9b0742565dccfb04" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://arxiv.org/abs/1807.09902">General-purpose tagging of freesound audio with audioset labels: Task description, dataset, and baseline</a><br></div>`)[0];
            popup_afbccd67ceab476f81019194ce573331.setContent(html_f026215b9fd9436d9b0742565dccfb04);
        

        circle_marker_6129fdbbe45a413e9d9bbc4abf01c722.bindPopup(popup_afbccd67ceab476f81019194ce573331)
        ;

        
    
    
            var circle_marker_fd32f4766eea4d7c9dc12a10870abac1 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_efdf9847093a4e7685c21c0b429479ad = L.popup({"maxWidth": "100%"});

        
            var html_d320c435295448e794ca71b154efbe57 = $(`<div id="html_d320c435295448e794ca71b154efbe57" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://ccrma.stanford.edu/~urinieto/MARL/publications/ISMIR2020_MoodPrediction.pdf">MOOD CLASSIFICATION USING LISTENING DATA</a><br></div>`)[0];
            popup_efdf9847093a4e7685c21c0b429479ad.setContent(html_d320c435295448e794ca71b154efbe57);
        

        circle_marker_fd32f4766eea4d7c9dc12a10870abac1.bindPopup(popup_efdf9847093a4e7685c21c0b429479ad)
        ;

        
    
    
            var circle_marker_6ee19e04326d48488e38e53633a42748 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_cb5881cb288d4cd4b8a61fe3526fac08 = L.popup({"maxWidth": "100%"});

        
            var html_00518422047e408aa728ee6f3c416c8f = $(`<div id="html_00518422047e408aa728ee6f3c416c8f" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3298689.3347052">Music cold-start and long-tail recommendation: bias in deep representations</a><br></div>`)[0];
            popup_cb5881cb288d4cd4b8a61fe3526fac08.setContent(html_00518422047e408aa728ee6f3c416c8f);
        

        circle_marker_6ee19e04326d48488e38e53633a42748.bindPopup(popup_cb5881cb288d4cd4b8a61fe3526fac08)
        ;

        
    
    
            var circle_marker_967539a0ae254abdb2ec96d66c6b589b = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a89dc9a06481440b84b0ed7dd0493de7 = L.popup({"maxWidth": "100%"});

        
            var html_038b202d67464fcf80102247a314400d = $(`<div id="html_038b202d67464fcf80102247a314400d" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://arxiv.org/abs/1908.06708">Recommender Systems Fairness Evaluation via Generalized Cross Entropy</a><br></div>`)[0];
            popup_a89dc9a06481440b84b0ed7dd0493de7.setContent(html_038b202d67464fcf80102247a314400d);
        

        circle_marker_967539a0ae254abdb2ec96d66c6b589b.bindPopup(popup_a89dc9a06481440b84b0ed7dd0493de7)
        ;

        
    
    
            var circle_marker_69d1417a22cb48ac93a25876a0c501cd = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a59eabe03a254f95bd03cc7e3831cea9 = L.popup({"maxWidth": "100%"});

        
            var html_dc1663f9bc034125addd690dae2149ee = $(`<div id="html_dc1663f9bc034125addd690dae2149ee" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682858/">Sound event envelope estimation in polyphonic mixtures</a><br></div>`)[0];
            popup_a59eabe03a254f95bd03cc7e3831cea9.setContent(html_dc1663f9bc034125addd690dae2149ee);
        

        circle_marker_69d1417a22cb48ac93a25876a0c501cd.bindPopup(popup_a59eabe03a254f95bd03cc7e3831cea9)
        ;

        
    
    
            var circle_marker_bd3e795073a5470e915f08ea29f7436b = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_22c95fafe87c4c14936133931ecffdee = L.popup({"maxWidth": "100%"});

        
            var html_4e9296d6f88c4455a01fb817e23037c0 = $(`<div id="html_4e9296d6f88c4455a01fb817e23037c0" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Naranjo-Alcazar_34_t1.pdf">TASK 1 DCASE 2020: ASC WITH MISMATCH DEVICES AND REDUCED SIZE MODEL USING RESIDUAL SQUEEZE-EXCITATION CNNS</a><br></div>`)[0];
            popup_22c95fafe87c4c14936133931ecffdee.setContent(html_4e9296d6f88c4455a01fb817e23037c0);
        

        circle_marker_bd3e795073a5470e915f08ea29f7436b.bindPopup(popup_22c95fafe87c4c14936133931ecffdee)
        ;

        
    
    
            var circle_marker_286682f630294addbc6816fe00429d48 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9d1785c267634453ad4faea4d1befce6 = L.popup({"maxWidth": "100%"});

        
            var html_5ae9967460b742228eea7d275a6d6659 = $(`<div id="html_5ae9967460b742228eea7d275a6d6659" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-319-63450-0_4">Acoustic features for environmental sound analysis</a><br></div>`)[0];
            popup_9d1785c267634453ad4faea4d1befce6.setContent(html_5ae9967460b742228eea7d275a6d6659);
        

        circle_marker_286682f630294addbc6816fe00429d48.bindPopup(popup_9d1785c267634453ad4faea4d1befce6)
        ;

        
    
    
            var circle_marker_da024652b0cb49ba9db60032774af3cb = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_dba9d270797e4112b64b2b43c0596a37 = L.popup({"maxWidth": "100%"});

        
            var html_0067c64f9814440e95aa74f43f9b0ee4 = $(`<div id="html_0067c64f9814440e95aa74f43f9b0ee4" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://hal.archives-ouvertes.fr/tel-01912022/">Apprentissage de représentations pour l'analyse de scènes sonores</a><br></div>`)[0];
            popup_dba9d270797e4112b64b2b43c0596a37.setContent(html_0067c64f9814440e95aa74f43f9b0ee4);
        

        circle_marker_da024652b0cb49ba9db60032774af3cb.bindPopup(popup_dba9d270797e4112b64b2b43c0596a37)
        ;

        
    
    
            var circle_marker_e13556f2fc01400dbd6c59f04c158d16 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b2f51f0c821449dc8db53ea5d603c8df = L.popup({"maxWidth": "100%"});

        
            var html_f37af1c924dc47668ca305f51b8a6ac7 = $(`<div id="html_f37af1c924dc47668ca305f51b8a6ac7" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877429/">Benchmark for Kitchen20, a daily life dataset for audio-based human action recognition</a><br></div>`)[0];
            popup_b2f51f0c821449dc8db53ea5d603c8df.setContent(html_f37af1c924dc47668ca305f51b8a6ac7);
        

        circle_marker_e13556f2fc01400dbd6c59f04c158d16.bindPopup(popup_b2f51f0c821449dc8db53ea5d603c8df)
        ;

        
    
    
            var circle_marker_8693b2aff50c403e9cd47399c9bb08bf = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_add7f60a693745cdab78bbcd592fd622 = L.popup({"maxWidth": "100%"});

        
            var html_d90004a5b54c49f69a1c7e454cfa23ff = $(`<div id="html_d90004a5b54c49f69a1c7e454cfa23ff" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8852143/">Cosine-similarity penalty to discriminate sound classes in weakly-supervised sound event detection</a><br></div>`)[0];
            popup_add7f60a693745cdab78bbcd592fd622.setContent(html_d90004a5b54c49f69a1c7e454cfa23ff);
        

        circle_marker_8693b2aff50c403e9cd47399c9bb08bf.bindPopup(popup_add7f60a693745cdab78bbcd592fd622)
        ;

        
    
    
            var circle_marker_64e2d482a05342a0a6b8eea70a2373ac = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_398695682ebb4b74996f4c14ab41ffad = L.popup({"maxWidth": "100%"});

        
            var html_d1146a3c695a458593a82202e227804e = $(`<div id="html_d1146a3c695a458593a82202e227804e" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://arxiv.org/abs/1810.01807">Disambiguating music artists at scale with audio metric learning</a><br></div>`)[0];
            popup_398695682ebb4b74996f4c14ab41ffad.setContent(html_d1146a3c695a458593a82202e227804e);
        

        circle_marker_64e2d482a05342a0a6b8eea70a2373ac.bindPopup(popup_398695682ebb4b74996f4c14ab41ffad)
        ;

        
    
    
            var circle_marker_965733c739a0455b8e8086f8d2e6680e = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_af1ee38447654bbcafba3e2773cf1a2a = L.popup({"maxWidth": "100%"});

        
            var html_adeed11ca64640039f85e4faa5ed37c5 = $(`<div id="html_adeed11ca64640039f85e4faa5ed37c5" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8937143/">Evaluation of post-processing algorithms for polyphonic sound event detection</a><br></div>`)[0];
            popup_af1ee38447654bbcafba3e2773cf1a2a.setContent(html_adeed11ca64640039f85e4faa5ed37c5);
        

        circle_marker_965733c739a0455b8e8086f8d2e6680e.bindPopup(popup_af1ee38447654bbcafba3e2773cf1a2a)
        ;

        
    
    
            var circle_marker_1774659f07d44708b2a627f4cd17443f = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_dd39abebc2c046cea7f49d1341d89eb5 = L.popup({"maxWidth": "100%"});

        
            var html_ae763a46e8c340449d94b392499d51d6 = $(`<div id="html_ae763a46e8c340449d94b392499d51d6" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="http://www-poleia.lip6.fr/~cord/pdfs/publis/2018dcasecord.pdf">Exploring deep vision models for acoustic scene classification</a><br></div>`)[0];
            popup_dd39abebc2c046cea7f49d1341d89eb5.setContent(html_ae763a46e8c340449d94b392499d51d6);
        

        circle_marker_1774659f07d44708b2a627f4cd17443f.bindPopup(popup_dd39abebc2c046cea7f49d1341d89eb5)
        ;

        
    
    
            var circle_marker_8664ab374dc34fe1900494cad360f905 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f22abaf97d45484abb4e80918b1f8cf6 = L.popup({"maxWidth": "100%"});

        
            var html_429f4885d4d64a2a92364a7de71d5f35 = $(`<div id="html_429f4885d4d64a2a92364a7de71d5f35" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933045/">Feature learning with matrix factorization applied to acoustic scene classification</a><br></div>`)[0];
            popup_f22abaf97d45484abb4e80918b1f8cf6.setContent(html_429f4885d4d64a2a92364a7de71d5f35);
        

        circle_marker_8664ab374dc34fe1900494cad360f905.bindPopup(popup_f22abaf97d45484abb4e80918b1f8cf6)
        ;

        
    
    
            var circle_marker_a77a815e23dd4ce986d3a9a9589990cc = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a374a89ea1aa4a0e818006d4cecae78d = L.popup({"maxWidth": "100%"});

        
            var html_9d3fce75981a48e28dc88d1b2231b7e4 = $(`<div id="html_9d3fce75981a48e28dc88d1b2231b7e4" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8168139/">Leveraging deep neural networks with nonnegative representations for improved environmental sound classification</a><br></div>`)[0];
            popup_a374a89ea1aa4a0e818006d4cecae78d.setContent(html_9d3fce75981a48e28dc88d1b2231b7e4);
        

        circle_marker_a77a815e23dd4ce986d3a9a9589990cc.bindPopup(popup_a374a89ea1aa4a0e818006d4cecae78d)
        ;

        
    
    
            var circle_marker_76260ef22f94448da89af4f95356b448 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_37922ebadfb54af2a28a94fcc306e816 = L.popup({"maxWidth": "100%"});

        
            var html_a39a5afd236c47919d86aa7992ceea89 = $(`<div id="html_a39a5afd236c47919d86aa7992ceea89" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Cances_69.pdf">Multi task learning and post processing optimization for sound event detection</a><br></div>`)[0];
            popup_37922ebadfb54af2a28a94fcc306e816.setContent(html_a39a5afd236c47919d86aa7992ceea89);
        

        circle_marker_76260ef22f94448da89af4f95356b448.bindPopup(popup_37922ebadfb54af2a28a94fcc306e816)
        ;

        
    
    
            var circle_marker_2d9a646b8d0a47ea955af1d7ecf76b44 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8e8f32b4a00e4b2c8412476637cd96dd = L.popup({"maxWidth": "100%"});

        
            var html_f5ace96b2e67431a8b240d64c9226662 = $(`<div id="html_f5ace96b2e67431a8b240d64c9226662" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://hal.inria.fr/hal-01636627/">Nonnegative feature learning methods for acoustic scene classification</a><br></div>`)[0];
            popup_8e8f32b4a00e4b2c8412476637cd96dd.setContent(html_f5ace96b2e67431a8b240d64c9226662);
        

        circle_marker_2d9a646b8d0a47ea955af1d7ecf76b44.bindPopup(popup_8e8f32b4a00e4b2c8412476637cd96dd)
        ;

        
    
    
            var circle_marker_cacfa9c62d5d4d7ea6e861f30c6fdd4e = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_328ab3fd49ee451ab2d305b7279b8aa6 = L.popup({"maxWidth": "100%"});

        
            var html_dc80bc025b044fff999ef9655bebb9b4 = $(`<div id="html_dc80bc025b044fff999ef9655bebb9b4" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://hal.telecom-paris.fr/hal-02934433/document">SHOULD WE CONSIDER THE USERS IN CONTEXTUAL MUSIC AUTO-TAGGING MODELS?</a><br></div>`)[0];
            popup_328ab3fd49ee451ab2d305b7279b8aa6.setContent(html_dc80bc025b044fff999ef9655bebb9b4);
        

        circle_marker_cacfa9c62d5d4d7ea6e861f30c6fdd4e.bindPopup(popup_328ab3fd49ee451ab2d305b7279b8aa6)
        ;

        
    
    
            var circle_marker_651415ab87d5408caab72e7b58fc710b = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0d3ac44d39d5465e8b9639a2f1577098 = L.popup({"maxWidth": "100%"});

        
            var html_38d160bb657046fdb0dd5e181d299558 = $(`<div id="html_38d160bb657046fdb0dd5e181d299558" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683774/">Semi-supervised triplet loss based learning of ambient audio embeddings</a><br></div>`)[0];
            popup_0d3ac44d39d5465e8b9639a2f1577098.setContent(html_38d160bb657046fdb0dd5e181d299558);
        

        circle_marker_651415ab87d5408caab72e7b58fc710b.bindPopup(popup_0d3ac44d39d5465e8b9639a2f1577098)
        ;

        
    
    
            var circle_marker_e8e27924e3164276808da7cfcb763779 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e5ed8f22befe4bf89d56069611e2400d = L.popup({"maxWidth": "100%"});

        
            var html_71a3d20340dc46f88857fe5b08b158dc = $(`<div id="html_71a3d20340dc46f88857fe5b08b158dc" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://hal.inria.fr/hal-02114652/">Sound event detection from partially annotated data: Trends and challenges</a><br></div>`)[0];
            popup_e5ed8f22befe4bf89d56069611e2400d.setContent(html_71a3d20340dc46f88857fe5b08b158dc);
        

        circle_marker_e8e27924e3164276808da7cfcb763779.bindPopup(popup_e5ed8f22befe4bf89d56069611e2400d)
        ;

        
    
    
            var circle_marker_02b47781a4e54e5eb0e80a2d13582e1a = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f4cc721d990f4767b23180d9041b4829 = L.popup({"maxWidth": "100%"});

        
            var html_bfe5850965e74814ba445fce08c4b24b = $(`<div id="html_bfe5850965e74814ba445fce08c4b24b" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://trepo.tuni.fi/bitstream/handle/10024/116599/DCASE_2018_proceedings.pdf?sequence=1#page=65">Sound event detection from weak annotations: weighted-gru versus multi-instance-learning</a><br></div>`)[0];
            popup_f4cc721d990f4767b23180d9041b4829.setContent(html_bfe5850965e74814ba445fce08c4b24b);
        

        circle_marker_02b47781a4e54e5eb0e80a2d13582e1a.bindPopup(popup_f4cc721d990f4767b23180d9041b4829)
        ;

        
    
    
            var circle_marker_424133f200d44006951e9db8afdcb99f = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_96d41b2fb3f74fa3a9b42d6fc7f88f10 = L.popup({"maxWidth": "100%"});

        
            var html_0a5aaa98e406481e8b9d241c06cd00b2 = $(`<div id="html_0a5aaa98e406481e8b9d241c06cd00b2" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60771">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</a><br></div>`)[0];
            popup_96d41b2fb3f74fa3a9b42d6fc7f88f10.setContent(html_0a5aaa98e406481e8b9d241c06cd00b2);
        

        circle_marker_424133f200d44006951e9db8afdcb99f.bindPopup(popup_96d41b2fb3f74fa3a9b42d6fc7f88f10)
        ;

        
    
    
            var circle_marker_0cefb4114b4e4a24a57ae0013e511cdf = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_67a64665b17e42d6bd429a0e1d8ba682 = L.popup({"maxWidth": "100%"});

        
            var html_001b02a475b44f25a50ee0b1ebd25a0e = $(`<div id="html_001b02a475b44f25a50ee0b1ebd25a0e" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9054478/">Sound event detection in synthetic domestic environments</a><br></div>`)[0];
            popup_67a64665b17e42d6bd429a0e1d8ba682.setContent(html_001b02a475b44f25a50ee0b1ebd25a0e);
        

        circle_marker_0cefb4114b4e4a24a57ae0013e511cdf.bindPopup(popup_67a64665b17e42d6bd429a0e1d8ba682)
        ;

        
    
    
            var circle_marker_5efdbb11729c419b88e22efbbac3806e = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_14d9045815b1404b88dd1548d67ef2fd = L.popup({"maxWidth": "100%"});

        
            var html_4b9be357693d47a69d372dd320bbc908 = $(`<div id="html_4b9be357693d47a69d372dd320bbc908" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://arxiv.org/abs/2007.03931">Training sound event detection on a heterogeneous dataset</a><br></div>`)[0];
            popup_14d9045815b1404b88dd1548d67ef2fd.setContent(html_4b9be357693d47a69d372dd320bbc908);
        

        circle_marker_5efdbb11729c419b88e22efbbac3806e.bindPopup(popup_14d9045815b1404b88dd1548d67ef2fd)
        ;

        
    
    
            var circle_marker_4a83fa902ca54e6f8e19cefd81e3bef8 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2ace63d527e74bfdaf84d31141f4436f = L.popup({"maxWidth": "100%"});

        
            var html_6936ee0897b848ea8b1dcbbc8947dbb7 = $(`<div id="html_6936ee0897b848ea8b1dcbbc8947dbb7" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://opus.bibliothek.uni-augsburg.de/opus4/files/45063/DCASE_2017+-+Sequence.pdf">Workshop (DCASE2017)</a><br></div>`)[0];
            popup_2ace63d527e74bfdaf84d31141f4436f.setContent(html_6936ee0897b848ea8b1dcbbc8947dbb7);
        

        circle_marker_4a83fa902ca54e6f8e19cefd81e3bef8.bindPopup(popup_2ace63d527e74bfdaf84d31141f4436f)
        ;

        
    
    
            var circle_marker_0b4ffc0762a246a9b148bc3b28426835 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_43f9468422b2461c90385b37b49e08bb = L.popup({"maxWidth": "100%"});

        
            var html_18250a34fda2458a8eaf7a63404023f2 = $(`<div id="html_18250a34fda2458a8eaf7a63404023f2" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="http://www.academia.edu/download/62174595/ECAI_RAI_TCL20200223-31870-1j44ahp.pdf">A Knowledge-based System for the Dynamic Generation and Classification of Novel Contents in Multimedia Broadcasting</a><br></div>`)[0];
            popup_43f9468422b2461c90385b37b49e08bb.setContent(html_18250a34fda2458a8eaf7a63404023f2);
        

        circle_marker_0b4ffc0762a246a9b148bc3b28426835.bindPopup(popup_43f9468422b2461c90385b37b49e08bb)
        ;

        
    
    
            var circle_marker_5c3c7b12c8a6486cb4c635895c9a397c = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b50ce1eb7d9b4e9a9c84fd21a577e445 = L.popup({"maxWidth": "100%"});

        
            var html_91821061df7b47579535c4597100f86b = $(`<div id="html_91821061df7b47579535c4597100f86b" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.researchgate.net/profile/Hossein_A_Rahmani/publication/337768473_A_Regression_Approach_to_Movie_Rating_Prediction_using_Multimedia_Content_and_Metadata/links/5df8b821a6fdcc283726d8c7/A-Regression-Approach-to-Movie-Rating-Prediction-using-Multimedia-Content-and-Metadata.pdf?_sg%5B0%5D=3b-miF1_LzXGwxQhQxRYO5FfklSPn0YfAlrK6IWfJilrpkC8-9h2ljnRbzLhZjp8z22CM_vzJpiQ-DaxhD7RLQ.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_sg%5B1%5D=0EyWPCbfTe4cM2OzZ6VGxzinAQj21QkakmKNQ9xBAXR2oXBv7SQahoq94YN7zTBLR7KuEuKMzCqZtGzBOqJvKV6rX2V1E4ISvHGuBwdOwd-p.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_iepl=">A Regression Approach to Movie Rating Prediction using Multimedia Content and Metadata</a><br></div>`)[0];
            popup_b50ce1eb7d9b4e9a9c84fd21a577e445.setContent(html_91821061df7b47579535c4597100f86b);
        

        circle_marker_5c3c7b12c8a6486cb4c635895c9a397c.bindPopup(popup_b50ce1eb7d9b4e9a9c84fd21a577e445)
        ;

        
    
    
            var circle_marker_e4b3300e29bf49b2a3bd74980e4a2f96 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1953522f20f94895b948d9b7868c81cd = L.popup({"maxWidth": "100%"});

        
            var html_aa1f641f07e5467bbf845f9519a1b7cb = $(`<div id="html_aa1f641f07e5467bbf845f9519a1b7cb" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7966035/">A convolutional neural network approach for acoustic scene classification</a><br></div>`)[0];
            popup_1953522f20f94895b948d9b7868c81cd.setContent(html_aa1f641f07e5467bbf845f9519a1b7cb);
        

        circle_marker_e4b3300e29bf49b2a3bd74980e4a2f96.bindPopup(popup_1953522f20f94895b948d9b7868c81cd)
        ;

        
    
    
            var circle_marker_d26c456be62445aa8cfe75f1af8d1b38 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d83c4a754bbd4b4fa596c7be3e8d342c = L.popup({"maxWidth": "100%"});

        
            var html_8a66199283d046dd9ef98ab1c61cc0c4 = $(`<div id="html_8a66199283d046dd9ef98ab1c61cc0c4" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/2005.10322">Adversarial Machine Learning in Recommender Systems: State of the art and Challenges</a><br></div>`)[0];
            popup_d83c4a754bbd4b4fa596c7be3e8d342c.setContent(html_8a66199283d046dd9ef98ab1c61cc0c4);
        

        circle_marker_d26c456be62445aa8cfe75f1af8d1b38.bindPopup(popup_d83c4a754bbd4b4fa596c7be3e8d342c)
        ;

        
    
    
            var circle_marker_b09b307ba7094516b66674bdae43717b = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d4ff6d9b595c4e32bbee85ecc7c95e91 = L.popup({"maxWidth": "100%"});

        
            var html_b4c015297b8b41ad8d62993ca5069bc7 = $(`<div id="html_b4c015297b8b41ad8d62993ca5069bc7" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3336191.3371877">Adversarial machine learning in recommender systems (aml-recsys)</a><br></div>`)[0];
            popup_d4ff6d9b595c4e32bbee85ecc7c95e91.setContent(html_b4c015297b8b41ad8d62993ca5069bc7);
        

        circle_marker_b09b307ba7094516b66674bdae43717b.bindPopup(popup_d4ff6d9b595c4e32bbee85ecc7c95e91)
        ;

        
    
    
            var circle_marker_ee0335f6779541debbc68ad3fb080727 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_bd9ee89a590c4ec7961d3852c5cd4a01 = L.popup({"maxWidth": "100%"});

        
            var html_b37d39d6a6f648fc9f3da3384f550691 = $(`<div id="html_b37d39d6a6f648fc9f3da3384f550691" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/1908.07968">Assessing the impact of a user-item collaborative attack on class of users</a><br></div>`)[0];
            popup_bd9ee89a590c4ec7961d3852c5cd4a01.setContent(html_b37d39d6a6f648fc9f3da3384f550691);
        

        circle_marker_ee0335f6779541debbc68ad3fb080727.bindPopup(popup_bd9ee89a590c4ec7961d3852c5cd4a01)
        ;

        
    
    
            var circle_marker_67e97b3f2ff845ab84d94391e05b511f = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_bde32d0042b14bf4b4268f5a70cb2141 = L.popup({"maxWidth": "100%"});

        
            var html_ef5e1c947da9426a955f02bee87e3e10 = $(`<div id="html_ef5e1c947da9426a955f02bee87e3e10" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3240407">Audio-visual encoding of multimedia content for enhancing movie recommendations</a><br></div>`)[0];
            popup_bde32d0042b14bf4b4268f5a70cb2141.setContent(html_ef5e1c947da9426a955f02bee87e3e10);
        

        circle_marker_67e97b3f2ff845ab84d94391e05b511f.bindPopup(popup_bde32d0042b14bf4b4268f5a70cb2141)
        ;

        
    
    
            var circle_marker_9428c664ed404b3d9d8dc2d15cf17206 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d647b11be9d44f88953b88cf43610ce3 = L.popup({"maxWidth": "100%"});

        
            var html_1fa5f070666946838aebc0bfda121276 = $(`<div id="html_1fa5f070666946838aebc0bfda121276" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8903002/">Automatic playlist generation using Convolutional Neural Networks and Recurrent Neural Networks</a><br></div>`)[0];
            popup_d647b11be9d44f88953b88cf43610ce3.setContent(html_1fa5f070666946838aebc0bfda121276);
        

        circle_marker_9428c664ed404b3d9d8dc2d15cf17206.bindPopup(popup_d647b11be9d44f88953b88cf43610ce3)
        ;

        
    
    
            var circle_marker_fa942882790244b49960880b6e647bbf = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_96d10265dbd74eb08dc272f7591a3b6c = L.popup({"maxWidth": "100%"});

        
            var html_2b0963ce73c74dd38810567b66a30fc4 = $(`<div id="html_2b0963ce73c74dd38810567b66a30fc4" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s13735-018-0154-2">Current challenges and visions in music recommender systems research</a><br></div>`)[0];
            popup_96d10265dbd74eb08dc272f7591a3b6c.setContent(html_2b0963ce73c74dd38810567b66a30fc4);
        

        circle_marker_fa942882790244b49960880b6e647bbf.bindPopup(popup_96d10265dbd74eb08dc272f7591a3b6c)
        ;

        
    
    
            var circle_marker_8b1fb4e56c614a3eb08a547b1e3f2789 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d1da2a2eafa241efb558a654cb6c2d55 = L.popup({"maxWidth": "100%"});

        
            var html_ac22d74881db4ca2b75e3470e5a9744e = $(`<div id="html_ac22d74881db4ca2b75e3470e5a9744e" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3267471.3267486">Efficient similarity based methods for the playlist continuation task</a><br></div>`)[0];
            popup_d1da2a2eafa241efb558a654cb6c2d55.setContent(html_ac22d74881db4ca2b75e3470e5a9744e);
        

        circle_marker_8b1fb4e56c614a3eb08a547b1e3f2789.bindPopup(popup_d1da2a2eafa241efb558a654cb6c2d55)
        ;

        
    
    
            var circle_marker_05f12253bb014ac5985d37905afdd5a3 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_37e3e6a6b11e4151a056d805ad6b6436 = L.popup({"maxWidth": "100%"});

        
            var html_1655de449df144fe8fe1098967ddfd72 = $(`<div id="html_1655de449df144fe8fe1098967ddfd72" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://library.oapen.org/bitstream/handle/20.500.12657/23079/1007079.pdf?sequence=1#page=84">Enhancing Video Recommendation Using Multimedia Content</a><br></div>`)[0];
            popup_37e3e6a6b11e4151a056d805ad6b6436.setContent(html_1655de449df144fe8fe1098967ddfd72);
        

        circle_marker_05f12253bb014ac5985d37905afdd5a3.bindPopup(popup_37e3e6a6b11e4151a056d805ad6b6436)
        ;

        
    
    
            var circle_marker_583a88e960e543b3b0b3af6ddf1c4353 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8b80ee4f05684b5ebeeadf0a734dad99 = L.popup({"maxWidth": "100%"});

        
            var html_891fd52f18a645bd8b7f278c64d8dd41 = $(`<div id="html_891fd52f18a645bd8b7f278c64d8dd41" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="http://sisinflab.poliba.it/publications/2020/ADDMAD20/SEBD2020___Knowledge_enhanced_Shilling_Attacks_for_recommendation.pdf">Knowledge-enhanced Shilling Attacks for Recommendation⋆</a><br></div>`)[0];
            popup_8b80ee4f05684b5ebeeadf0a734dad99.setContent(html_891fd52f18a645bd8b7f278c64d8dd41);
        

        circle_marker_583a88e960e543b3b0b3af6ddf1c4353.bindPopup(popup_8b80ee4f05684b5ebeeadf0a734dad99)
        ;

        
    
    
            var circle_marker_009390523f7a4e6e93d2c35a3384efea = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_06152fc89ab7478daa5f73139faa5787 = L.popup({"maxWidth": "100%"});

        
            var html_864e43a7a0b24cfeb601b46888dc6816 = $(`<div id="html_864e43a7a0b24cfeb601b46888dc6816" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3359555.3359563">Leveraging laziness, browsing-pattern aware stacked models for sequential accommodation learning to rank</a><br></div>`)[0];
            popup_06152fc89ab7478daa5f73139faa5787.setContent(html_864e43a7a0b24cfeb601b46888dc6816);
        

        circle_marker_009390523f7a4e6e93d2c35a3384efea.bindPopup(popup_06152fc89ab7478daa5f73139faa5787)
        ;

        
    
    
            var circle_marker_89e46156d87f42dd96df7554eb34c55a = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_64ac407eeded4161a399d4dfdfd5373a = L.popup({"maxWidth": "100%"});

        
            var html_f66de4c73bcf47cd9f951a9e40fb6422 = $(`<div id="html_f66de4c73bcf47cd9f951a9e40fb6422" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3204949.3208141">MMTF-14K: a multifaceted movie trailer feature dataset for recommendation and retrieval</a><br></div>`)[0];
            popup_64ac407eeded4161a399d4dfdfd5373a.setContent(html_f66de4c73bcf47cd9f951a9e40fb6422);
        

        circle_marker_89e46156d87f42dd96df7554eb34c55a.bindPopup(popup_64ac407eeded4161a399d4dfdfd5373a)
        ;

        
    
    
            var circle_marker_162f90204cfd407c857003a287a56db5 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8b5ec393cacc481d9f6c8d10bacb6245 = L.popup({"maxWidth": "100%"});

        
            var html_9448e134a8a744b7bc2b355de2df21e7 = $(`<div id="html_9448e134a8a744b7bc2b355de2df21e7" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="">Movie Rating Prediction Using Multimedia Content and Modeling as a Classification Problem.</a><br></div>`)[0];
            popup_8b5ec393cacc481d9f6c8d10bacb6245.setContent(html_9448e134a8a744b7bc2b355de2df21e7);
        

        circle_marker_162f90204cfd407c857003a287a56db5.bindPopup(popup_8b5ec393cacc481d9f6c8d10bacb6245)
        ;

        
    
    
            var circle_marker_97fbc6bd95da49358b1525191154042d = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_23662f4ba1b849e39a71246f260588b1 = L.popup({"maxWidth": "100%"});

        
            var html_99866b0802484a668b7a8825b5892fe7 = $(`<div id="html_99866b0802484a668b7a8825b5892fe7" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877452/">Movie genome recommender: A novel recommender system based on multimedia content</a><br></div>`)[0];
            popup_23662f4ba1b849e39a71246f260588b1.setContent(html_99866b0802484a668b7a8825b5892fe7);
        

        circle_marker_97fbc6bd95da49358b1525191154042d.bindPopup(popup_23662f4ba1b849e39a71246f260588b1)
        ;

        
    
    
            var circle_marker_0920f928042f4281b264042693714803 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c692f00642664539bba05b4828066fff = L.popup({"maxWidth": "100%"});

        
            var html_a608c46237144984bded3b929c8b8652 = $(`<div id="html_a608c46237144984bded3b929c8b8652" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09221-y">Movie genome: alleviating new item cold start in movie recommendation</a><br></div>`)[0];
            popup_c692f00642664539bba05b4828066fff.setContent(html_a608c46237144984bded3b929c8b8652);
        

        circle_marker_0920f928042f4281b264042693714803.bindPopup(popup_c692f00642664539bba05b4828066fff)
        ;

        
    
    
            var circle_marker_86a2e99e2f76428bac0e6b2acc6eb3e0 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8ef2c29abe534f7b86012b5ee9264eb0 = L.popup({"maxWidth": "100%"});

        
            var html_7cb6d10898a244a4a54bf92a768fab75 = $(`<div id="html_7cb6d10898a244a4a54bf92a768fab75" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3241620">Multimedia recommender systems</a><br></div>`)[0];
            popup_8ef2c29abe534f7b86012b5ee9264eb0.setContent(html_7cb6d10898a244a4a54bf92a768fab75);
        

        circle_marker_86a2e99e2f76428bac0e6b2acc6eb3e0.bindPopup(popup_8ef2c29abe534f7b86012b5ee9264eb0)
        ;

        
    
    
            var circle_marker_6387d7dcc3554ff4bf064a0047b7159b = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0a5e7354378b486180c06d232a5258cb = L.popup({"maxWidth": "100%"});

        
            var html_f19e78360f3c491ea1d417f83a1f0072 = $(`<div id="html_f19e78360f3c491ea1d417f83a1f0072" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3301275.3302266">Prediction of music pairwise preferences from facial expressions</a><br></div>`)[0];
            popup_0a5e7354378b486180c06d232a5258cb.setContent(html_f19e78360f3c491ea1d417f83a1f0072);
        

        circle_marker_6387d7dcc3554ff4bf064a0047b7159b.bindPopup(popup_0a5e7354378b486180c06d232a5258cb)
        ;

        
    
    
            var circle_marker_e760a741c4804112becf52b68fb43d54 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9a09bbd2a28946c7bf11f673cf011b93 = L.popup({"maxWidth": "100%"});

        
            var html_9a2d17a47cf14f178d293cfa7298edfb = $(`<div id="html_9a2d17a47cf14f178d293cfa7298edfb" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/1908.06708">Recommender Systems Fairness Evaluation via Generalized Cross Entropy</a><br></div>`)[0];
            popup_9a09bbd2a28946c7bf11f673cf011b93.setContent(html_9a2d17a47cf14f178d293cfa7298edfb);
        

        circle_marker_e760a741c4804112becf52b68fb43d54.bindPopup(popup_9a09bbd2a28946c7bf11f673cf011b93)
        ;

        
    
    
            var circle_marker_757819b4fa6a4a9994f35d1b478b5465 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0efa220cc672453a84e062568100133e = L.popup({"maxWidth": "100%"});

        
            var html_e40503079e034501a3fd94a0ec395df0 = $(`<div id="html_e40503079e034501a3fd94a0ec395df0" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/342211693_Recommender_Systems_Leveraging_Multimedia_Content/links/5f1a654d45851515ef44cb80/Recommender-Systems-Leveraging-Multimedia-Content.pdf">Recommender systems leveraging multimedia content</a><br></div>`)[0];
            popup_0efa220cc672453a84e062568100133e.setContent(html_e40503079e034501a3fd94a0ec395df0);
        

        circle_marker_757819b4fa6a4a9994f35d1b478b5465.bindPopup(popup_0efa220cc672453a84e062568100133e)
        ;

        
    
    
            var circle_marker_11000dd950b64eaaa97c5ca7e9a177b9 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_fcc8d31b1bae4e529b0af88e4e384600 = L.popup({"maxWidth": "100%"});

        
            var html_ffb2543e6cf94210ab2445e591beb00a = $(`<div id="html_ffb2543e6cf94210ab2445e591beb00a" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877420/">Retrieving relevant and diverse movie clips using the mfvcd-7k multifaceted video clip dataset</a><br></div>`)[0];
            popup_fcc8d31b1bae4e529b0af88e4e384600.setContent(html_ffb2543e6cf94210ab2445e591beb00a);
        

        circle_marker_11000dd950b64eaaa97c5ca7e9a177b9.bindPopup(popup_fcc8d31b1bae4e529b0af88e4e384600)
        ;

        
    
    
            var circle_marker_83acd1df6c6a48f1b50b4a27396fcabd = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_17d0cd1a855e427b8e790bbb343e1c59 = L.popup({"maxWidth": "100%"});

        
            var html_695ff9a7c78d4804a81a24f194c9fd17 = $(`<div id="html_695ff9a7c78d4804a81a24f194c9fd17" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="http://www-ictserv.poliba.it/publications/2020/ADDDM20/2020_Anelli_ESWC2020.pdf">SAShA: Semantic-Aware Shilling Attacks on Recommender Systems Exploiting Knowledge Graphs</a><br></div>`)[0];
            popup_17d0cd1a855e427b8e790bbb343e1c59.setContent(html_695ff9a7c78d4804a81a24f194c9fd17);
        

        circle_marker_83acd1df6c6a48f1b50b4a27396fcabd.bindPopup(popup_17d0cd1a855e427b8e790bbb343e1c59)
        ;

        
    
    
            var circle_marker_72601f347b0d43468ebdcce2d603289b = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ce0cd78d1faf43eeaee73439ac11d4b4 = L.popup({"maxWidth": "100%"});

        
            var html_58dec9c969264d028c1f34892c945b25 = $(`<div id="html_58dec9c969264d028c1f34892c945b25" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-49461-2_18">Sasha: Semantic-aware shilling attacks on recommender systems exploiting knowledge graphs</a><br></div>`)[0];
            popup_ce0cd78d1faf43eeaee73439ac11d4b4.setContent(html_58dec9c969264d028c1f34892c945b25);
        

        circle_marker_72601f347b0d43468ebdcce2d603289b.bindPopup(popup_ce0cd78d1faf43eeaee73439ac11d4b4)
        ;

        
    
    
            var circle_marker_9720fda27e404063a82a9dacfd03bb3f = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_32a92f57662446ae9e883137b074b340 = L.popup({"maxWidth": "100%"});

        
            var html_0ef84505c9f34f54a340b3cc6bf407a4 = $(`<div id="html_0ef84505c9f34f54a340b3cc6bf407a4" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/334548980_The_2019_Multimedia_for_Recommender_System_Task_MovieREC_and_NewsREEL_at_MediaEval/links/5d30c3ac299bf1547cc258c1/The-2019-Multimedia-for-Recommender-System-Task-MovieREC-and-NewsREEL-at-MediaEval.pdf">The 2019 Multimedia for Recommender System Task: MovieREC and NewsREEL at MediaEval</a><br></div>`)[0];
            popup_32a92f57662446ae9e883137b074b340.setContent(html_0ef84505c9f34f54a340b3cc6bf407a4);
        

        circle_marker_9720fda27e404063a82a9dacfd03bb3f.bindPopup(popup_32a92f57662446ae9e883137b074b340)
        ;

        
    
    
            var circle_marker_6b981d996c8a46aaa8f72cc28165b9b9 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9a8c7ae68c804d3a8602897ef2930e75 = L.popup({"maxWidth": "100%"});

        
            var html_9280e7623a9844e49c4a8e5c720c31d2 = $(`<div id="html_9280e7623a9844e49c4a8e5c720c31d2" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.jku.at/fileadmin/gruppen/173/Research/deldjoo_mediaeval_2018.pdf">The MediaEval 2018 Movie Recommendation Task: Recommending Movies Using Content.</a><br></div>`)[0];
            popup_9a8c7ae68c804d3a8602897ef2930e75.setContent(html_9280e7623a9844e49c4a8e5c720c31d2);
        

        circle_marker_6b981d996c8a46aaa8f72cc28165b9b9.bindPopup(popup_9a8c7ae68c804d3a8602897ef2930e75)
        ;

        
    
    
            var circle_marker_d63da52e33c64deba22b35f552c3a26c = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_db08dc5753684aeba8fbf222b5868bf5 = L.popup({"maxWidth": "100%"});

        
            var html_72a0f445fdaf4188b5b76738b7c771cf = $(`<div id="html_72a0f445fdaf4188b5b76738b7c771cf" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/1908.11055">Towards Evaluating User Profiling Methods Based on Explicit Ratings on Item Features</a><br></div>`)[0];
            popup_db08dc5753684aeba8fbf222b5868bf5.setContent(html_72a0f445fdaf4188b5b76738b7c771cf);
        

        circle_marker_d63da52e33c64deba22b35f552c3a26c.bindPopup(popup_db08dc5753684aeba8fbf222b5868bf5)
        ;

        
    
    
            var circle_marker_0f6f3a029e024c53b520a3ccff367d7c = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3483a2b282504f89b7c9ef1e66f0f858 = L.popup({"maxWidth": "100%"});

        
            var html_1de9072a086a403492f2e20d29e7d19c = $(`<div id="html_1de9072a086a403492f2e20d29e7d19c" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09231-w">Trends in content-based recommendation</a><br></div>`)[0];
            popup_3483a2b282504f89b7c9ef1e66f0f858.setContent(html_1de9072a086a403492f2e20d29e7d19c);
        

        circle_marker_0f6f3a029e024c53b520a3ccff367d7c.bindPopup(popup_3483a2b282504f89b7c9ef1e66f0f858)
        ;

        
    
    
            var circle_marker_91321b31f743457bbf01223e27a158fe = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9d2a16a112a740ac978b0ad060507646 = L.popup({"maxWidth": "100%"});

        
            var html_2d3ae3a2aff94fb3be5825ddde3723dd = $(`<div id="html_2d3ae3a2aff94fb3be5825ddde3723dd" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/2005.00145">Unsupervised Domain Adaptation for Acoustic Scene Classification Using Band-Wise Statistics Matching</a><br></div>`)[0];
            popup_9d2a16a112a740ac978b0ad060507646.setContent(html_2d3ae3a2aff94fb3be5825ddde3723dd);
        

        circle_marker_91321b31f743457bbf01223e27a158fe.bindPopup(popup_9d2a16a112a740ac978b0ad060507646)
        ;

        
    
    
            var circle_marker_c4ceea9af6f340ecbea1ad002fbda659 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_fcce8cb9b9dd44b3b62809e76cc59ac7 = L.popup({"maxWidth": "100%"});

        
            var html_9c0444bf96284721b11397231bbc9246 = $(`<div id="html_9c0444bf96284721b11397231bbc9246" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.politesi.polimi.it/handle/10589/141256">Video recommendation by exploiting the multimedia content</a><br></div>`)[0];
            popup_fcce8cb9b9dd44b3b62809e76cc59ac7.setContent(html_9c0444bf96284721b11397231bbc9246);
        

        circle_marker_c4ceea9af6f340ecbea1ad002fbda659.bindPopup(popup_fcce8cb9b9dd44b3b62809e76cc59ac7)
        ;

        
    
    
            var circle_marker_d3d29f7277434eec865158d43ac267b1 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e8c0ec19d65a4d83ac270593e23b3f44 = L.popup({"maxWidth": "100%"});

        
            var html_f3067ee578b640a6beea2e9c5cc44659 = $(`<div id="html_f3067ee578b640a6beea2e9c5cc44659" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Hossein_A_Rahmani/publication/337768473_A_Regression_Approach_to_Movie_Rating_Prediction_using_Multimedia_Content_and_Metadata/links/5df8b821a6fdcc283726d8c7/A-Regression-Approach-to-Movie-Rating-Prediction-using-Multimedia-Content-and-Metadata.pdf?_sg%5B0%5D=3b-miF1_LzXGwxQhQxRYO5FfklSPn0YfAlrK6IWfJilrpkC8-9h2ljnRbzLhZjp8z22CM_vzJpiQ-DaxhD7RLQ.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_sg%5B1%5D=0EyWPCbfTe4cM2OzZ6VGxzinAQj21QkakmKNQ9xBAXR2oXBv7SQahoq94YN7zTBLR7KuEuKMzCqZtGzBOqJvKV6rX2V1E4ISvHGuBwdOwd-p.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_iepl=">A Regression Approach to Movie Rating Prediction using Multimedia Content and Metadata</a><br></div>`)[0];
            popup_e8c0ec19d65a4d83ac270593e23b3f44.setContent(html_f3067ee578b640a6beea2e9c5cc44659);
        

        circle_marker_d3d29f7277434eec865158d43ac267b1.bindPopup(popup_e8c0ec19d65a4d83ac270593e23b3f44)
        ;

        
    
    
            var circle_marker_bb53880fdb034a579f6615cd310e6386 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f6689da8809547538d34d7c6267cc0a9 = L.popup({"maxWidth": "100%"});

        
            var html_1fe36630785c4e68a11697eefb227280 = $(`<div id="html_1fe36630785c4e68a11697eefb227280" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dafx16.vutbr.cz/dafxpapers/09-DAFx-16_paper_37-PN.pdf">A cosine-distance based neural network for music artist recognition using raw i-vector features</a><br></div>`)[0];
            popup_f6689da8809547538d34d7c6267cc0a9.setContent(html_1fe36630785c4e68a11697eefb227280);
        

        circle_marker_bb53880fdb034a579f6615cd310e6386.bindPopup(popup_f6689da8809547538d34d7c6267cc0a9)
        ;

        
    
    
            var circle_marker_23fee5f3504d48fa9ea9cd008ac1c42a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_296ebfcb6cc443789bb663e7649f9a56 = L.popup({"maxWidth": "100%"});

        
            var html_528b22e68d504d6faddb9ab712cf0c19 = $(`<div id="html_528b22e68d504d6faddb9ab712cf0c19" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3167132.3167280">A hybrid approach to music playlist continuation based on playlist-song membership</a><br></div>`)[0];
            popup_296ebfcb6cc443789bb663e7649f9a56.setContent(html_528b22e68d504d6faddb9ab712cf0c19);
        

        circle_marker_23fee5f3504d48fa9ea9cd008ac1c42a.bindPopup(popup_296ebfcb6cc443789bb663e7649f9a56)
        ;

        
    
    
            var circle_marker_0999b220c31c4c15aaeb56be3c93715a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9ef156b5ab544ffb883a12d8603287bd = L.popup({"maxWidth": "100%"});

        
            var html_9027ed7aea3946fea0725186b1c2a790 = $(`<div id="html_9027ed7aea3946fea0725186b1c2a790" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8081711/">A hybrid approach with multi-channel i-vectors and convolutional neural networks for acoustic scene classification</a><br></div>`)[0];
            popup_9ef156b5ab544ffb883a12d8603287bd.setContent(html_9027ed7aea3946fea0725186b1c2a790);
        

        circle_marker_0999b220c31c4c15aaeb56be3c93715a.bindPopup(popup_9ef156b5ab544ffb883a12d8603287bd)
        ;

        
    
    
            var circle_marker_ce258e828ef047799a9fc906e0bf22e3 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_18d6254769254ae78f69beb8d905ff1c = L.popup({"maxWidth": "100%"});

        
            var html_ae8b9a58eac243a3a0fa9ddb2a0eb978 = $(`<div id="html_ae8b9a58eac243a3a0fa9ddb2a0eb978" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8999242/">Acoustic Scene Classification Using Deep Mixtures of Pre-trained Convolutional Neural Networks</a><br></div>`)[0];
            popup_18d6254769254ae78f69beb8d905ff1c.setContent(html_ae8b9a58eac243a3a0fa9ddb2a0eb978);
        

        circle_marker_ce258e828ef047799a9fc906e0bf22e3.bindPopup(popup_18d6254769254ae78f69beb8d905ff1c)
        ;

        
    
    
            var circle_marker_cbbac9a8d46b49a0aa68d9020fac3b14 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2aa3ef915c0440e682e65bc00dd4905b = L.popup({"maxWidth": "100%"});

        
            var html_aa865d917ef444e4aeb61ae16b82378b = $(`<div id="html_aa865d917ef444e4aeb61ae16b82378b" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053582/">Acoustic Scene Classification for Mismatched Recording Devices Using Heated-Up Softmax and Spectrum Correction</a><br></div>`)[0];
            popup_2aa3ef915c0440e682e65bc00dd4905b.setContent(html_aa865d917ef444e4aeb61ae16b82378b);
        

        circle_marker_cbbac9a8d46b49a0aa68d9020fac3b14.bindPopup(popup_2aa3ef915c0440e682e65bc00dd4905b)
        ;

        
    
    
            var circle_marker_a59da4a1c32549d2a92bd8e3cb8219d7 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f7ef5633c47149f68d96d74fddcd619b = L.popup({"maxWidth": "100%"});

        
            var html_4bb1c8378b9249efb5d72fc7f9e7b164 = $(`<div id="html_4bb1c8378b9249efb5d72fc7f9e7b164" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/3002.pdf">Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation.</a><br></div>`)[0];
            popup_f7ef5633c47149f68d96d74fddcd619b.setContent(html_4bb1c8378b9249efb5d72fc7f9e7b164);
        

        circle_marker_a59da4a1c32549d2a92bd8e3cb8219d7.bindPopup(popup_f7ef5633c47149f68d96d74fddcd619b)
        ;

        
    
    
            var circle_marker_fc2302d7a34b41b0bfcf804c74c65a84 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_51020fa4daca4a1781a1689424bf078c = L.popup({"maxWidth": "100%"});

        
            var html_86b1660d43a1493caf2f7590c8cb9293 = $(`<div id="html_86b1660d43a1493caf2f7590c8cb9293" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://trepo.tuni.fi/bitstream/handle/10024/116599/DCASE_2018_proceedings.pdf?sequence=1&isAllowed=y#page=35">Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters</a><br></div>`)[0];
            popup_51020fa4daca4a1781a1689424bf078c.setContent(html_86b1660d43a1493caf2f7590c8cb9293);
        

        circle_marker_fc2302d7a34b41b0bfcf804c74c65a84.bindPopup(popup_51020fa4daca4a1781a1689424bf078c)
        ;

        
    
    
            var circle_marker_d9a4c1b644da41c6b9def021d89e8c22 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_80033a78f6054f3b9b0c1335b916fd0f = L.popup({"maxWidth": "100%"});

        
            var html_d39ccc054dac428bbe71720298ffd17a = $(`<div id="html_d39ccc054dac428bbe71720298ffd17a" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="">Acoustic scene classification using deep mixture of parallel convolutional neural networks</a><br></div>`)[0];
            popup_80033a78f6054f3b9b0c1335b916fd0f.setContent(html_d39ccc054dac428bbe71720298ffd17a);
        

        circle_marker_d9a4c1b644da41c6b9def021d89e8c22.bindPopup(popup_80033a78f6054f3b9b0c1335b916fd0f)
        ;

        
    
    
            var circle_marker_8b33fab0a28a455d9a5708a6a12aa81c = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_acda52f09a9a448bacfa6d6108a51214 = L.popup({"maxWidth": "100%"});

        
            var html_9a24345ac843436ebc8aa9c8dc950731 = $(`<div id="html_9a24345ac843436ebc8aa9c8dc950731" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Dorfer_97.pdf">Acoustic scene classification with fully convolutional neural networks and I-vectors</a><br></div>`)[0];
            popup_acda52f09a9a448bacfa6d6108a51214.setContent(html_9a24345ac843436ebc8aa9c8dc950731);
        

        circle_marker_8b33fab0a28a455d9a5708a6a12aa81c.bindPopup(popup_acda52f09a9a448bacfa6d6108a51214)
        ;

        
    
    
            var circle_marker_c887c4c88a2442a5a63a44db0f8edaa7 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_440d43c874e94e66bddf40fb2a99c591 = L.popup({"maxWidth": "100%"});

        
            var html_2e764a07af2645df9cfea0a2c6db6235 = $(`<div id="html_2e764a07af2645df9cfea0a2c6db6235" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8784816/">Acoustic scene classification with mismatched recording devices using mixture of experts layer</a><br></div>`)[0];
            popup_440d43c874e94e66bddf40fb2a99c591.setContent(html_2e764a07af2645df9cfea0a2c6db6235);
        

        circle_marker_c887c4c88a2442a5a63a44db0f8edaa7.bindPopup(popup_440d43c874e94e66bddf40fb2a99c591)
        ;

        
    
    
            var circle_marker_71bfde5e79a747f5b5bcc5259d02a3a0 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9761fab4c2c74275992a589a2af4a175 = L.popup({"maxWidth": "100%"});

        
            var html_6396c16e29cb4730bc680b1bfbaef377 = $(`<div id="html_6396c16e29cb4730bc680b1bfbaef377" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Bernhard_Lehner/publication/337834114_ACOUSTIC_SCENE_CLASSIFICATION_WITH_REJECT_OPTION_BASED_ON_RESNETS/links/5dee3d10299bf10bc34ce38b/ACOUSTIC-SCENE-CLASSIFICATION-WITH-REJECT-OPTION-BASED-ON-RESNETS.pdf">Acoustic scene classification with reject option based on resnets</a><br></div>`)[0];
            popup_9761fab4c2c74275992a589a2af4a175.setContent(html_6396c16e29cb4730bc680b1bfbaef377);
        

        circle_marker_71bfde5e79a747f5b5bcc5259d02a3a0.bindPopup(popup_9761fab4c2c74275992a589a2af4a175)
        ;

        
    
    
            var circle_marker_02716da35a9d4cac8d772b524f267127 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9a1d49ff73e6499184f88ee5b730db3f = L.popup({"maxWidth": "100%"});

        
            var html_ed95b6bd6e474552964649d50c27e425 = $(`<div id="html_ed95b6bd6e474552964649d50c27e425" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-57321-8_25">Applying AI in Practice: Key Challenges and Lessons Learned</a><br></div>`)[0];
            popup_9a1d49ff73e6499184f88ee5b730db3f.setContent(html_ed95b6bd6e474552964649d50c27e425);
        

        circle_marker_02716da35a9d4cac8d772b524f267127.bindPopup(popup_9a1d49ff73e6499184f88ee5b730db3f)
        ;

        
    
    
            var circle_marker_260b3bf6ba604310bf5cb971ec9bd015 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_bf40a98a0fd74dbd8d1c024d6496133f = L.popup({"maxWidth": "100%"});

        
            var html_b43e2debab62446cb633f5129dafdd8d = $(`<div id="html_b43e2debab62446cb633f5129dafdd8d" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Paischer_37_t2.pdf">Audio tagging with convolutional neural networks trained with noisy data</a><br></div>`)[0];
            popup_bf40a98a0fd74dbd8d1c024d6496133f.setContent(html_b43e2debab62446cb633f5129dafdd8d);
        

        circle_marker_260b3bf6ba604310bf5cb971ec9bd015.bindPopup(popup_bf40a98a0fd74dbd8d1c024d6496133f)
        ;

        
    
    
            var circle_marker_9a97a4a7925d4228a768853dab7ce8cb = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_76e4b416530849bab8c039377a01e64c = L.popup({"maxWidth": "100%"});

        
            var html_59cd646add6c471dae1eaeaa3e8e6fdd = $(`<div id="html_59cd646add6c471dae1eaeaa3e8e6fdd" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3240407">Audio-visual encoding of multimedia content for enhancing movie recommendations</a><br></div>`)[0];
            popup_76e4b416530849bab8c039377a01e64c.setContent(html_59cd646add6c471dae1eaeaa3e8e6fdd);
        

        circle_marker_9a97a4a7925d4228a768853dab7ce8cb.bindPopup(popup_76e4b416530849bab8c039377a01e64c)
        ;

        
    
    
            var circle_marker_3b3ad1141d15491d85525015be20ae3b = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_98f3adc06a114cb69cd89ab2b19b602e = L.popup({"maxWidth": "100%"});

        
            var html_be1e6e15cbcf481baae393fb108f9942 = $(`<div id="html_be1e6e15cbcf481baae393fb108f9942" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dbis.uibk.ac.at/sites/default/files/2019-06/paper_2.pdf">Autoencoders for Next-Track-Recommendation.</a><br></div>`)[0];
            popup_98f3adc06a114cb69cd89ab2b19b602e.setContent(html_be1e6e15cbcf481baae393fb108f9942);
        

        circle_marker_3b3ad1141d15491d85525015be20ae3b.bindPopup(popup_98f3adc06a114cb69cd89ab2b19b602e)
        ;

        
    
    
            var circle_marker_fc9bcfa570ee42f1b8b1554ec3aad34d = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_65d9736c77804233afed82a0fbe49135 = L.popup({"maxWidth": "100%"});

        
            var html_d789b251a8df46329c36071c9ffceeec = $(`<div id="html_d789b251a8df46329c36071c9ffceeec" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koutini_142.pdf">CP-JKU SUBMISSIONS TO DCASE'20: LOW-COMPLEXITY CROSS-DEVICE ACOUSTIC SCENE CLASSIFICATION WITH RF-REGULARIZED CNNS</a><br></div>`)[0];
            popup_65d9736c77804233afed82a0fbe49135.setContent(html_d789b251a8df46329c36071c9ffceeec);
        

        circle_marker_fc9bcfa570ee42f1b8b1554ec3aad34d.bindPopup(popup_65d9736c77804233afed82a0fbe49135)
        ;

        
    
    
            var circle_marker_3068456f0f104bee893252837c1538b7 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_587d34062f754d9286fd6fa4fb2f4c54 = L.popup({"maxWidth": "100%"});

        
            var html_62c8bb71cf9448628e22dc997c7e9f38 = $(`<div id="html_62c8bb71cf9448628e22dc997c7e9f38" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Koutini_99_t2.pdf">CP-JKU Submissions to DCASE'19: Acoustic Scene Classification and Audio Tagging with REceptive-Field-Regularized CNNs</a><br></div>`)[0];
            popup_587d34062f754d9286fd6fa4fb2f4c54.setContent(html_62c8bb71cf9448628e22dc997c7e9f38);
        

        circle_marker_3068456f0f104bee893252837c1538b7.bindPopup(popup_587d34062f754d9286fd6fa4fb2f4c54)
        ;

        
    
    
            var circle_marker_5ed830d976134adda886cdecd2caf68b = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_4df474a3aa854dceadf5c3863359a783 = L.popup({"maxWidth": "100%"});

        
            var html_8a4f45f3453f4a7da0465f6566d20d02 = $(`<div id="html_8a4f45f3453f4a7da0465f6566d20d02" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2016/technical_reports/DCASE2016_Eghbal-Zadeh_1028.pdf">CP-JKU submissions for DCASE-2016: A hybrid approach using binaural i-vectors and deep convolutional neural networks</a><br></div>`)[0];
            popup_4df474a3aa854dceadf5c3863359a783.setContent(html_8a4f45f3453f4a7da0465f6566d20d02);
        

        circle_marker_5ed830d976134adda886cdecd2caf68b.bindPopup(popup_4df474a3aa854dceadf5c3863359a783)
        ;

        
    
    
            var circle_marker_af6ad2a03660417b8cb0da52e922bad3 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_019eb414bc2d46b49dcc2c3fa4e4db20 = L.popup({"maxWidth": "100%"});

        
            var html_bcb9fb582f8b41c29eb8cfa5af0011e3 = $(`<div id="html_bcb9fb582f8b41c29eb8cfa5af0011e3" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Lehner_142.pdf">Classifying short acoustic scenes with I-vectors and CNNs: Challenges and optimisations for the 2017 DCASE ASC task</a><br></div>`)[0];
            popup_019eb414bc2d46b49dcc2c3fa4e4db20.setContent(html_bcb9fb582f8b41c29eb8cfa5af0011e3);
        

        circle_marker_af6ad2a03660417b8cb0da52e922bad3.bindPopup(popup_019eb414bc2d46b49dcc2c3fa4e4db20)
        ;

        
    
    
            var circle_marker_14a7235644934f9da91830977bde0dec = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f7b0fab5f2a24fdfa3bed4233aca058c = L.popup({"maxWidth": "100%"});

        
            var html_9fa0f6e71509463b8a16cfadde1feb33 = $(`<div id="html_9fa0f6e71509463b8a16cfadde1feb33" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s13735-018-0154-2">Current challenges and visions in music recommender systems research</a><br></div>`)[0];
            popup_f7b0fab5f2a24fdfa3bed4233aca058c.setContent(html_9fa0f6e71509463b8a16cfadde1feb33);
        

        circle_marker_14a7235644934f9da91830977bde0dec.bindPopup(popup_f7b0fab5f2a24fdfa3bed4233aca058c)
        ;

        
    
    
            var circle_marker_22971f6c3c62454fa77a7fd305bb0e84 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8bf8b79d2dd34c6aba5289e782fed206 = L.popup({"maxWidth": "100%"});

        
            var html_de8cf9c714034fb7b99de0a607af27ba = $(`<div id="html_de8cf9c714034fb7b99de0a607af27ba" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="">DEEP WITHIN-CLASS COVARIANCE ANALYSIS FOR ACOUSTIC SCENE CLASSIFICATION</a><br></div>`)[0];
            popup_8bf8b79d2dd34c6aba5289e782fed206.setContent(html_de8cf9c714034fb7b99de0a607af27ba);
        

        circle_marker_22971f6c3c62454fa77a7fd305bb0e84.bindPopup(popup_8bf8b79d2dd34c6aba5289e782fed206)
        ;

        
    
    
            var circle_marker_8d82916f692844df98c406d62d2bc7b4 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ba2516e2ea334988b248bb87229e30e1 = L.popup({"maxWidth": "100%"});

        
            var html_01fcfc7adc4e4b6e832aac639810b731 = $(`<div id="html_01fcfc7adc4e4b6e832aac639810b731" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.frontiersin.org/articles/10.3389/fams.2019.00044/abstract">Deep Learning in Music Recommendation Systems</a><br></div>`)[0];
            popup_ba2516e2ea334988b248bb87229e30e1.setContent(html_01fcfc7adc4e4b6e832aac639810b731);
        

        circle_marker_8d82916f692844df98c406d62d2bc7b4.bindPopup(popup_ba2516e2ea334988b248bb87229e30e1)
        ;

        
    
    
            var circle_marker_cebd21159a84468da50a9e099e69735a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e88d04b8d55a47bb8fac95ee2f49b9ad = L.popup({"maxWidth": "100%"});

        
            var html_3b9458af91e743e5a70a373e8a76f93b = $(`<div id="html_3b9458af91e743e5a70a373e8a76f93b" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1806.08840">Deep snp: An end-to-end deep neural network with attention-based localization for break-point detection in snp array genomic data</a><br></div>`)[0];
            popup_e88d04b8d55a47bb8fac95ee2f49b9ad.setContent(html_3b9458af91e743e5a70a373e8a76f93b);
        

        circle_marker_cebd21159a84468da50a9e099e69735a.bindPopup(popup_e88d04b8d55a47bb8fac95ee2f49b9ad)
        ;

        
    
    
            var circle_marker_6732695e87dd42f080458459006a7ab7 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0bf9f39a0c574f04a62e60b56909b057 = L.popup({"maxWidth": "100%"});

        
            var html_cf35868028584bb0bb8e85011277014f = $(`<div id="html_cf35868028584bb0bb8e85011277014f" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1711.04022">Deep within-class covariance analysis for robust audio representation learning</a><br></div>`)[0];
            popup_0bf9f39a0c574f04a62e60b56909b057.setContent(html_cf35868028584bb0bb8e85011277014f);
        

        circle_marker_6732695e87dd42f080458459006a7ab7.bindPopup(popup_0bf9f39a0c574f04a62e60b56909b057)
        ;

        
    
    
            var circle_marker_3797082006e24654b0e907ef00dc2f61 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_684438c634a440338554e5d188654698 = L.popup({"maxWidth": "100%"});

        
            var html_29f6181f236548b293f9a4a154802b3f = $(`<div id="html_29f6181f236548b293f9a4a154802b3f" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.liebertpub.com/doi/abs/10.1089/cmb.2018.0172">DeepSNP: An End-to-End Deep Neural Network with Attention-Based Localization for Breakpoint Detection in Single-Nucleotide Polymorphism Array Genomic Data</a><br></div>`)[0];
            popup_684438c634a440338554e5d188654698.setContent(html_29f6181f236548b293f9a4a154802b3f);
        

        circle_marker_3797082006e24654b0e907ef00dc2f61.bindPopup(popup_684438c634a440338554e5d188654698)
        ;

        
    
    
            var circle_marker_d35b74c7e71a42fa94068df2b565b35c = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d1148cc6551a41f5a4f1f44c9ccb2620 = L.popup({"maxWidth": "100%"});

        
            var html_0bf0bef5deed4a7b96de5a5104c8df00 = $(`<div id="html_0bf0bef5deed4a7b96de5a5104c8df00" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Bernhard_Lehner/publication/331223906_Detecting_the_Presence_of_Singing_Voice_in_Mixed_Music_Signals/links/5c6d205892851c1c9deee284/Detecting-the-Presence-of-Singing-Voice-in-Mixed-Music-Signals.pdf">Detecting the Presence of Singing Voice in Mixed Music Signals</a><br></div>`)[0];
            popup_d1148cc6551a41f5a4f1f44c9ccb2620.setContent(html_0bf0bef5deed4a7b96de5a5104c8df00);
        

        circle_marker_d35b74c7e71a42fa94068df2b565b35c.bindPopup(popup_d1148cc6551a41f5a4f1f44c9ccb2620)
        ;

        
    
    
            var circle_marker_e017ba128fe0496ab0e7ce535cc62701 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_fbb3468688b54949ae39a53d97bf6eb8 = L.popup({"maxWidth": "100%"});

        
            var html_82e12cfdcb324da399bee70aada6c5c3 = $(`<div id="html_82e12cfdcb324da399bee70aada6c5c3" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1911.05833">Emotion and Theme Recognition in Music with Frequency-Aware RF-Regularized CNNs</a><br></div>`)[0];
            popup_fbb3468688b54949ae39a53d97bf6eb8.setContent(html_82e12cfdcb324da399bee70aada6c5c3);
        

        circle_marker_e017ba128fe0496ab0e7ce535cc62701.bindPopup(popup_fbb3468688b54949ae39a53d97bf6eb8)
        ;

        
    
    
            var circle_marker_137bff8151e5412da152996f26245ec6 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3feded9532024b408d75538761de7c1d = L.popup({"maxWidth": "100%"});

        
            var html_ac9548ea71b44681ba1f5bb53d74abd7 = $(`<div id="html_ac9548ea71b44681ba1f5bb53d74abd7" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1909.02869">Exploiting parallel audio recordings to enforce device invariance in cnn-based acoustic scene classification</a><br></div>`)[0];
            popup_3feded9532024b408d75538761de7c1d.setContent(html_ac9548ea71b44681ba1f5bb53d74abd7);
        

        circle_marker_137bff8151e5412da152996f26245ec6.bindPopup(popup_3feded9532024b408d75538761de7c1d)
        ;

        
    
    
            var circle_marker_4a5263bfe64b457e81c86cd06678d755 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ced52cf685dc4980baf5d08a8b38ac33 = L.popup({"maxWidth": "100%"});

        
            var html_086253a91a4543159a59824e3beef225 = $(`<div id="html_086253a91a4543159a59824e3beef225" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-018-9215-8">Feature-combination hybrid recommender systems for automated music playlist continuation</a><br></div>`)[0];
            popup_ced52cf685dc4980baf5d08a8b38ac33.setContent(html_086253a91a4543159a59824e3beef225);
        

        circle_marker_4a5263bfe64b457e81c86cd06678d755.bindPopup(popup_ced52cf685dc4980baf5d08a8b38ac33)
        ;

        
    
    
            var circle_marker_c5cbd0752df74c67a0716db065e1ce71 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3afd72cdf29748f9b5b283fc569a884e = L.popup({"maxWidth": "100%"});

        
            var html_601f2b4a15b64b55a81feb1fb2e1898d = $(`<div id="html_601f2b4a15b64b55a81feb1fb2e1898d" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1808.05340">Genre-agnostic key classification with convolutional neural networks</a><br></div>`)[0];
            popup_3afd72cdf29748f9b5b283fc569a884e.setContent(html_601f2b4a15b64b55a81feb1fb2e1898d);
        

        circle_marker_c5cbd0752df74c67a0716db065e1ce71.bindPopup(popup_3afd72cdf29748f9b5b283fc569a884e)
        ;

        
    
    
            var circle_marker_6adb17e8a9c74df69645b28995dfc7af = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9133efe234b74477b5a12e2603119556 = L.popup({"maxWidth": "100%"});

        
            var html_ffbfb2a65b5a4abcb6ce161eae056281 = $(`<div id="html_ffbfb2a65b5a4abcb6ce161eae056281" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Hamid_Eghbal-Zadeh/publication/280227823_I-VECTORS_FOR_TIMBRE-BASED_MUSIC_SIMILARITY_AND_MUSIC_ARTIST_CLASSIFICATION/links/55d4676608ae0b8f3ef9fa8c/I-VECTORS-FOR-TIMBRE-BASED-MUSIC-SIMILARITY-AND-MUSIC-ARTIST-CLASSIFICATION.pdf">I-Vectors for Timbre-Based Music Similarity and Music Artist Classification.</a><br></div>`)[0];
            popup_9133efe234b74477b5a12e2603119556.setContent(html_ffbfb2a65b5a4abcb6ce161eae056281);
        

        circle_marker_6adb17e8a9c74df69645b28995dfc7af.bindPopup(popup_9133efe234b74477b5a12e2603119556)
        ;

        
    
    
            var circle_marker_b02122a6b02b4f549028c00580aee29c = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c7d08459857d46a7b9d2cee3063eb3fa = L.popup({"maxWidth": "100%"});

        
            var html_52344c41674340ef85703e175d3cffa3 = $(`<div id="html_52344c41674340ef85703e175d3cffa3" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://archives.ismir.net/ismir2019/paper/000003.pdf">Intelligent User Interfaces for Music Discovery: The Past 20 Years and What's to Come.</a><br></div>`)[0];
            popup_c7d08459857d46a7b9d2cee3063eb3fa.setContent(html_52344c41674340ef85703e175d3cffa3);
        

        circle_marker_b02122a6b02b4f549028c00580aee29c.bindPopup(popup_c7d08459857d46a7b9d2cee3063eb3fa)
        ;

        
    
    
            var circle_marker_d4e0fb50200a4fe38ef8f167718cb3ad = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3cf3af1822fa4d91961602df8f6d6482 = L.popup({"maxWidth": "100%"});

        
            var html_a51f583d18e9478caea1aeea6cbcd5f8 = $(`<div id="html_a51f583d18e9478caea1aeea6cbcd5f8" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.jku.at/fileadmin/gruppen/173/Research/Iterative_Knowledge_Distillation_In_R-CNNs.pdf">Iterative knowledge distillation in R-CNNs for weakly-labeled semisupervised sound event detection</a><br></div>`)[0];
            popup_3cf3af1822fa4d91961602df8f6d6482.setContent(html_a51f583d18e9478caea1aeea6cbcd5f8);
        

        circle_marker_d4e0fb50200a4fe38ef8f167718cb3ad.bindPopup(popup_3cf3af1822fa4d91961602df8f6d6482)
        ;

        
    
    
            var circle_marker_b32e0fb91ec0440ba7e274abf83720f8 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f33b34581e904ffa94948d2ffbf63df9 = L.popup({"maxWidth": "100%"});

        
            var html_bf44e59fad2c4f5f8bf82563a2f9f949 = $(`<div id="html_bf44e59fad2c4f5f8bf82563a2f9f949" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dbis.uibk.ac.at/sites/default/files/2019-06/paper_1.pdf">Language Models for Next-Track Music Recommendation.</a><br></div>`)[0];
            popup_f33b34581e904ffa94948d2ffbf63df9.setContent(html_bf44e59fad2c4f5f8bf82563a2f9f949);
        

        circle_marker_b32e0fb91ec0440ba7e274abf83720f8.bindPopup(popup_f33b34581e904ffa94948d2ffbf63df9)
        ;

        
    
    
            var circle_marker_4ee59e3a8c94417eb85566e01089c627 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6c60089a652f420eaf82c1657059b3ac = L.popup({"maxWidth": "100%"});

        
            var html_2963405d4d9343049036482424c7a826 = $(`<div id="html_2963405d4d9343049036482424c7a826" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-10997-4_42">Machine learning approaches to hybrid music recommender systems</a><br></div>`)[0];
            popup_6c60089a652f420eaf82c1657059b3ac.setContent(html_2963405d4d9343049036482424c7a826);
        

        circle_marker_4ee59e3a8c94417eb85566e01089c627.bindPopup(popup_6c60089a652f420eaf82c1657059b3ac)
        ;

        
    
    
            var circle_marker_0ac3aea653df43c0b864c92ab520574a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f1017ef9cb8749ea873a736df6a8ab6b = L.popup({"maxWidth": "100%"});

        
            var html_f4a2c66335ea4e79a5cd53449c79d16d = $(`<div id="html_f4a2c66335ea4e79a5cd53449c79d16d" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://transactions.ismir.net/articles/10.5334/tismir.39/?utm_source=TrendMD&utm_medium=cpc&utm_campaign=Transactions_of_the_International_Society_for_Music_Information_Retrieval_TrendMD_0">Modeling Popularity and Temporal Drift of Music Genre Preferences</a><br></div>`)[0];
            popup_f1017ef9cb8749ea873a736df6a8ab6b.setContent(html_f4a2c66335ea4e79a5cd53449c79d16d);
        

        circle_marker_0ac3aea653df43c0b864c92ab520574a.bindPopup(popup_f1017ef9cb8749ea873a736df6a8ab6b)
        ;

        
    
    
            var circle_marker_737c82d6287d4766a97794a8eae88223 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a2c88f93416448e6b7080cb6b15e84ca = L.popup({"maxWidth": "100%"});

        
            var html_6e9ef8ebdbfc4e7c8ed8a14d74d6eed7 = $(`<div id="html_6e9ef8ebdbfc4e7c8ed8a14d74d6eed7" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2004.10618">Moment-Based Domain Adaptation: Learning Bounds and Algorithms</a><br></div>`)[0];
            popup_a2c88f93416448e6b7080cb6b15e84ca.setContent(html_6e9ef8ebdbfc4e7c8ed8a14d74d6eed7);
        

        circle_marker_737c82d6287d4766a97794a8eae88223.bindPopup(popup_a2c88f93416448e6b7080cb6b15e84ca)
        ;

        
    
    
            var circle_marker_e692126d2b0a4dfba1ad3ddd1e103b79 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_80db484e68f944c4a63b120066843ba3 = L.popup({"maxWidth": "100%"});

        
            var html_20baf4bbb806482cbcbf8a7a04269f62 = $(`<div id="html_20baf4bbb806482cbcbf8a7a04269f62" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877452/">Movie genome recommender: A novel recommender system based on multimedia content</a><br></div>`)[0];
            popup_80db484e68f944c4a63b120066843ba3.setContent(html_20baf4bbb806482cbcbf8a7a04269f62);
        

        circle_marker_e692126d2b0a4dfba1ad3ddd1e103b79.bindPopup(popup_80db484e68f944c4a63b120066843ba3)
        ;

        
    
    
            var circle_marker_f5a8a317ac37498194e4428bc997473c = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7c302a03d3634f3d8b41fa4940ba65d0 = L.popup({"maxWidth": "100%"});

        
            var html_a27bc43a8be54e6cbc983a38ba41caf0 = $(`<div id="html_a27bc43a8be54e6cbc983a38ba41caf0" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2002.00251">Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with Visual Computing for Improved Music Video Analysis</a><br></div>`)[0];
            popup_7c302a03d3634f3d8b41fa4940ba65d0.setContent(html_a27bc43a8be54e6cbc983a38ba41caf0);
        

        circle_marker_f5a8a317ac37498194e4428bc997473c.bindPopup(popup_7c302a03d3634f3d8b41fa4940ba65d0)
        ;

        
    
    
            var circle_marker_a97821f0e8ea47289a99861583abea12 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b79f40281ed34b139b2e0d06157cb4b4 = L.popup({"maxWidth": "100%"});

        
            var html_3f9fc58a95ee42febe25877595c450e4 = $(`<div id="html_3f9fc58a95ee42febe25877595c450e4" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1811.04419">Multi-temporal resolution convolutional neural networks for acoustic scene classification</a><br></div>`)[0];
            popup_b79f40281ed34b139b2e0d06157cb4b4.setContent(html_3f9fc58a95ee42febe25877595c450e4);
        

        circle_marker_a97821f0e8ea47289a99861583abea12.bindPopup(popup_b79f40281ed34b139b2e0d06157cb4b4)
        ;

        
    
    
            var circle_marker_3210d905e6194bf19451053a467bc357 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_85f5d7f079df4419acc8cf3abd52b357 = L.popup({"maxWidth": "100%"});

        
            var html_6ab5038886db41ec87ee083fa249bdb5 = $(`<div id="html_6ab5038886db41ec87ee083fa249bdb5" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3241620">Multimedia recommender systems</a><br></div>`)[0];
            popup_85f5d7f079df4419acc8cf3abd52b357.setContent(html_6ab5038886db41ec87ee083fa249bdb5);
        

        circle_marker_3210d905e6194bf19451053a467bc357.bindPopup(popup_85f5d7f079df4419acc8cf3abd52b357)
        ;

        
    
    
            var circle_marker_1bea4e5ce975490ba5cad9c08471d38e = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ac7a7b65cccb4345a017472e6c77d1f0 = L.popup({"maxWidth": "100%"});

        
            var html_06d14d12b57d420991ea43443d8a3b66 = $(`<div id="html_06d14d12b57d420991ea43443d8a3b66" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://www.cp.jku.at/people/vall/vall_etal_ecml-pkdd2017.pdf">Music Playlist Continuation by Learning from Hand-Curated Examples and Song Features</a><br></div>`)[0];
            popup_ac7a7b65cccb4345a017472e6c77d1f0.setContent(html_06d14d12b57d420991ea43443d8a3b66);
        

        circle_marker_1bea4e5ce975490ba5cad9c08471d38e.bindPopup(popup_ac7a7b65cccb4345a017472e6c77d1f0)
        ;

        
    
    
            var circle_marker_db09c7e4c9264531bfbea36e58841868 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_cd62efb3178e421cbd77435e40c86961 = L.popup({"maxWidth": "100%"});

        
            var html_bd2e053777ae4468acd1d777dd38d6c3 = $(`<div id="html_bd2e053777ae4468acd1d777dd38d6c3" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3125486.3125494">Music playlist continuation by learning from hand-curated examples and song features: Alleviating the cold-start problem for rare and out-of-set songs</a><br></div>`)[0];
            popup_cd62efb3178e421cbd77435e40c86961.setContent(html_bd2e053777ae4468acd1d777dd38d6c3);
        

        circle_marker_db09c7e4c9264531bfbea36e58841868.bindPopup(popup_cd62efb3178e421cbd77435e40c86961)
        ;

        
    
    
            var circle_marker_25a5f0cffc6f44a1a87e5c77b8a1ae74 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b1a40ab4fc184d44a54a96d58022ef27 = L.popup({"maxWidth": "100%"});

        
            var html_916d3a10c60140eaa4a7a476784d59ba = $(`<div id="html_916d3a10c60140eaa4a7a476784d59ba" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/2484028.2484193">Music similarity and retrieval</a><br></div>`)[0];
            popup_b1a40ab4fc184d44a54a96d58022ef27.setContent(html_916d3a10c60140eaa4a7a476784d59ba);
        

        circle_marker_25a5f0cffc6f44a1a87e5c77b8a1ae74.bindPopup(popup_b1a40ab4fc184d44a54a96d58022ef27)
        ;

        
    
    
            var circle_marker_4b0433e00c88442c94429ca9250029e8 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_340f6735d161484e8b69966dca21708a = L.popup({"maxWidth": "100%"});

        
            var html_366d5cadb5d94e1ebd6fd0b4d7ba7526 = $(`<div id="html_366d5cadb5d94e1ebd6fd0b4d7ba7526" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Hamid_Eghbal-Zadeh/publication/305687848_NOISE_ROBUST_MUSIC_ARTIST_RECOGNITION_USING_I-VECTOR_FEATURES/links/5799cded08aeb0ffcd0f818c/NOISE-ROBUST-MUSIC-ARTIST-RECOGNITION-USING-I-VECTOR-FEATURES.pdf">Noise robust music artist recognition using i-vector</a><br></div>`)[0];
            popup_340f6735d161484e8b69966dca21708a.setContent(html_366d5cadb5d94e1ebd6fd0b4d7ba7526);
        

        circle_marker_4b0433e00c88442c94429ca9250029e8.bindPopup(popup_340f6735d161484e8b69966dca21708a)
        ;

        
    
    
            var circle_marker_1894756c23e0426c8f3498607a6605ae = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_91c755d84ff84acea97e7748883dc668 = L.popup({"maxWidth": "100%"});

        
            var html_6a16402ae3214e12a8fcbb55ca985889 = $(`<div id="html_6a16402ae3214e12a8fcbb55ca985889" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2008.02194">On the Characterization of Expressive Performance in Classical Music: First Results of the Con Espressione Game</a><br></div>`)[0];
            popup_91c755d84ff84acea97e7748883dc668.setContent(html_6a16402ae3214e12a8fcbb55ca985889);
        

        circle_marker_1894756c23e0426c8f3498607a6605ae.bindPopup(popup_91c755d84ff84acea97e7748883dc668)
        ;

        
    
    
            var circle_marker_669ebec4007d4e1dbb80531c63f28112 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d7db437dc85b4579a201b553dbfbfeda = L.popup({"maxWidth": "100%"});

        
            var html_863a4efb735f438089a72f83b39981ff = $(`<div id="html_863a4efb735f438089a72f83b39981ff" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://www.ifs.tuwien.ac.at/~vogl/slides/ismir-18-exhibition.pdf">Pose Estimation</a><br></div>`)[0];
            popup_d7db437dc85b4579a201b553dbfbfeda.setContent(html_863a4efb735f438089a72f83b39981ff);
        

        circle_marker_669ebec4007d4e1dbb80531c63f28112.bindPopup(popup_d7db437dc85b4579a201b553dbfbfeda)
        ;

        
    
    
            var circle_marker_bb269da48db849269904205616590431 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_5585b27877b44ae9a87f23b22fefa07f = L.popup({"maxWidth": "100%"});

        
            var html_0efaa442a8d54529a849f94445f2e2c5 = $(`<div id="html_0efaa442a8d54529a849f94445f2e2c5" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2007.13503">Receptive-Field Regularized CNNs for Music Classification and Tagging</a><br></div>`)[0];
            popup_5585b27877b44ae9a87f23b22fefa07f.setContent(html_0efaa442a8d54529a849f94445f2e2c5);
        

        circle_marker_bb269da48db849269904205616590431.bindPopup(popup_5585b27877b44ae9a87f23b22fefa07f)
        ;

        
    
    
            var circle_marker_8870299f24dd43bda5acdaf17b52c8a0 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8e898a83b09a46758b6c4161801caf21 = L.popup({"maxWidth": "100%"});

        
            var html_8f8a0ee3f4944b5b9b672e812797d46d = $(`<div id="html_8f8a0ee3f4944b5b9b672e812797d46d" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1909.02859">Receptive-field-regularized CNN variants for acoustic scene classification</a><br></div>`)[0];
            popup_8e898a83b09a46758b6c4161801caf21.setContent(html_8f8a0ee3f4944b5b9b672e812797d46d);
        

        circle_marker_8870299f24dd43bda5acdaf17b52c8a0.bindPopup(popup_8e898a83b09a46758b6c4161801caf21)
        ;

        
    
    
            var circle_marker_4a3b046f1acc405eb3f0baf218f2a4ec = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_45557168100d449b92d4f9b1fe056dbe = L.popup({"maxWidth": "100%"});

        
            var html_2e6dd9f6acf8438bb36cd7b4db02c142 = $(`<div id="html_2e6dd9f6acf8438bb36cd7b4db02c142" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/342211693_Recommender_Systems_Leveraging_Multimedia_Content/links/5f1a654d45851515ef44cb80/Recommender-Systems-Leveraging-Multimedia-Content.pdf">Recommender systems leveraging multimedia content</a><br></div>`)[0];
            popup_45557168100d449b92d4f9b1fe056dbe.setContent(html_2e6dd9f6acf8438bb36cd7b4db02c142);
        

        circle_marker_4a3b046f1acc405eb3f0baf218f2a4ec.bindPopup(popup_45557168100d449b92d4f9b1fe056dbe)
        ;

        
    
    
            var circle_marker_cb69124ce5d946caa52a47e5fa34c5a5 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_438d5129265744b3b556f5fa1ac7ea95 = L.popup({"maxWidth": "100%"});

        
            var html_6c4cb7d2eb0b4d6a9ebd02d4a6ca44c7 = $(`<div id="html_6c4cb7d2eb0b4d6a9ebd02d4a6ca44c7" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877420/">Retrieving relevant and diverse movie clips using the mfvcd-7k multifaceted video clip dataset</a><br></div>`)[0];
            popup_438d5129265744b3b556f5fa1ac7ea95.setContent(html_6c4cb7d2eb0b4d6a9ebd02d4a6ca44c7);
        

        circle_marker_cb69124ce5d946caa52a47e5fa34c5a5.bindPopup(popup_438d5129265744b3b556f5fa1ac7ea95)
        ;

        
    
    
            var circle_marker_bc8000a88abf4c4682405938a36f1fa5 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0622489e12da4478bad604af9a484ab1 = L.popup({"maxWidth": "100%"});

        
            var html_45c8690aa57948a28a7aaeb26f8db3d7 = $(`<div id="html_45c8690aa57948a28a7aaeb26f8db3d7" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8587031/">Robust machine learning based acoustic classification of a material transport process</a><br></div>`)[0];
            popup_0622489e12da4478bad604af9a484ab1.setContent(html_45c8690aa57948a28a7aaeb26f8db3d7);
        

        circle_marker_bc8000a88abf4c4682405938a36f1fa5.bindPopup(popup_0622489e12da4478bad604af9a484ab1)
        ;

        
    
    
            var circle_marker_80018fef3aed4ea186e1a6fb49535322 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_eced40dfe54b491c92615dd3f81ce3c8 = L.popup({"maxWidth": "100%"});

        
            var html_621975b737864742bfafe21b856ecf06 = $(`<div id="html_621975b737864742bfafe21b856ecf06" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1810.06897">Sound event detection using weakly-labeled semi-supervised data with GCRNNS, VAT and Self-Adaptive Label Refinement</a><br></div>`)[0];
            popup_eced40dfe54b491c92615dd3f81ce3c8.setContent(html_621975b737864742bfafe21b856ecf06);
        

        circle_marker_80018fef3aed4ea186e1a6fb49535322.bindPopup(popup_eced40dfe54b491c92615dd3f81ce3c8)
        ;

        
    
    
            var circle_marker_ee1884f0cd4241168971189b0b21ef28 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_dce0d385e79d4f979208799bb2750e0d = L.popup({"maxWidth": "100%"});

        
            var html_fa070e832bb941c18fcfb8ad600bc80c = $(`<div id="html_fa070e832bb941c18fcfb8ad600bc80c" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/334548980_The_2019_Multimedia_for_Recommender_System_Task_MovieREC_and_NewsREEL_at_MediaEval/links/5d30c3ac299bf1547cc258c1/The-2019-Multimedia-for-Recommender-System-Task-MovieREC-and-NewsREEL-at-MediaEval.pdf">The 2019 Multimedia for Recommender System Task: MovieREC and NewsREEL at MediaEval</a><br></div>`)[0];
            popup_dce0d385e79d4f979208799bb2750e0d.setContent(html_fa070e832bb941c18fcfb8ad600bc80c);
        

        circle_marker_ee1884f0cd4241168971189b0b21ef28.bindPopup(popup_dce0d385e79d4f979208799bb2750e0d)
        ;

        
    
    
            var circle_marker_df4dceddff58472ebf6238d5ff6e8592 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9a9db41babc148c9b0f6ebfcb395ac14 = L.popup({"maxWidth": "100%"});

        
            var html_bac11c72ce4a45a88e8e154fb1402ff1 = $(`<div id="html_bac11c72ce4a45a88e8e154fb1402ff1" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8902732/">The receptive field as a regularizer in deep convolutional neural networks for acoustic scene classification</a><br></div>`)[0];
            popup_9a9db41babc148c9b0f6ebfcb395ac14.setContent(html_bac11c72ce4a45a88e8e154fb1402ff1);
        

        circle_marker_df4dceddff58472ebf6238d5ff6e8592.bindPopup(popup_9a9db41babc148c9b0f6ebfcb395ac14)
        ;

        
    
    
            var circle_marker_29daca4bb00f4adc9dd7c99689760269 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a2f5f6c7566549d3b3fa937e1a7ab848 = L.popup({"maxWidth": "100%"});

        
            var html_e298920cda9f4f7db265c310b50e5dc1 = $(`<div id="html_e298920cda9f4f7db265c310b50e5dc1" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.jku.at/fileadmin/gruppen/173/Research/vall_mdw_2016.pdf">Timbral and semantic features for music playlists</a><br></div>`)[0];
            popup_a2f5f6c7566549d3b3fa937e1a7ab848.setContent(html_e298920cda9f4f7db265c310b50e5dc1);
        

        circle_marker_29daca4bb00f4adc9dd7c99689760269.bindPopup(popup_a2f5f6c7566549d3b3fa937e1a7ab848)
        ;

        
    
    
            var circle_marker_f4d3ab81687e4b77acc9237c047e0f8c = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2e5122b135844271947253d501390417 = L.popup({"maxWidth": "100%"});

        
            var html_c88c74b695784b479729c8ed5c18dbe6 = $(`<div id="html_c88c74b695784b479729c8ed5c18dbe6" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Dorfer_999.pdf">Training general-purpose audio tagging networks with noisy labels and iterative self-verification</a><br></div>`)[0];
            popup_2e5122b135844271947253d501390417.setContent(html_c88c74b695784b479729c8ed5c18dbe6);
        

        circle_marker_f4d3ab81687e4b77acc9237c047e0f8c.bindPopup(popup_2e5122b135844271947253d501390417)
        ;

        
    
    
            var circle_marker_84d8e75325684d24ad849ba376756b8d = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9eaa634b994d42e2bd0423c3e417c3c2 = L.popup({"maxWidth": "100%"});

        
            var html_960a7de6aa7d41fb8dcf09614e8008ee = $(`<div id="html_960a7de6aa7d41fb8dcf09614e8008ee" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09231-w">Trends in content-based recommendation</a><br></div>`)[0];
            popup_9eaa634b994d42e2bd0423c3e417c3c2.setContent(html_960a7de6aa7d41fb8dcf09614e8008ee);
        

        circle_marker_84d8e75325684d24ad849ba376756b8d.bindPopup(popup_9eaa634b994d42e2bd0423c3e417c3c2)
        ;

        
    
    
            var circle_marker_48573782aa68441e917c4904f0af9c12 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d2ed022f7d244bfe869dca451cf89b00 = L.popup({"maxWidth": "100%"});

        
            var html_90a79600722b42e2834553ed507006b2 = $(`<div id="html_90a79600722b42e2834553ed507006b2" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://books.google.com/books?hl=de&lr=&id=6jvEDwAAQBAJ&oi=fnd&pg=PA223&ots=FUoGquoOv0&sig=3RyQmnd8MD1YU2VZq8Kbbhl_ezU">User Awareness in Music Recommender Systems</a><br></div>`)[0];
            popup_d2ed022f7d244bfe869dca451cf89b00.setContent(html_90a79600722b42e2834553ed507006b2);
        

        circle_marker_48573782aa68441e917c4904f0af9c12.bindPopup(popup_d2ed022f7d244bfe869dca451cf89b00)
        ;

        
    
    
            var circle_marker_99ced6d5e5bc4f25b5d08b8c9e15508f = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f7161d89e5484a35a2a5b9761e0d9599 = L.popup({"maxWidth": "100%"});

        
            var html_e3b9f8df40d642848d7366b59f6366d9 = $(`<div id="html_e3b9f8df40d642848d7366b59f6366d9" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2003.10699">Utilizing Human Memory Processes to Model Genre Preferences for Personalized Music Recommendations</a><br></div>`)[0];
            popup_f7161d89e5484a35a2a5b9761e0d9599.setContent(html_e3b9f8df40d642848d7366b59f6366d9);
        

        circle_marker_99ced6d5e5bc4f25b5d08b8c9e15508f.bindPopup(popup_f7161d89e5484a35a2a5b9761e0d9599)
        ;

        
    
    
            var circle_marker_5cf9c1053e4a47a3b8bd15012dc9b735 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b71f5799321d4d0a906cd85abec29654 = L.popup({"maxWidth": "100%"});

        
            var html_82c62f0ce4754bb9a1fc01da6a6c13a8 = $(`<div id="html_82c62f0ce4754bb9a1fc01da6a6c13a8" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://www.researchgate.net/profile/Hossein_A_Rahmani/publication/337768473_A_Regression_Approach_to_Movie_Rating_Prediction_using_Multimedia_Content_and_Metadata/links/5df8b821a6fdcc283726d8c7/A-Regression-Approach-to-Movie-Rating-Prediction-using-Multimedia-Content-and-Metadata.pdf?_sg%5B0%5D=3b-miF1_LzXGwxQhQxRYO5FfklSPn0YfAlrK6IWfJilrpkC8-9h2ljnRbzLhZjp8z22CM_vzJpiQ-DaxhD7RLQ.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_sg%5B1%5D=0EyWPCbfTe4cM2OzZ6VGxzinAQj21QkakmKNQ9xBAXR2oXBv7SQahoq94YN7zTBLR7KuEuKMzCqZtGzBOqJvKV6rX2V1E4ISvHGuBwdOwd-p.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_iepl=">A Regression Approach to Movie Rating Prediction using Multimedia Content and Metadata</a><br></div>`)[0];
            popup_b71f5799321d4d0a906cd85abec29654.setContent(html_82c62f0ce4754bb9a1fc01da6a6c13a8);
        

        circle_marker_5cf9c1053e4a47a3b8bd15012dc9b735.bindPopup(popup_b71f5799321d4d0a906cd85abec29654)
        ;

        
    
    
            var circle_marker_d68ca8b17cf04d348042bc92baf21426 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_842163ba0ba24ad5a88c3bf5d55daa0f = L.popup({"maxWidth": "100%"});

        
            var html_72807cbca4b648c4a8b5b0eb132f1564 = $(`<div id="html_72807cbca4b648c4a8b5b0eb132f1564" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8964809/">Acoustic Scene Classification using Binaural Representation and Classifier Combination</a><br></div>`)[0];
            popup_842163ba0ba24ad5a88c3bf5d55daa0f.setContent(html_72807cbca4b648c4a8b5b0eb132f1564);
        

        circle_marker_d68ca8b17cf04d348042bc92baf21426.bindPopup(popup_842163ba0ba24ad5a88c3bf5d55daa0f)
        ;

        
    
    
            var circle_marker_b39680223e8f4389a18c202172578de1 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c7b588a1e71447be9b7f82126a3eb2f8 = L.popup({"maxWidth": "100%"});

        
            var html_2cdb38c48a814fb4ae052ea1c7714e45 = $(`<div id="html_2cdb38c48a814fb4ae052ea1c7714e45" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://arxiv.org/abs/1907.07127">Acoustic scene classification using fusion of attentive convolutional neural networks for DCASE2019 challenge</a><br></div>`)[0];
            popup_c7b588a1e71447be9b7f82126a3eb2f8.setContent(html_2cdb38c48a814fb4ae052ea1c7714e45);
        

        circle_marker_b39680223e8f4389a18c202172578de1.bindPopup(popup_c7b588a1e71447be9b7f82126a3eb2f8)
        ;

        
    
    
            var circle_marker_d5d826c834954468a5d8be95950a5575 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_54535fc93a334552a7348afd96418535 = L.popup({"maxWidth": "100%"});

        
            var html_5deea28585a84fb1bde9198bdf6b19bc = $(`<div id="html_5deea28585a84fb1bde9198bdf6b19bc" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://arxiv.org/abs/1810.04273">Convolutional neural networks and x-vector embedding for DCASE2018 acoustic scene classification challenge</a><br></div>`)[0];
            popup_54535fc93a334552a7348afd96418535.setContent(html_5deea28585a84fb1bde9198bdf6b19bc);
        

        circle_marker_d5d826c834954468a5d8be95950a5575.bindPopup(popup_54535fc93a334552a7348afd96418535)
        ;

        
    
    
            var circle_marker_870eebc3af854b41a405342c38706af3 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_dfc7531d6bf54c0dbc85369960597be5 = L.popup({"maxWidth": "100%"});

        
            var html_96cd528e0e9b477da03c4d720a8b56cc = $(`<div id="html_96cd528e0e9b477da03c4d720a8b56cc" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8270136/">On the usage of i-vector representation for online handwritten signature verification</a><br></div>`)[0];
            popup_dfc7531d6bf54c0dbc85369960597be5.setContent(html_96cd528e0e9b477da03c4d720a8b56cc);
        

        circle_marker_870eebc3af854b41a405342c38706af3.bindPopup(popup_dfc7531d6bf54c0dbc85369960597be5)
        ;

        
    
    
            var circle_marker_151be96bbd284a2ea44af7be32df5a19 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a3a2c7d2d5fb44fc9d94578c823363e2 = L.popup({"maxWidth": "100%"});

        
            var html_4eafa76da1a14393a4cebe92a6021b26 = $(`<div id="html_4eafa76da1a14393a4cebe92a6021b26" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://digital-library.theiet.org/content/journals/10.1049/iet-bmt.2017.0059">Online signature verification using i-vector representation</a><br></div>`)[0];
            popup_a3a2c7d2d5fb44fc9d94578c823363e2.setContent(html_4eafa76da1a14393a4cebe92a6021b26);
        

        circle_marker_151be96bbd284a2ea44af7be32df5a19.bindPopup(popup_a3a2c7d2d5fb44fc9d94578c823363e2)
        ;

        
    
    
            var circle_marker_a41dc5e83ea3436e976ab427a8fcaa2d = L.circleMarker(
                [49.8167003, 15.4749544],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1fbf59bb89974527bc8fcc24c813b66e = L.popup({"maxWidth": "100%"});

        
            var html_d0c63c5814fa48899bf5e3c71418a346 = $(`<div id="html_d0c63c5814fa48899bf5e3c71418a346" style="width: 100.0%; height: 100.0%;">Country : Czech Republic<br>                         Paper : <a href="https://arxiv.org/abs/1907.07127">Acoustic scene classification using fusion of attentive convolutional neural networks for DCASE2019 challenge</a><br></div>`)[0];
            popup_1fbf59bb89974527bc8fcc24c813b66e.setContent(html_d0c63c5814fa48899bf5e3c71418a346);
        

        circle_marker_a41dc5e83ea3436e976ab427a8fcaa2d.bindPopup(popup_1fbf59bb89974527bc8fcc24c813b66e)
        ;

        
    
    
            var circle_marker_e665e037f1e94b479728444dae7d9902 = L.circleMarker(
                [49.8167003, 15.4749544],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7af98e51cb404d7ab0086776729f42c1 = L.popup({"maxWidth": "100%"});

        
            var html_6797cf3f74b04ee5814c2b14675d9d64 = $(`<div id="html_6797cf3f74b04ee5814c2b14675d9d64" style="width: 100.0%; height: 100.0%;">Country : Czech Republic<br>                         Paper : <a href="https://arxiv.org/abs/1810.04273">Convolutional neural networks and x-vector embedding for DCASE2018 acoustic scene classification challenge</a><br></div>`)[0];
            popup_7af98e51cb404d7ab0086776729f42c1.setContent(html_6797cf3f74b04ee5814c2b14675d9d64);
        

        circle_marker_e665e037f1e94b479728444dae7d9902.bindPopup(popup_7af98e51cb404d7ab0086776729f42c1)
        ;

        
    
    
            var circle_marker_4bc0e1880b204e20a2382875c6956633 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_17052ef122d54c77a16b7d2e05f05a7c = L.popup({"maxWidth": "100%"});

        
            var html_acaeaaa3200548909f9ef30fcf75583a = $(`<div id="html_acaeaaa3200548909f9ef30fcf75583a" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-981-13-8707-4_8">A Comparison of Attention Mechanisms of Convolutional Neural Network in Weakly Labeled Audio Tagging</a><br></div>`)[0];
            popup_17052ef122d54c77a16b7d2e05f05a7c.setContent(html_acaeaaa3200548909f9ef30fcf75583a);
        

        circle_marker_4bc0e1880b204e20a2382875c6956633.bindPopup(popup_17052ef122d54c77a16b7d2e05f05a7c)
        ;

        
    
    
            var circle_marker_87b9a0df4efa4cfa9b61c23fc041c6ab = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7792b7cfd7a9459a82927ee70b34dbc9 = L.popup({"maxWidth": "100%"});

        
            var html_2e21ffe351d344daa8ea25ddf2b0d82e = $(`<div id="html_2e21ffe351d344daa8ea25ddf2b0d82e" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8794834/">A Novel Social Situation Analytics-Based Recommendation Algorithm for Multimedia Social Networks</a><br></div>`)[0];
            popup_7792b7cfd7a9459a82927ee70b34dbc9.setContent(html_2e21ffe351d344daa8ea25ddf2b0d82e);
        

        circle_marker_87b9a0df4efa4cfa9b61c23fc041c6ab.bindPopup(popup_7792b7cfd7a9459a82927ee70b34dbc9)
        ;

        
    
    
            var circle_marker_7d69e93d75f54356a1d0e6b0dc6181cd = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a7ad61556f1848fe9e31cc30292f2d36 = L.popup({"maxWidth": "100%"});

        
            var html_ad9fbb02b4cd4b5d9597d47d61d11722 = $(`<div id="html_ad9fbb02b4cd4b5d9597d47d61d11722" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/1904.05243">A compact and discriminative feature based on auditory summary statistics for acoustic scene classification</a><br></div>`)[0];
            popup_a7ad61556f1848fe9e31cc30292f2d36.setContent(html_ad9fbb02b4cd4b5d9597d47d61d11722);
        

        circle_marker_7d69e93d75f54356a1d0e6b0dc6181cd.bindPopup(popup_a7ad61556f1848fe9e31cc30292f2d36)
        ;

        
    
    
            var circle_marker_b212d8c023ae44e4892ad2ea87124313 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f2eb1cdb69d043239e40ac039547dfe9 = L.popup({"maxWidth": "100%"});

        
            var html_131b9d9e512d47e7b153519be2572422 = $(`<div id="html_131b9d9e512d47e7b153519be2572422" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Dinkel_38.pdf">A hybrid asr model approach on weakly labeled scene classification</a><br></div>`)[0];
            popup_f2eb1cdb69d043239e40ac039547dfe9.setContent(html_131b9d9e512d47e7b153519be2572422);
        

        circle_marker_b212d8c023ae44e4892ad2ea87124313.bindPopup(popup_f2eb1cdb69d043239e40ac039547dfe9)
        ;

        
    
    
            var circle_marker_e61b86ec87cd4b7daa539acc42fb3e7b = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_895cf6bc1c27406a9077c8349dbca0ab = L.popup({"maxWidth": "100%"});

        
            var html_c13f546eb2774668bd98618afcabf7b5 = $(`<div id="html_c13f546eb2774668bd98618afcabf7b5" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682376/">A region based attention method for weakly supervised sound event detection and classification</a><br></div>`)[0];
            popup_895cf6bc1c27406a9077c8349dbca0ab.setContent(html_c13f546eb2774668bd98618afcabf7b5);
        

        circle_marker_e61b86ec87cd4b7daa539acc42fb3e7b.bindPopup(popup_895cf6bc1c27406a9077c8349dbca0ab)
        ;

        
    
    
            var circle_marker_3f3c55d4361a47d4891c4fa49219bbc9 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2a2a3e43f1b24beabd2e3923265d6304 = L.popup({"maxWidth": "100%"});

        
            var html_6e01259b86854022bea2b346e63e764c = $(`<div id="html_6e01259b86854022bea2b346e63e764c" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s00034-019-01094-1">A survey: Neural network-based deep learning for acoustic event detection</a><br></div>`)[0];
            popup_2a2a3e43f1b24beabd2e3923265d6304.setContent(html_6e01259b86854022bea2b346e63e764c);
        

        circle_marker_3f3c55d4361a47d4891c4fa49219bbc9.bindPopup(popup_2a2a3e43f1b24beabd2e3923265d6304)
        ;

        
    
    
            var circle_marker_137319ace9ea495fbb9286f7436f6095 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_4421d12fe14644d2ae1e591537fa723b = L.popup({"maxWidth": "100%"});

        
            var html_432a5cb303fe47da801b76b0d911bb93 = $(`<div id="html_432a5cb303fe47da801b76b0d911bb93" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2005.13146">ACGAN-based Data Augmentation Integrated with Long-term Scalogram for Acoustic Scene Classification</a><br></div>`)[0];
            popup_4421d12fe14644d2ae1e591537fa723b.setContent(html_432a5cb303fe47da801b76b0d911bb93);
        

        circle_marker_137319ace9ea495fbb9286f7436f6095.bindPopup(popup_4421d12fe14644d2ae1e591537fa723b)
        ;

        
    
    
            var circle_marker_4cb3ce9616644d2bb6916fee72607a1a = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_155f4aa754d14e649ed516e5588e5e0d = L.popup({"maxWidth": "100%"});

        
            var html_e8c93c3247524d5e89947c9301a53148 = $(`<div id="html_e8c93c3247524d5e89947c9301a53148" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://api.research-repository.uwa.edu.au/portalfiles/portal/50929769/THESIS_DOCTOR_OF_PHILOSOPHY_XIA_Xianjun_2019.pdf">Acoustic Event Detection Utilizing Event Class and Localization Information</a><br></div>`)[0];
            popup_155f4aa754d14e649ed516e5588e5e0d.setContent(html_e8c93c3247524d5e89947c9301a53148);
        

        circle_marker_4cb3ce9616644d2bb6916fee72607a1a.bindPopup(popup_155f4aa754d14e649ed516e5588e5e0d)
        ;

        
    
    
            var circle_marker_4ce55738a81b4d19b312de262c9a9036 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_860520c6ead14ddab1a51ec0f532f3e9 = L.popup({"maxWidth": "100%"});

        
            var html_5d180d8012f94c0f8dacc90965ae0d3e = $(`<div id="html_5d180d8012f94c0f8dacc90965ae0d3e" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/1904.05204">Acoustic scene classification by implicitly identifying distinct sound events</a><br></div>`)[0];
            popup_860520c6ead14ddab1a51ec0f532f3e9.setContent(html_5d180d8012f94c0f8dacc90965ae0d3e);
        

        circle_marker_4ce55738a81b4d19b312de262c9a9036.bindPopup(popup_860520c6ead14ddab1a51ec0f532f3e9)
        ;

        
    
    
            var circle_marker_ad4349cd2c024756a460e61c0b47d84b = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_fe6b37b8aae14c03b7950f0318af2154 = L.popup({"maxWidth": "100%"});

        
            var html_6354b074a7f44586ba1bd1726fb04396 = $(`<div id="html_6354b074a7f44586ba1bd1726fb04396" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Yang_22.pdf">Acoustic scene classification using CNN ensembles and primary ambient extraction</a><br></div>`)[0];
            popup_fe6b37b8aae14c03b7950f0318af2154.setContent(html_6354b074a7f44586ba1bd1726fb04396);
        

        circle_marker_ad4349cd2c024756a460e61c0b47d84b.bindPopup(popup_fe6b37b8aae14c03b7950f0318af2154)
        ;

        
    
    
            var circle_marker_66941c9f6ec94ea39b82f51481fb69eb = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_4763d39e67ca47bfb43f5ff369604585 = L.popup({"maxWidth": "100%"});

        
            var html_7dbb600276184c09be7c6d1c5682cc99 = $(`<div id="html_7dbb600276184c09be7c6d1c5682cc99" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8455765/">Acoustic scene classification using deep audio feature and BLSTM network</a><br></div>`)[0];
            popup_4763d39e67ca47bfb43f5ff369604585.setContent(html_7dbb600276184c09be7c6d1c5682cc99);
        

        circle_marker_66941c9f6ec94ea39b82f51481fb69eb.bindPopup(popup_4763d39e67ca47bfb43f5ff369604585)
        ;

        
    
    
            var circle_marker_0572c6653fae4530b9b226e1d7582faf = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1a1b0f10715d441a9b8b502f52b575dc = L.popup({"maxWidth": "100%"});

        
            var html_c1708fea378b4b0da124d85d6018d451 = $(`<div id="html_c1708fea378b4b0da124d85d6018d451" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8897625/">Adaptive multi-scale detection of acoustic events</a><br></div>`)[0];
            popup_1a1b0f10715d441a9b8b502f52b575dc.setContent(html_c1708fea378b4b0da124d85d6018d451);
        

        circle_marker_0572c6653fae4530b9b226e1d7582faf.bindPopup(popup_1a1b0f10715d441a9b8b502f52b575dc)
        ;

        
    
    
            var circle_marker_1d36843b30764b70a3df571a51189ff5 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2a0653c231af4fb080f79a43822645bb = L.popup({"maxWidth": "100%"});

        
            var html_e897a037eaf343618fd5b2dce5468462 = $(`<div id="html_e897a037eaf343618fd5b2dce5468462" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2008.00107">An Acoustic Segment Model Based Segment Unit Selection Approach to Acoustic Scene Classification with Partial Utterances</a><br></div>`)[0];
            popup_2a0653c231af4fb080f79a43822645bb.setContent(html_e897a037eaf343618fd5b2dce5468462);
        

        circle_marker_1d36843b30764b70a3df571a51189ff5.bindPopup(popup_2a0653c231af4fb080f79a43822645bb)
        ;

        
    
    
            var circle_marker_a201c8d83c0e46d18e37140be806976a = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_29846843513e45c291cff850580e21c6 = L.popup({"maxWidth": "100%"});

        
            var html_b9a3afa2e13149a996da17488ae3bb15 = $(`<div id="html_b9a3afa2e13149a996da17488ae3bb15" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://joics.org/gallery/ics-3652.pdf">An Exploratory Study of Collaborative Filtering Vs. Content Based Movie Recommendation</a><br></div>`)[0];
            popup_29846843513e45c291cff850580e21c6.setContent(html_b9a3afa2e13149a996da17488ae3bb15);
        

        circle_marker_a201c8d83c0e46d18e37140be806976a.bindPopup(popup_29846843513e45c291cff850580e21c6)
        ;

        
    
    
            var circle_marker_6d23571146f2457fade1e72504c99427 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6ca6b2c3f86f41519ecacd39f1516412 = L.popup({"maxWidth": "100%"});

        
            var html_5c6cdddfb01b40ef89557e88a2f544ef = $(`<div id="html_5c6cdddfb01b40ef89557e88a2f544ef" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8706712/">An investigation of transfer learning mechanism for acoustic scene classification</a><br></div>`)[0];
            popup_6ca6b2c3f86f41519ecacd39f1516412.setContent(html_5c6cdddfb01b40ef89557e88a2f544ef);
        

        circle_marker_6d23571146f2457fade1e72504c99427.bindPopup(popup_6ca6b2c3f86f41519ecacd39f1516412)
        ;

        
    
    
            var circle_marker_325668ad4d27469d930164166722e296 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b5a212374e144c82885fc0b1617c0065 = L.popup({"maxWidth": "100%"});

        
            var html_7e085cb404e2412b81ce935d08c92c39 = $(`<div id="html_7e085cb404e2412b81ce935d08c92c39" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://ir.ia.ac.cn/bitstream/173211/21601/1/bare_conf.pdf">Anomaly Detection via Minimum Likelihood Generative Adversarial Networks</a><br></div>`)[0];
            popup_b5a212374e144c82885fc0b1617c0065.setContent(html_7e085cb404e2412b81ce935d08c92c39);
        

        circle_marker_325668ad4d27469d930164166722e296.bindPopup(popup_b5a212374e144c82885fc0b1617c0065)
        ;

        
    
    
            var circle_marker_e4f6f68e1f6d4f168da0f1bcb86f8a04 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_99f0cfbbfc4a4d9799d0e95c217e766c = L.popup({"maxWidth": "100%"});

        
            var html_d75bd37f023e41919d31dc08b4d2e557 = $(`<div id="html_d75bd37f023e41919d31dc08b4d2e557" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8545381/">Anomaly detection via minimum likelihood generative adversarial networks</a><br></div>`)[0];
            popup_99f0cfbbfc4a4d9799d0e95c217e766c.setContent(html_d75bd37f023e41919d31dc08b4d2e557);
        

        circle_marker_e4f6f68e1f6d4f168da0f1bcb86f8a04.bindPopup(popup_99f0cfbbfc4a4d9799d0e95c217e766c)
        ;

        
    
    
            var circle_marker_5d9190406e284b4b97642a3eb31ddf98 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b3b17b5903e84c1ca30c5681d2831104 = L.popup({"maxWidth": "100%"});

        
            var html_ce388fe8212a405eb0c008f4f00862b3 = $(`<div id="html_ce388fe8212a405eb0c008f4f00862b3" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Zhang_78.pdf">BUPT SUBMISSIONS TO DCASE 2020: LOW-COMPLEXITY ACOUSTIC SCENE CLASSIFICATION WITH POST TRAINING STATIC QUANTIZATION AND …</a><br></div>`)[0];
            popup_b3b17b5903e84c1ca30c5681d2831104.setContent(html_ce388fe8212a405eb0c008f4f00862b3);
        

        circle_marker_5d9190406e284b4b97642a3eb31ddf98.bindPopup(popup_b3b17b5903e84c1ca30c5681d2831104)
        ;

        
    
    
            var circle_marker_c96db76d453545148171e5165c893f28 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_748ff548bbcd496584fb80241c8b398d = L.popup({"maxWidth": "100%"});

        
            var html_eb0802d9ee054525925ba293ca8a892f = $(`<div id="html_eb0802d9ee054525925ba293ca8a892f" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8689028/">CMBPR: Category-Aided Multi-Channel Bayesian Personalized Ranking for Short Video Recommendation</a><br></div>`)[0];
            popup_748ff548bbcd496584fb80241c8b398d.setContent(html_eb0802d9ee054525925ba293ca8a892f);
        

        circle_marker_c96db76d453545148171e5165c893f28.bindPopup(popup_748ff548bbcd496584fb80241c8b398d)
        ;

        
    
    
            var circle_marker_7d0ec4b773dc4ec5ab056311b21019e3 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_14b13414f8294490874436ca20d1c3a9 = L.popup({"maxWidth": "100%"});

        
            var html_59e3d57ea5474d8eb0319f992529ef52 = $(`<div id="html_59e3d57ea5474d8eb0319f992529ef52" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Zhu_52.pdf">DCASE 2019 challenge task1 technical report</a><br></div>`)[0];
            popup_14b13414f8294490874436ca20d1c3a9.setContent(html_59e3d57ea5474d8eb0319f992529ef52);
        

        circle_marker_7d0ec4b773dc4ec5ab056311b21019e3.bindPopup(popup_14b13414f8294490874436ca20d1c3a9)
        ;

        
    
    
            var circle_marker_53d3f5f8fe874c2585ad71be226ca1ee = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f227fc72b84641e8bbcc51cd915d2828 = L.popup({"maxWidth": "100%"});

        
            var html_f95396a3a85a4714aebcf55671e17e02 = $(`<div id="html_f95396a3a85a4714aebcf55671e17e02" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2007.12864">DD-CNN: Depthwise Disout Convolutional Neural Network for Low-complexity Acoustic Scene Classification</a><br></div>`)[0];
            popup_f227fc72b84641e8bbcc51cd915d2828.setContent(html_f95396a3a85a4714aebcf55671e17e02);
        

        circle_marker_53d3f5f8fe874c2585ad71be226ca1ee.bindPopup(popup_f227fc72b84641e8bbcc51cd915d2828)
        ;

        
    
    
            var circle_marker_05fd5f0c23ba4f73823a358e0d2a4b2a = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7aac75b5daaa4d7f8be35af092b0f767 = L.popup({"maxWidth": "100%"});

        
            var html_ead7e258ca7549588e65839c8a860a06 = $(`<div id="html_ead7e258ca7549588e65839c8a860a06" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053459/">Duration robust weakly supervised sound event detection</a><br></div>`)[0];
            popup_7aac75b5daaa4d7f8be35af092b0f767.setContent(html_ead7e258ca7549588e65839c8a860a06);
        

        circle_marker_05fd5f0c23ba4f73823a358e0d2a4b2a.bindPopup(popup_7aac75b5daaa4d7f8be35af092b0f767)
        ;

        
    
    
            var circle_marker_4e6fb66785ec4f8d93c5cd2b1e60ccaa = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7f8df37e9f0c40759767e83f52b92cef = L.popup({"maxWidth": "100%"});

        
            var html_7f9fd74efb6d40b383f925b1689fa1ce = $(`<div id="html_7f9fd74efb6d40b383f925b1689fa1ce" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://web.pkusz.edu.cn/adsp/files/2020/08/Interspeech2020_%E7%8E%8B%E8%B5%AB%E9%BA%9F_Environmental-Sound-Classification-with-Parallel-Temporal-spectral-Attention.pdf">Environmental sound classification with parallel temporal-spectral attention</a><br></div>`)[0];
            popup_7f8df37e9f0c40759767e83f52b92cef.setContent(html_7f9fd74efb6d40b383f925b1689fa1ce);
        

        circle_marker_4e6fb66785ec4f8d93c5cd2b1e60ccaa.bindPopup(popup_7f8df37e9f0c40759767e83f52b92cef)
        ;

        
    
    
            var circle_marker_e6c9e4d3b5054e3d9067991baaa91b5f = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c600cde84b4141c1aa8ef119c5c5992e = L.popup({"maxWidth": "100%"});

        
            var html_0c4e8466569344c4b3405e45075f6cfa = $(`<div id="html_0c4e8466569344c4b3405e45075f6cfa" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8869872/">Feature Re-Learning with Data Augmentation for Video Relevance Prediction</a><br></div>`)[0];
            popup_c600cde84b4141c1aa8ef119c5c5992e.setContent(html_0c4e8466569344c4b3405e45075f6cfa);
        

        circle_marker_e6c9e4d3b5054e3d9067991baaa91b5f.bindPopup(popup_c600cde84b4141c1aa8ef119c5c5992e)
        ;

        
    
    
            var circle_marker_bb318dbd60b64c079020d55aeede8266 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_225400e04da64625b903920eb58c8a75 = L.popup({"maxWidth": "100%"});

        
            var html_764fa3326ae9463084876a1d5e5148bc = $(`<div id="html_764fa3326ae9463084876a1d5e5148bc" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Xu_131.pdf">Fusion model based on convolutional neural networks with two features for acoustic scene classification</a><br></div>`)[0];
            popup_225400e04da64625b903920eb58c8a75.setContent(html_764fa3326ae9463084876a1d5e5148bc);
        

        circle_marker_bb318dbd60b64c079020d55aeede8266.bindPopup(popup_225400e04da64625b903920eb58c8a75)
        ;

        
    
    
            var circle_marker_ce74bfc53b724272989635cde0358825 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_20fd483354c54c9190da2c1e0f89225e = L.popup({"maxWidth": "100%"});

        
            var html_a4bd538703c1416a85aaab155f4a48c1 = $(`<div id="html_a4bd538703c1416a85aaab155f4a48c1" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2003.12222">GPVAD: Towards noise robust voice activity detection via weakly supervised sound event detection</a><br></div>`)[0];
            popup_20fd483354c54c9190da2c1e0f89225e.setContent(html_a4bd538703c1416a85aaab155f4a48c1);
        

        circle_marker_ce74bfc53b724272989635cde0358825.bindPopup(popup_20fd483354c54c9190da2c1e0f89225e)
        ;

        
    
    
            var circle_marker_5a4cc6489d494a08a6a73b3e0ea53287 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_491de4ac794644be944d4886a324289d = L.popup({"maxWidth": "100%"});

        
            var html_a7d3dfe57bf14bd28d434e321b6fabed = $(`<div id="html_a7d3dfe57bf14bd28d434e321b6fabed" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/1909.06178">Guided learning convolution system for dcase 2019 task 4</a><br></div>`)[0];
            popup_491de4ac794644be944d4886a324289d.setContent(html_a7d3dfe57bf14bd28d434e321b6fabed);
        

        circle_marker_5a4cc6489d494a08a6a73b3e0ea53287.bindPopup(popup_491de4ac794644be944d4886a324289d)
        ;

        
    
    
            var circle_marker_477a220dac6c402f9d8d8b745fd505e1 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_93f1a0dcd46a4442ab42a82fbcfc5206 = L.popup({"maxWidth": "100%"});

        
            var html_1cdd590e5a7048f3ba4c71a93913d077 = $(`<div id="html_1cdd590e5a7048f3ba4c71a93913d077" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053584/">Guided learning for weakly-labeled semi-supervised sound event detection</a><br></div>`)[0];
            popup_93f1a0dcd46a4442ab42a82fbcfc5206.setContent(html_1cdd590e5a7048f3ba4c71a93913d077);
        

        circle_marker_477a220dac6c402f9d8d8b745fd505e1.bindPopup(popup_93f1a0dcd46a4442ab42a82fbcfc5206)
        ;

        
    
    
            var circle_marker_66b171232e8847ce9b6264afbe9f7792 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c15681f91cea49d196905d5b33e37da2 = L.popup({"maxWidth": "100%"});

        
            var html_ad84e57107e7404cba21ae5bf320d389 = $(`<div id="html_ad84e57107e7404cba21ae5bf320d389" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053519/">High-Resolution Attention Network with Acoustic Segment Model for Acoustic Scene Classification</a><br></div>`)[0];
            popup_c15681f91cea49d196905d5b33e37da2.setContent(html_ad84e57107e7404cba21ae5bf320d389);
        

        circle_marker_66b171232e8847ce9b6264afbe9f7792.bindPopup(popup_c15681f91cea49d196905d5b33e37da2)
        ;

        
    
    
            var circle_marker_492427df26a4465f9268f84dde7ab6be = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_dc45431feb28405393b6bc0f1c91267f = L.popup({"maxWidth": "100%"});

        
            var html_fa819dacf2074784bb84207c16c4d70e = $(`<div id="html_fa819dacf2074784bb84207c16c4d70e" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9023236/">Hybrid Constant-Q Transform Based CNN Ensemble for Acoustic Scene Classification</a><br></div>`)[0];
            popup_dc45431feb28405393b6bc0f1c91267f.setContent(html_fa819dacf2074784bb84207c16c4d70e);
        

        circle_marker_492427df26a4465f9268f84dde7ab6be.bindPopup(popup_dc45431feb28405393b6bc0f1c91267f)
        ;

        
    
    
            var circle_marker_c1f6ef6e52cf43f19a8612dabacd6bdf = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_74bc4363a8e7479384f41c059f587f8f = L.popup({"maxWidth": "100%"});

        
            var html_f025c3368c4a49ae82d3a74f40aceebf = $(`<div id="html_f025c3368c4a49ae82d3a74f40aceebf" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0957417419301794">Hybrid feature-based analysis of video's affective content using protagonist detection</a><br></div>`)[0];
            popup_74bc4363a8e7479384f41c059f587f8f.setContent(html_f025c3368c4a49ae82d3a74f40aceebf);
        

        circle_marker_c1f6ef6e52cf43f19a8612dabacd6bdf.bindPopup(popup_74bc4363a8e7479384f41c059f587f8f)
        ;

        
    
    
            var circle_marker_a3c6be1437a1446085735a2badfa5528 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d2bfcc2f6dbe4fedb586d9ddd09c15fd = L.popup({"maxWidth": "100%"});

        
            var html_4e49044dcf19452d819862eb608de967 = $(`<div id="html_4e49044dcf19452d819862eb608de967" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2004.02182">Imbalanced Data Learning by Minority Class Augmentation using Capsule Adversarial Networks</a><br></div>`)[0];
            popup_d2bfcc2f6dbe4fedb586d9ddd09c15fd.setContent(html_4e49044dcf19452d819862eb608de967);
        

        circle_marker_a3c6be1437a1446085735a2badfa5528.bindPopup(popup_d2bfcc2f6dbe4fedb586d9ddd09c15fd)
        ;

        
    
    
            var circle_marker_d2e3dc03b4a34ec3afa9baa1d78fe374 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_33e4a206d26f47d8aad66bc59e69a2c8 = L.popup({"maxWidth": "100%"});

        
            var html_12bf32b3184340849658be0f6d3b0a50 = $(`<div id="html_12bf32b3184340849658be0f6d3b0a50" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.mdpi.com/1099-4300/22/9/1055/htm">Improving Multi-Agent Generative Adversarial Nets with Variational Latent Representation</a><br></div>`)[0];
            popup_33e4a206d26f47d8aad66bc59e69a2c8.setContent(html_12bf32b3184340849658be0f6d3b0a50);
        

        circle_marker_d2e3dc03b4a34ec3afa9baa1d78fe374.bindPopup(popup_33e4a206d26f47d8aad66bc59e69a2c8)
        ;

        
    
    
            var circle_marker_199c89f838d14baf980968950ccc013a = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_14bd96b2b3a04caaaacf7dce081c4a77 = L.popup({"maxWidth": "100%"});

        
            var html_008019fa34344afc99edb5aa97b951f4 = $(`<div id="html_008019fa34344afc99edb5aa97b951f4" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/1907.06639">Integrating the data augmentation scheme with various classifiers for acoustic scene modeling</a><br></div>`)[0];
            popup_14bd96b2b3a04caaaacf7dce081c4a77.setContent(html_008019fa34344afc99edb5aa97b951f4);
        

        circle_marker_199c89f838d14baf980968950ccc013a.bindPopup(popup_14bd96b2b3a04caaaacf7dce081c4a77)
        ;

        
    
    
            var circle_marker_6b26b3bbe696455181373905289f2211 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_5ee1a9dc83314873a1124298fcfe1b93 = L.popup({"maxWidth": "100%"});

        
            var html_c9f1545916494d26ba6f1c2e6b502e81 = $(`<div id="html_c9f1545916494d26ba6f1c2e6b502e81" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.ijcai.org/Proceedings/2020/0379.pdf">Internal and Contextual Attention Network for Cold-start Multi-channel Matching in Recommendation</a><br></div>`)[0];
            popup_5ee1a9dc83314873a1124298fcfe1b93.setContent(html_c9f1545916494d26ba6f1c2e6b502e81);
        

        circle_marker_6b26b3bbe696455181373905289f2211.bindPopup(popup_5ee1a9dc83314873a1124298fcfe1b93)
        ;

        
    
    
            var circle_marker_7544def85f4e4d9d8f352e6f5f1740a4 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_73b10bed9296407b9b939d781c27da2d = L.popup({"maxWidth": "100%"});

        
            var html_36c64ba257fb4482bdea6beab82e4a28 = $(`<div id="html_36c64ba257fb4482bdea6beab82e4a28" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8922774/">Investigation of Different CNN-Based Models for Improved Bird Sound Classification</a><br></div>`)[0];
            popup_73b10bed9296407b9b939d781c27da2d.setContent(html_36c64ba257fb4482bdea6beab82e4a28);
        

        circle_marker_7544def85f4e4d9d8f352e6f5f1740a4.bindPopup(popup_73b10bed9296407b9b939d781c27da2d)
        ;

        
    
    
            var circle_marker_e8f75863d77f46beaeca91aa48bbbaea = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ccdd6d720fc34109ac591283363d9a69 = L.popup({"maxWidth": "100%"});

        
            var html_5d2476ec1f094573a9c95441f09d331a = $(`<div id="html_5d2476ec1f094573a9c95441f09d331a" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0957417419300661">Investigation of acoustic and visual features for acoustic scene classification</a><br></div>`)[0];
            popup_ccdd6d720fc34109ac591283363d9a69.setContent(html_5d2476ec1f094573a9c95441f09d331a);
        

        circle_marker_e8f75863d77f46beaeca91aa48bbbaea.bindPopup(popup_ccdd6d720fc34109ac591283363d9a69)
        ;

        
    
    
            var circle_marker_346e6923f5f94e098fca30106657bfee = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0ec07e6847424034a0ed32c13d8f327e = L.popup({"maxWidth": "100%"});

        
            var html_de774f5566ef44fbb2737824d39fabeb = $(`<div id="html_de774f5566ef44fbb2737824d39fabeb" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8823934/">Learning attentive representations for environmental sound classification</a><br></div>`)[0];
            popup_0ec07e6847424034a0ed32c13d8f327e.setContent(html_de774f5566ef44fbb2737824d39fabeb);
        

        circle_marker_346e6923f5f94e098fca30106657bfee.bindPopup(popup_0ec07e6847424034a0ed32c13d8f327e)
        ;

        
    
    
            var circle_marker_f513dc0e3f9f445c877f6a3fd802db1b = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1d68ed311db7476884838c564b286cf0 = L.popup({"maxWidth": "100%"});

        
            var html_2f4200b7451642a8ab1077f651f97349 = $(`<div id="html_2f4200b7451642a8ab1077f651f97349" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-00764-5_2">Mixup-based acoustic scene classification using multi-channel convolutional neural network</a><br></div>`)[0];
            popup_1d68ed311db7476884838c564b286cf0.setContent(html_2f4200b7451642a8ab1077f651f97349);
        

        circle_marker_f513dc0e3f9f445c877f6a3fd802db1b.bindPopup(popup_1d68ed311db7476884838c564b286cf0)
        ;

        
    
    
            var circle_marker_672fb6cfd2d14f6ea4fa41dd28a228fb = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_81a2d9c550ac4bae8156fc056683ced1 = L.popup({"maxWidth": "100%"});

        
            var html_3ca6240b7ccc4465836d76ea52a98b3a = $(`<div id="html_3ca6240b7ccc4465836d76ea52a98b3a" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053023/">Multi-Branch Learning for Weakly-Labeled Sound Event Detection</a><br></div>`)[0];
            popup_81a2d9c550ac4bae8156fc056683ced1.setContent(html_3ca6240b7ccc4465836d76ea52a98b3a);
        

        circle_marker_672fb6cfd2d14f6ea4fa41dd28a228fb.bindPopup(popup_81a2d9c550ac4bae8156fc056683ced1)
        ;

        
    
    
            var circle_marker_eb6f238c75cd4d55883ea5dbf4900700 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_043c8f3110f741c69f3ccfd54ae4882c = L.popup({"maxWidth": "100%"});

        
            var html_c8777b81eb76457fbbbb87ecf5019730 = $(`<div id="html_c8777b81eb76457fbbbb87ecf5019730" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8576017/">Multi-LCNN: A Hybrid Neural Network Based on Integrated Time-Frequency Characteristics for Acoustic Scene Classification</a><br></div>`)[0];
            popup_043c8f3110f741c69f3ccfd54ae4882c.setContent(html_c8777b81eb76457fbbbb87ecf5019730);
        

        circle_marker_eb6f238c75cd4d55883ea5dbf4900700.bindPopup(popup_043c8f3110f741c69f3ccfd54ae4882c)
        ;

        
    
    
            var circle_marker_1c889b65c18d48dd9c987994cc6069c4 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_75a325875b9c4d1eafbab6cca1426d3e = L.popup({"maxWidth": "100%"});

        
            var html_a534c6a4b33546cdadf3b2e4384af13f = $(`<div id="html_a534c6a4b33546cdadf3b2e4384af13f" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8925176/">Multi-scale convolutional recurrent neural network with ensemble method for weakly labeled sound event detection</a><br></div>`)[0];
            popup_75a325875b9c4d1eafbab6cca1426d3e.setContent(html_a534c6a4b33546cdadf3b2e4384af13f);
        

        circle_marker_1c889b65c18d48dd9c987994cc6069c4.bindPopup(popup_75a325875b9c4d1eafbab6cca1426d3e)
        ;

        
    
    
            var circle_marker_897b7ff2f4b549b1a78067fe7a0e9b4d = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_047a1bbf23a74b568529f4e1c9969b3c = L.popup({"maxWidth": "100%"});

        
            var html_35d1a73b513c4e7995d79d67cd32de68 = $(`<div id="html_35d1a73b513c4e7995d79d67cd32de68" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0003682X19312897">Multi-scale semantic feature fusion and data augmentation for acoustic scene classification</a><br></div>`)[0];
            popup_047a1bbf23a74b568529f4e1c9969b3c.setContent(html_35d1a73b513c4e7995d79d67cd32de68);
        

        circle_marker_897b7ff2f4b549b1a78067fe7a0e9b4d.bindPopup(popup_047a1bbf23a74b568529f4e1c9969b3c)
        ;

        
    
    
            var circle_marker_c049af3131a94da0bdb6f3d24df034c2 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_60cbe5ddef674200acc5ef09e230f38c = L.popup({"maxWidth": "100%"});

        
            var html_3a4ec9105e5a4d8d978c969e988bf173 = $(`<div id="html_3a4ec9105e5a4d8d978c969e988bf173" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2002.00522">Novelty Detection via Non-Adversarial Generative Network</a><br></div>`)[0];
            popup_60cbe5ddef674200acc5ef09e230f38c.setContent(html_3a4ec9105e5a4d8d978c969e988bf173);
        

        circle_marker_c049af3131a94da0bdb6f3d24df034c2.bindPopup(popup_60cbe5ddef674200acc5ef09e230f38c)
        ;

        
    
    
            var circle_marker_3fd97343ed6b4711a86765b8f4111b9c = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_25bbd7a1568f42beb5cb3624119a5421 = L.popup({"maxWidth": "100%"});

        
            var html_c83145d420cb44a8a1f60d74c38e18d2 = $(`<div id="html_c83145d420cb44a8a1f60d74c38e18d2" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0888327020305616">Oversampling adversarial network for class-imbalanced fault diagnosis</a><br></div>`)[0];
            popup_25bbd7a1568f42beb5cb3624119a5421.setContent(html_c83145d420cb44a8a1f60d74c38e18d2);
        

        circle_marker_3fd97343ed6b4711a86765b8f4111b9c.bindPopup(popup_25bbd7a1568f42beb5cb3624119a5421)
        ;

        
    
    
            var circle_marker_474dad035d2046e9838340c526aca4e4 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_83152f7e83674606994638052ace1deb = L.popup({"maxWidth": "100%"});

        
            var html_d3b4c17beb8a4e2cb8ec7d1bc63b70a2 = $(`<div id="html_d3b4c17beb8a4e2cb8ec7d1bc63b70a2" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2005.11459">Power Pooling Operators and Confidence Learning for Semi-Supervised Sound Event Detection</a><br></div>`)[0];
            popup_83152f7e83674606994638052ace1deb.setContent(html_d3b4c17beb8a4e2cb8ec7d1bc63b70a2);
        

        circle_marker_474dad035d2046e9838340c526aca4e4.bindPopup(popup_83152f7e83674606994638052ace1deb)
        ;

        
    
    
            var circle_marker_7e0728edd2814613bb15b37d83603d7c = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_93297a0f18cf4e2581d4cb1dc6c32b55 = L.popup({"maxWidth": "100%"});

        
            var html_f7c56ac992b84700b6a525fd0f92a998 = $(`<div id="html_f7c56ac992b84700b6a525fd0f92a998" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2008.00110">Relational Teacher Student Learning with Neural Label Embedding for Device Adaptation in Acoustic Scene Classification</a><br></div>`)[0];
            popup_93297a0f18cf4e2581d4cb1dc6c32b55.setContent(html_f7c56ac992b84700b6a525fd0f92a998);
        

        circle_marker_7e0728edd2814613bb15b37d83603d7c.bindPopup(popup_93297a0f18cf4e2581d4cb1dc6c32b55)
        ;

        
    
    
            var circle_marker_6e6c52ab8ca2468eaf5f1523aeda43a3 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_20a66c1f69ac48d185282572b0887514 = L.popup({"maxWidth": "100%"});

        
            var html_c0d03a0a05cc45328003a1556bf55088 = $(`<div id="html_c0d03a0a05cc45328003a1556bf55088" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11859-018-1308-z">Shallow convolutional neural networks for acoustic scene classification</a><br></div>`)[0];
            popup_20a66c1f69ac48d185282572b0887514.setContent(html_c0d03a0a05cc45328003a1556bf55088);
        

        circle_marker_6e6c52ab8ca2468eaf5f1523aeda43a3.bindPopup(popup_20a66c1f69ac48d185282572b0887514)
        ;

        
    
    
            var circle_marker_4f9bd0acb13041a9bc4d875676181c30 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_bd6f9f8c44424c0eabd207b79bc4c26a = L.popup({"maxWidth": "100%"});

        
            var html_d0b314ce8390415db7e0f271ed996712 = $(`<div id="html_d0b314ce8390415db7e0f271ed996712" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9103031/">Sound Event Detection Using Multiple Optimized Kernels</a><br></div>`)[0];
            popup_bd6f9f8c44424c0eabd207b79bc4c26a.setContent(html_d0b314ce8390415db7e0f271ed996712);
        

        circle_marker_4f9bd0acb13041a9bc4d875676181c30.bindPopup(popup_bd6f9f8c44424c0eabd207b79bc4c26a)
        ;

        
    
    
            var circle_marker_cc2dc06b98d34f68946a7d9d8b283d38 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7a7b240c0bde44a590f2d58f1db16fab = L.popup({"maxWidth": "100%"});

        
            var html_2beca079396b4cbdb871479b6b3cbeee = $(`<div id="html_2beca079396b4cbdb871479b6b3cbeee" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9076321/">Specialized decision surface and disentangled feature for weakly-supervised polyphonic sound event detection</a><br></div>`)[0];
            popup_7a7b240c0bde44a590f2d58f1db16fab.setContent(html_2beca079396b4cbdb871479b6b3cbeee);
        

        circle_marker_cc2dc06b98d34f68946a7d9d8b283d38.bindPopup(popup_7a7b240c0bde44a590f2d58f1db16fab)
        ;

        
    
    
            var circle_marker_f1b9b4d220d447f289d7470eadbeefba = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b5ab29c913ec4210a3a3f8ec1e1d9814 = L.popup({"maxWidth": "100%"});

        
            var html_386e52692f4a4b828812e3a958e5d8ed = $(`<div id="html_386e52692f4a4b828812e3a958e5d8ed" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8905033/">Speech Emotion Recognition with Hybrid Neural Network</a><br></div>`)[0];
            popup_b5ab29c913ec4210a3a3f8ec1e1d9814.setContent(html_386e52692f4a4b828812e3a958e5d8ed);
        

        circle_marker_f1b9b4d220d447f289d7470eadbeefba.bindPopup(popup_b5ab29c913ec4210a3a3f8ec1e1d9814)
        ;

        
    
    
            var circle_marker_10aad479fed542299fe8ed0acc0582d3 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ceb4df6d80ed458dab44103c144a2fc4 = L.popup({"maxWidth": "100%"});

        
            var html_ef2b2f1f8bcd470f81115d226b5306b2 = $(`<div id="html_ef2b2f1f8bcd470f81115d226b5306b2" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0925231219316996">TNAM: A tag-aware neural attention model for Top-N recommendation</a><br></div>`)[0];
            popup_ceb4df6d80ed458dab44103c144a2fc4.setContent(html_ef2b2f1f8bcd470f81115d226b5306b2);
        

        circle_marker_10aad479fed542299fe8ed0acc0582d3.bindPopup(popup_ceb4df6d80ed458dab44103c144a2fc4)
        ;

        
    
    
            var circle_marker_5c92c40486994e719e582069a500cc69 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d86fa686f4914157b05f3b36731f8263 = L.popup({"maxWidth": "100%"});

        
            var html_7cbdc0ada8ea42ee943a9276851ce97d = $(`<div id="html_7cbdc0ada8ea42ee943a9276851ce97d" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8492416/">Transfer learning for wearable long-term social speech evaluations</a><br></div>`)[0];
            popup_d86fa686f4914157b05f3b36731f8263.setContent(html_7cbdc0ada8ea42ee943a9276851ce97d);
        

        circle_marker_5c92c40486994e719e582069a500cc69.bindPopup(popup_d86fa686f4914157b05f3b36731f8263)
        ;

        
    
    
            var circle_marker_8c6f191e2ba74d5897d2e4e878413039 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_001b4473ce1845e287d04e0eac365fec = L.popup({"maxWidth": "100%"});

        
            var html_9c35809de96c49619847bf1f5d0e2e97 = $(`<div id="html_9c35809de96c49619847bf1f5d0e2e97" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Liu_69.pdf">Ustc-nelslip system for dcase 2018 challenge task 4</a><br></div>`)[0];
            popup_001b4473ce1845e287d04e0eac365fec.setContent(html_9c35809de96c49619847bf1f5d0e2e97);
        

        circle_marker_8c6f191e2ba74d5897d2e4e878413039.bindPopup(popup_001b4473ce1845e287d04e0eac365fec)
        ;

        
    
    
            var circle_marker_c3e5aed434464ee18bd6e2041095d7a1 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d6479f2246d6459ebc8358eb6021dab4 = L.popup({"maxWidth": "100%"});

        
            var html_86ee6b116ae04a4ca39b91ef382ab7b2 = $(`<div id="html_86ee6b116ae04a4ca39b91ef382ab7b2" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2007.00222">A Transformer-based Audio Captioning Model with Keyword Estimation</a><br></div>`)[0];
            popup_d6479f2246d6459ebc8358eb6021dab4.setContent(html_86ee6b116ae04a4ca39b91ef382ab7b2);
        

        circle_marker_c3e5aed434464ee18bd6e2041095d7a1.bindPopup(popup_d6479f2246d6459ebc8358eb6021dab4)
        ;

        
    
    
            var circle_marker_e930002e2b0c432a9dec323ab7650798 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_27c87563c3d14abd97cd656f348c7b75 = L.popup({"maxWidth": "100%"});

        
            var html_3cb8b6337e0b44ce83c9dbc340ea3030 = $(`<div id="html_3cb8b6337e0b44ce83c9dbc340ea3030" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553423/">Anomalous sound event detection based on wavenet</a><br></div>`)[0];
            popup_27c87563c3d14abd97cd656f348c7b75.setContent(html_3cb8b6337e0b44ce83c9dbc340ea3030);
        

        circle_marker_e930002e2b0c432a9dec323ab7650798.bindPopup(popup_27c87563c3d14abd97cd656f348c7b75)
        ;

        
    
    
            var circle_marker_51a6b83f210546f9ae4d327699d6ca27 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_fc6239ac3c64418989d6ebebab6881fd = L.popup({"maxWidth": "100%"});

        
            var html_41456743ac3f4d93bb0865be3d2c35b4 = $(`<div id="html_41456743ac3f4d93bb0865be3d2c35b4" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2002.06021">Hodge and Podge: Hybrid Supervised Sound Event Detection with Multi-Hot MixMatch and Composition Consistence Training</a><br></div>`)[0];
            popup_fc6239ac3c64418989d6ebebab6881fd.setContent(html_41456743ac3f4d93bb0865be3d2c35b4);
        

        circle_marker_51a6b83f210546f9ae4d327699d6ca27.bindPopup(popup_fc6239ac3c64418989d6ebebab6881fd)
        ;

        
    
    
            var circle_marker_9b8b111cc75b4dc98b50e45519546024 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_548155c39b3f41708c0f70d43fbf8ae8 = L.popup({"maxWidth": "100%"});

        
            var html_64e1465b93914597b5ea74d981f5695a = $(`<div id="html_64e1465b93914597b5ea74d981f5695a" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/1907.07398">Hodgepodge: Sound event detection based on ensemble of semi-supervised learning methods</a><br></div>`)[0];
            popup_548155c39b3f41708c0f70d43fbf8ae8.setContent(html_64e1465b93914597b5ea74d981f5695a);
        

        circle_marker_9b8b111cc75b4dc98b50e45519546024.bindPopup(popup_548155c39b3f41708c0f70d43fbf8ae8)
        ;

        
    
    
            var circle_marker_899da119c5144a7688a9ce451bbc32b3 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_871a531828464eb1b4e9ed49314c492c = L.popup({"maxWidth": "100%"});

        
            var html_801a4322bb1544d38c3965559d569f8b = $(`<div id="html_801a4322bb1544d38c3965559d569f8b" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2004.02182">Imbalanced Data Learning by Minority Class Augmentation using Capsule Adversarial Networks</a><br></div>`)[0];
            popup_871a531828464eb1b4e9ed49314c492c.setContent(html_801a4322bb1544d38c3965559d569f8b);
        

        circle_marker_899da119c5144a7688a9ce451bbc32b3.bindPopup(popup_871a531828464eb1b4e9ed49314c492c)
        ;

        
    
    
            var circle_marker_d5240fc83a514be9b2762f2d371642f7 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8dd706a234e54b32837332ec306ce26f = L.popup({"maxWidth": "100%"});

        
            var html_994a3e3a94964bc898e94cb5663863d3 = $(`<div id="html_994a3e3a94964bc898e94cb5663863d3" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://archives.ismir.net/ismir2019/paper/000003.pdf">Intelligent User Interfaces for Music Discovery: The Past 20 Years and What's to Come.</a><br></div>`)[0];
            popup_8dd706a234e54b32837332ec306ce26f.setContent(html_994a3e3a94964bc898e94cb5663863d3);
        

        circle_marker_d5240fc83a514be9b2762f2d371642f7.bindPopup(popup_8dd706a234e54b32837332ec306ce26f)
        ;

        
    
    
            var circle_marker_40a0bbbbe08b4452a96e4acdff94956f = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_58f25c578ed04acfb78196a2656dcd4b = L.popup({"maxWidth": "100%"});

        
            var html_4de89bb826c64e07ae2b5cb9a06a20d5 = $(`<div id="html_4de89bb826c64e07ae2b5cb9a06a20d5" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682772/">Joint acoustic and class inference for weakly supervised sound event detection</a><br></div>`)[0];
            popup_58f25c578ed04acfb78196a2656dcd4b.setContent(html_4de89bb826c64e07ae2b5cb9a06a20d5);
        

        circle_marker_40a0bbbbe08b4452a96e4acdff94956f.bindPopup(popup_58f25c578ed04acfb78196a2656dcd4b)
        ;

        
    
    
            var circle_marker_fde419b6497f4520ae900128a178f513 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6941bf6f00e645feb39df5b473e9dac2 = L.popup({"maxWidth": "100%"});

        
            var html_5f9af7d6892944fdadfeac3ead16e474 = $(`<div id="html_5f9af7d6892944fdadfeac3ead16e474" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3241620">Multimedia recommender systems</a><br></div>`)[0];
            popup_6941bf6f00e645feb39df5b473e9dac2.setContent(html_5f9af7d6892944fdadfeac3ead16e474);
        

        circle_marker_fde419b6497f4520ae900128a178f513.bindPopup(popup_6941bf6f00e645feb39df5b473e9dac2)
        ;

        
    
    
            var circle_marker_e6cb99e821194c7d8bef4eb5871b50a4 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c0a34d2985fc40c598581709d02c18f9 = L.popup({"maxWidth": "100%"});

        
            var html_720c8aceaee047da968e99f74a7124f7 = $(`<div id="html_720c8aceaee047da968e99f74a7124f7" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2004.04371">Music Artist Classification with WaveNet Classifier for Raw Waveform Audio Data</a><br></div>`)[0];
            popup_c0a34d2985fc40c598581709d02c18f9.setContent(html_720c8aceaee047da968e99f74a7124f7);
        

        circle_marker_e6cb99e821194c7d8bef4eb5871b50a4.bindPopup(popup_c0a34d2985fc40c598581709d02c18f9)
        ;

        
    
    
            var circle_marker_7172973bfdc54c19a078a4ff244329a1 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_94a6cf78e5c84cafbac1ca8e10dd1800 = L.popup({"maxWidth": "100%"});

        
            var html_350eb43c9bf34972aca902c9d396a10c = $(`<div id="html_350eb43c9bf34972aca902c9d396a10c" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0888327020305616">Oversampling adversarial network for class-imbalanced fault diagnosis</a><br></div>`)[0];
            popup_94a6cf78e5c84cafbac1ca8e10dd1800.setContent(html_350eb43c9bf34972aca902c9d396a10c);
        

        circle_marker_7172973bfdc54c19a078a4ff244329a1.bindPopup(popup_94a6cf78e5c84cafbac1ca8e10dd1800)
        ;

        
    
    
            var circle_marker_16eb2104fe2d4e238babb95c119af8a5 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a735533197ed4a668426cdfadd29f780 = L.popup({"maxWidth": "100%"});

        
            var html_d81808ad44dc4e00885ffa482b44656d = $(`<div id="html_d81808ad44dc4e00885ffa482b44656d" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683068/">Scene-dependent Anomalous Acoustic-event Detection Based on Conditional Wavenet and I-vector</a><br></div>`)[0];
            popup_a735533197ed4a668426cdfadd29f780.setContent(html_d81808ad44dc4e00885ffa482b44656d);
        

        circle_marker_16eb2104fe2d4e238babb95c119af8a5.bindPopup(popup_a735533197ed4a668426cdfadd29f780)
        ;

        
    
    
            var circle_marker_a3dd431539e24ba4957f453be4d6cc7e = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0c5e63809cf1489787f3d3d2fca88aef = L.popup({"maxWidth": "100%"});

        
            var html_399e044407d24d2489149a0ad355423a = $(`<div id="html_399e044407d24d2489149a0ad355423a" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2006.15253">Sound Event Detection Using Duration Robust Loss Function</a><br></div>`)[0];
            popup_0c5e63809cf1489787f3d3d2fca88aef.setContent(html_399e044407d24d2489149a0ad355423a);
        

        circle_marker_a3dd431539e24ba4957f453be4d6cc7e.bindPopup(popup_0c5e63809cf1489787f3d3d2fca88aef)
        ;

        
    
    
            var circle_marker_0efbfc780d20426684e0ed251d0858df = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_55cd2e4d4c9742668c8e294f59ad7ebc = L.popup({"maxWidth": "100%"});

        
            var html_fb9b3c32f5c945f686f284d1769943bd = $(`<div id="html_fb9b3c32f5c945f686f284d1769943bd" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8905033/">Speech Emotion Recognition with Hybrid Neural Network</a><br></div>`)[0];
            popup_55cd2e4d4c9742668c8e294f59ad7ebc.setContent(html_fb9b3c32f5c945f686f284d1769943bd);
        

        circle_marker_0efbfc780d20426684e0ed251d0858df.bindPopup(popup_55cd2e4d4c9742668c8e294f59ad7ebc)
        ;

        
    
    
            var circle_marker_12f4488d933b4919aaba064ecee8a5be = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6d731320e3ef4d29b08f9362466852e3 = L.popup({"maxWidth": "100%"});

        
            var html_8d99dc70ea9f4343a41a57930e411a62 = $(`<div id="html_8d99dc70ea9f4343a41a57930e411a62" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8501554/">Unsupervised detection of anomalous sound based on deep learning and the Neyman–Pearson lemma</a><br></div>`)[0];
            popup_6d731320e3ef4d29b08f9362466852e3.setContent(html_8d99dc70ea9f4343a41a57930e411a62);
        

        circle_marker_12f4488d933b4919aaba064ecee8a5be.bindPopup(popup_6d731320e3ef4d29b08f9362466852e3)
        ;

        
    
    
            var circle_marker_d6f1d544c2954d3cb5376ff8b92da034 = L.circleMarker(
                [61.0666922, -107.9917071],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_fa028dc4a74c46f4a4ee24150c3f01f0 = L.popup({"maxWidth": "100%"});

        
            var html_140a78f048de456688870e5469f29126 = $(`<div id="html_140a78f048de456688870e5469f29126" style="width: 100.0%; height: 100.0%;">Country : Canada<br>                         Paper : <a href="https://arxiv.org/abs/1903.07714">A RAD approach to deep mixture models</a><br></div>`)[0];
            popup_fa028dc4a74c46f4a4ee24150c3f01f0.setContent(html_140a78f048de456688870e5469f29126);
        

        circle_marker_d6f1d544c2954d3cb5376ff8b92da034.bindPopup(popup_fa028dc4a74c46f4a4ee24150c3f01f0)
        ;

        
    
    
            var circle_marker_43075012bc31475a92cd2dc156e9bb7b = L.circleMarker(
                [61.0666922, -107.9917071],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1fc85fa49f4d4dd2a791f1dd3a4c6665 = L.popup({"maxWidth": "100%"});

        
            var html_47f43c2286a24c9f9060573578cec45a = $(`<div id="html_47f43c2286a24c9f9060573578cec45a" style="width: 100.0%; height: 100.0%;">Country : Canada<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933046/">Combining temporal features by local binary pattern for acoustic scene classification</a><br></div>`)[0];
            popup_1fc85fa49f4d4dd2a791f1dd3a4c6665.setContent(html_47f43c2286a24c9f9060573578cec45a);
        

        circle_marker_43075012bc31475a92cd2dc156e9bb7b.bindPopup(popup_1fc85fa49f4d4dd2a791f1dd3a4c6665)
        ;

        
    
    
            var circle_marker_c957a354236348f791213cd2a7a9e8c9 = L.circleMarker(
                [61.0666922, -107.9917071],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7e32435199a0469e98423a939b2c1855 = L.popup({"maxWidth": "100%"});

        
            var html_1204d5c2427247baaa817c461bfa3211 = $(`<div id="html_1204d5c2427247baaa817c461bfa3211" style="width: 100.0%; height: 100.0%;">Country : Canada<br>                         Paper : <a href="https://arxiv.org/abs/1808.05777">Unsupervised adversarial domain adaptation for acoustic scene classification</a><br></div>`)[0];
            popup_7e32435199a0469e98423a939b2c1855.setContent(html_1204d5c2427247baaa817c461bfa3211);
        

        circle_marker_c957a354236348f791213cd2a7a9e8c9.bindPopup(popup_7e32435199a0469e98423a939b2c1855)
        ;

        
    
    
            var circle_marker_ac55ea957bb046cf87d651a72804434f = L.circleMarker(
                [61.0666922, -107.9917071],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b4e86c312ad64a2881b1bab788700399 = L.popup({"maxWidth": "100%"});

        
            var html_447701716d424254b2f71482c83596c6 = $(`<div id="html_447701716d424254b2f71482c83596c6" style="width: 100.0%; height: 100.0%;">Country : Canada<br>                         Paper : <a href="https://arxiv.org/abs/2008.07702">VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations</a><br></div>`)[0];
            popup_b4e86c312ad64a2881b1bab788700399.setContent(html_447701716d424254b2f71482c83596c6);
        

        circle_marker_ac55ea957bb046cf87d651a72804434f.bindPopup(popup_b4e86c312ad64a2881b1bab788700399)
        ;

        
    
    
            var circle_marker_b9b66b3de33a4182a1b599393b26339a = L.circleMarker(
                [45.9852129, 24.6859225],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e8622b1a1f9c47c2abc47d31be273423 = L.popup({"maxWidth": "100%"});

        
            var html_f9b1da8c552a43e996844190485d6a9c = $(`<div id="html_f9b1da8c552a43e996844190485d6a9c" style="width: 100.0%; height: 100.0%;">Country : Romania<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3240407">Audio-visual encoding of multimedia content for enhancing movie recommendations</a><br></div>`)[0];
            popup_e8622b1a1f9c47c2abc47d31be273423.setContent(html_f9b1da8c552a43e996844190485d6a9c);
        

        circle_marker_b9b66b3de33a4182a1b599393b26339a.bindPopup(popup_e8622b1a1f9c47c2abc47d31be273423)
        ;

        
    
    
            var circle_marker_dc598934a1444beea9bd5efc126bb72f = L.circleMarker(
                [45.9852129, 24.6859225],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_cca6d07022e94a08853f8622e276377b = L.popup({"maxWidth": "100%"});

        
            var html_381d550131fc4601bb982ad5a094a7da = $(`<div id="html_381d550131fc4601bb982ad5a094a7da" style="width: 100.0%; height: 100.0%;">Country : Romania<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3204949.3208141">MMTF-14K: a multifaceted movie trailer feature dataset for recommendation and retrieval</a><br></div>`)[0];
            popup_cca6d07022e94a08853f8622e276377b.setContent(html_381d550131fc4601bb982ad5a094a7da);
        

        circle_marker_dc598934a1444beea9bd5efc126bb72f.bindPopup(popup_cca6d07022e94a08853f8622e276377b)
        ;

        
    
    
            var circle_marker_19ab4ddbfffb4e889ed32ab971e586ec = L.circleMarker(
                [45.9852129, 24.6859225],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6468797dfb074c49b6518fdd39afcb0d = L.popup({"maxWidth": "100%"});

        
            var html_52dbbc40ec1d4ca0866c8b81ab582c3f = $(`<div id="html_52dbbc40ec1d4ca0866c8b81ab582c3f" style="width: 100.0%; height: 100.0%;">Country : Romania<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09221-y">Movie genome: alleviating new item cold start in movie recommendation</a><br></div>`)[0];
            popup_6468797dfb074c49b6518fdd39afcb0d.setContent(html_52dbbc40ec1d4ca0866c8b81ab582c3f);
        

        circle_marker_19ab4ddbfffb4e889ed32ab971e586ec.bindPopup(popup_6468797dfb074c49b6518fdd39afcb0d)
        ;

        
    
    
            var circle_marker_4393172a0a9142ff921e03c2d3504688 = L.circleMarker(
                [45.9852129, 24.6859225],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_4bfed6d9059745568f81ea6206257a90 = L.popup({"maxWidth": "100%"});

        
            var html_db8f6f4d9b0a4723a509d36d1ef6bca0 = $(`<div id="html_db8f6f4d9b0a4723a509d36d1ef6bca0" style="width: 100.0%; height: 100.0%;">Country : Romania<br>                         Paper : <a href="https://www.jku.at/fileadmin/gruppen/173/Research/deldjoo_mediaeval_2018.pdf">The MediaEval 2018 Movie Recommendation Task: Recommending Movies Using Content.</a><br></div>`)[0];
            popup_4bfed6d9059745568f81ea6206257a90.setContent(html_db8f6f4d9b0a4723a509d36d1ef6bca0);
        

        circle_marker_4393172a0a9142ff921e03c2d3504688.bindPopup(popup_4bfed6d9059745568f81ea6206257a90)
        ;

        
    
    
            var circle_marker_6f2637e7342943f280bc8570f946e9ae = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3383ec91b45a40b39ccc09eeb581a5ca = L.popup({"maxWidth": "100%"});

        
            var html_ebf0cf6512a0407d84cbf994d5760d53 = $(`<div id="html_ebf0cf6512a0407d84cbf994d5760d53" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9140774/">A Re-trained Model Based On Multi-kernel Convolutional Neural Network for Acoustic Scene Classification</a><br></div>`)[0];
            popup_3383ec91b45a40b39ccc09eeb581a5ca.setContent(html_ebf0cf6512a0407d84cbf994d5760d53);
        

        circle_marker_6f2637e7342943f280bc8570f946e9ae.bindPopup(popup_3383ec91b45a40b39ccc09eeb581a5ca)
        ;

        
    
    
            var circle_marker_7e231a1e54e9443791e47e92e43bdb28 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_bfb46e4b05a946cf9fddc6ce1e2b1597 = L.popup({"maxWidth": "100%"});

        
            var html_b5a6ca23fdb94a7c952b29cc0aa1c237 = $(`<div id="html_b5a6ca23fdb94a7c952b29cc0aa1c237" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8999242/">Acoustic Scene Classification Using Deep Mixtures of Pre-trained Convolutional Neural Networks</a><br></div>`)[0];
            popup_bfb46e4b05a946cf9fddc6ce1e2b1597.setContent(html_b5a6ca23fdb94a7c952b29cc0aa1c237);
        

        circle_marker_7e231a1e54e9443791e47e92e43bdb28.bindPopup(popup_bfb46e4b05a946cf9fddc6ce1e2b1597)
        ;

        
    
    
            var circle_marker_0e665e74a8b248508ec6d39830615514 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e69c40f91c1041259daf957b09391a11 = L.popup({"maxWidth": "100%"});

        
            var html_796e171985844cc99d563a9fb6199c14 = $(`<div id="html_796e171985844cc99d563a9fb6199c14" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053582/">Acoustic Scene Classification for Mismatched Recording Devices Using Heated-Up Softmax and Spectrum Correction</a><br></div>`)[0];
            popup_e69c40f91c1041259daf957b09391a11.setContent(html_796e171985844cc99d563a9fb6199c14);
        

        circle_marker_0e665e74a8b248508ec6d39830615514.bindPopup(popup_e69c40f91c1041259daf957b09391a11)
        ;

        
    
    
            var circle_marker_9aba55fb11b74046b48c87b0ccf8c9e2 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_3aeec8e73f4d45af9488a83f2561538e = L.popup({"maxWidth": "100%"});

        
            var html_0e195f622a9c4235ae5c49fc0f2edb73 = $(`<div id="html_0e195f622a9c4235ae5c49fc0f2edb73" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/3002.pdf">Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation.</a><br></div>`)[0];
            popup_3aeec8e73f4d45af9488a83f2561538e.setContent(html_0e195f622a9c4235ae5c49fc0f2edb73);
        

        circle_marker_9aba55fb11b74046b48c87b0ccf8c9e2.bindPopup(popup_3aeec8e73f4d45af9488a83f2561538e)
        ;

        
    
    
            var circle_marker_7613cd7a4c7c45ebb36b9d849277bb6c = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_dfe636a47e6e4b488ec2b7a8ecd5da84 = L.popup({"maxWidth": "100%"});

        
            var html_76ec3d7ecaba492b9b14621f65ffeaef = $(`<div id="html_76ec3d7ecaba492b9b14621f65ffeaef" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://trepo.tuni.fi/bitstream/handle/10024/116599/DCASE_2018_proceedings.pdf?sequence=1&isAllowed=y#page=35">Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters</a><br></div>`)[0];
            popup_dfe636a47e6e4b488ec2b7a8ecd5da84.setContent(html_76ec3d7ecaba492b9b14621f65ffeaef);
        

        circle_marker_7613cd7a4c7c45ebb36b9d849277bb6c.bindPopup(popup_dfe636a47e6e4b488ec2b7a8ecd5da84)
        ;

        
    
    
            var circle_marker_22dbdc66b49945008c78ce37d6a2a1e2 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e66a69858cea4c19a1320b5c62f2bf32 = L.popup({"maxWidth": "100%"});

        
            var html_a7eace6fac78489caa4e9c39dda8b94a = $(`<div id="html_a7eace6fac78489caa4e9c39dda8b94a" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8784816/">Acoustic scene classification with mismatched recording devices using mixture of experts layer</a><br></div>`)[0];
            popup_e66a69858cea4c19a1320b5c62f2bf32.setContent(html_a7eace6fac78489caa4e9c39dda8b94a);
        

        circle_marker_22dbdc66b49945008c78ce37d6a2a1e2.bindPopup(popup_e66a69858cea4c19a1320b5c62f2bf32)
        ;

        
    
    
            var circle_marker_f7fe7b62760e4968a9755d270ab3477b = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_5c109df0993c44a2a2db59bad667d72a = L.popup({"maxWidth": "100%"});

        
            var html_abd58af38db74c90a79aa887530500a4 = $(`<div id="html_abd58af38db74c90a79aa887530500a4" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_HCM_6.pdf">CDNN-CRNN JOINED MODEL FOR ACOUSTIC SCENE CLASSIFICATION</a><br></div>`)[0];
            popup_5c109df0993c44a2a2db59bad667d72a.setContent(html_abd58af38db74c90a79aa887530500a4);
        

        circle_marker_f7fe7b62760e4968a9755d270ab3477b.bindPopup(popup_5c109df0993c44a2a2db59bad667d72a)
        ;

        
    
    
            var circle_marker_15c65b4892a848f4878157c9cacd20e4 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_03ca4c48bed64ad5ab431ef57c26fb58 = L.popup({"maxWidth": "100%"});

        
            var html_e9febc90e52840758719f45467cba233 = $(`<div id="html_e9febc90e52840758719f45467cba233" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://www.researchgate.net/profile/Nghia_Duong_Trung/publication/339593299_Genres_and_ActorsActresses_as_Interpolated_Tags_for_Improving_Movie_Recommender_Systems/links/5e68a2ad299bf1744f72db23/Genres-and-Actors-Actresses-as-Interpolated-Tags-for-Improving-Movie-Recommender-Systems.pdf">Genres and Actors/Actresses as Interpolated Tags for Improving Movie Recommender Systems</a><br></div>`)[0];
            popup_03ca4c48bed64ad5ab431ef57c26fb58.setContent(html_e9febc90e52840758719f45467cba233);
        

        circle_marker_15c65b4892a848f4878157c9cacd20e4.bindPopup(popup_03ca4c48bed64ad5ab431ef57c26fb58)
        ;

        
    
    
            var circle_marker_914e0a6ac84e4b77803767527cd430cc = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6e0f44e7b0f9465c94fe74707fba7394 = L.popup({"maxWidth": "100%"});

        
            var html_9f2206f00c63441ca82198caca17a7a1 = $(`<div id="html_9f2206f00c63441ca82198caca17a7a1" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://www.researchgate.net/profile/Nghia_Duong_Trung/publication/339778055_Movie_Recommender_Systems_Made_Through_Tag_Interpolation/links/5e678f4492851c7ce0579a68/Movie-Recommender-Systems-Made-Through-Tag-Interpolation.pdf">Movie Recommender Systems Made Through Tag Interpolation</a><br></div>`)[0];
            popup_6e0f44e7b0f9465c94fe74707fba7394.setContent(html_9f2206f00c63441ca82198caca17a7a1);
        

        circle_marker_914e0a6ac84e4b77803767527cd430cc.bindPopup(popup_6e0f44e7b0f9465c94fe74707fba7394)
        ;

        
    
    
            var circle_marker_0262e72b7a314a9486188d624e12744d = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e3830d0f355641828fad59ec81fed9e6 = L.popup({"maxWidth": "100%"});

        
            var html_54893908fe1342a483c1eca401f9bfcf = $(`<div id="html_54893908fe1342a483c1eca401f9bfcf" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://arxiv.org/abs/2002.04502">Robust Acoustic Scene Classification using a Multi-Spectrogram Encoder-Decoder Framework</a><br></div>`)[0];
            popup_e3830d0f355641828fad59ec81fed9e6.setContent(html_54893908fe1342a483c1eca401f9bfcf);
        

        circle_marker_0262e72b7a314a9486188d624e12744d.bindPopup(popup_e3830d0f355641828fad59ec81fed9e6)
        ;

        
    
    
            var circle_marker_d59566a0be0d4cc89e1634c1480426a8 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a7411f0d3cfd4fda9ff9922c7b87db3e = L.popup({"maxWidth": "100%"});

        
            var html_32bed61182dd4bd9a78219d758aa1ff0 = $(`<div id="html_32bed61182dd4bd9a78219d758aa1ff0" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9049622/">A Hybrid Approach for Mobile Phone Clustering with Speech Recordings</a><br></div>`)[0];
            popup_a7411f0d3cfd4fda9ff9922c7b87db3e.setContent(html_32bed61182dd4bd9a78219d758aa1ff0);
        

        circle_marker_d59566a0be0d4cc89e1634c1480426a8.bindPopup(popup_a7411f0d3cfd4fda9ff9922c7b87db3e)
        ;

        
    
    
            var circle_marker_4430947846544849895407769fbfd73b = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_97f6ad61ca284d0bb502b1ba3ded2060 = L.popup({"maxWidth": "100%"});

        
            var html_f5d20968315249b5b0d560ac8e3b2d6f = $(`<div id="html_f5d20968315249b5b0d560ac8e3b2d6f" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://www.researchgate.net/profile/Lam_Pham4/publication/335829391_A_Robust_Framework_for_Acoustic_Scene_Classification/links/5dc03975a6fdcc2128011ee7/A-Robust-Framework-for-Acoustic-Scene-Classification.pdf">A Robust Framework for Acoustic Scene Classification.</a><br></div>`)[0];
            popup_97f6ad61ca284d0bb502b1ba3ded2060.setContent(html_f5d20968315249b5b0d560ac8e3b2d6f);
        

        circle_marker_4430947846544849895407769fbfd73b.bindPopup(popup_97f6ad61ca284d0bb502b1ba3ded2060)
        ;

        
    
    
            var circle_marker_414b5c54af9241909bad7dab19a3c5b8 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e39e297a65b444e096bf0fcd513f1ac1 = L.popup({"maxWidth": "100%"});

        
            var html_8370538314d348a88550e9446b1826d7 = $(`<div id="html_8370538314d348a88550e9446b1826d7" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://core.ac.uk/download/pdf/323227034.pdf#page=156">ACOUSTIC SCENE CLASSIFICATION FROM BINAURAL SIGNALS USING CONVOLUTIONAL NEURAL NETWORKS</a><br></div>`)[0];
            popup_e39e297a65b444e096bf0fcd513f1ac1.setContent(html_8370538314d348a88550e9446b1826d7);
        

        circle_marker_414b5c54af9241909bad7dab19a3c5b8.bindPopup(popup_e39e297a65b444e096bf0fcd513f1ac1)
        ;

        
    
    
            var circle_marker_c917bf7d0adc42439b6f2d2f2b1d8385 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_694120e766a242ac9cdd8f388fa645c7 = L.popup({"maxWidth": "100%"});

        
            var html_4e6f24dab909438bb67b6b5d4c55abb2 = $(`<div id="html_4e6f24dab909438bb67b6b5d4c55abb2" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Zhao_161.pdf">ADSC submission for DCASE 2017: Acoustic scene classification using deep residual convolutional neural networks</a><br></div>`)[0];
            popup_694120e766a242ac9cdd8f388fa645c7.setContent(html_4e6f24dab909438bb67b6b5d4c55abb2);
        

        circle_marker_c917bf7d0adc42439b6f2d2f2b1d8385.bindPopup(popup_694120e766a242ac9cdd8f388fa645c7)
        ;

        
    
    
            var circle_marker_0ccbf262f99e49b981461db5e6828a5f = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ab797839f6784c608239dcd971f07569 = L.popup({"maxWidth": "100%"});

        
            var html_1cbe4e52a0f047fd9ed92820a9da24c1 = $(`<div id="html_1cbe4e52a0f047fd9ed92820a9da24c1" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60748">Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks</a><br></div>`)[0];
            popup_ab797839f6784c608239dcd971f07569.setContent(html_1cbe4e52a0f047fd9ed92820a9da24c1);
        

        circle_marker_0ccbf262f99e49b981461db5e6828a5f.bindPopup(popup_ab797839f6784c608239dcd971f07569)
        ;

        
    
    
            var circle_marker_c23b5ffb8504442088a792f467b8b807 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b7ddd89cfd2240eca2f6efc57d08a31c = L.popup({"maxWidth": "100%"});

        
            var html_3bdf8687a12c456ba16feb7d561235eb = $(`<div id="html_3bdf8687a12c456ba16feb7d561235eb" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240631">Learning and fusing multimodal deep features for acoustic scene categorization</a><br></div>`)[0];
            popup_b7ddd89cfd2240eca2f6efc57d08a31c.setContent(html_3bdf8687a12c456ba16feb7d561235eb);
        

        circle_marker_c23b5ffb8504442088a792f467b8b807.bindPopup(popup_b7ddd89cfd2240eca2f6efc57d08a31c)
        ;

        
    
    
            var circle_marker_406c715506fb46a0963be878ff7f1cac = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8050323bbd9348fa8a7da32a4140e505 = L.popup({"maxWidth": "100%"});

        
            var html_80a74ee238294000be854386957ffd0c = $(`<div id="html_80a74ee238294000be854386957ffd0c" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://pdfs.semanticscholar.org/5e34/9da3165367f4fb2e8ad3c07f2ed95411457a.pdf">On the Importance of Audio-Source Separation for Singer Identification in Polyphonic Music.</a><br></div>`)[0];
            popup_8050323bbd9348fa8a7da32a4140e505.setContent(html_80a74ee238294000be854386957ffd0c);
        

        circle_marker_406c715506fb46a0963be878ff7f1cac.bindPopup(popup_8050323bbd9348fa8a7da32a4140e505)
        ;

        
    
    
            var circle_marker_0a1ee80581e741ecbcaa0c80d2d3f677 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_33e7fc80d2c54ff78b97494772dfd19e = L.popup({"maxWidth": "100%"});

        
            var html_e3831cca18ed43f8b478c38d75cab8ad = $(`<div id="html_e3831cca18ed43f8b478c38d75cab8ad" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553052/">A layer-wise score level ensemble framework for acoustic scene classification</a><br></div>`)[0];
            popup_33e7fc80d2c54ff78b97494772dfd19e.setContent(html_e3831cca18ed43f8b478c38d75cab8ad);
        

        circle_marker_0a1ee80581e741ecbcaa0c80d2d3f677.bindPopup(popup_33e7fc80d2c54ff78b97494772dfd19e)
        ;

        
    
    
            var circle_marker_b2164e6c23ae487ba0d2c1f495d29ac4 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_41fb60b657514caa9fb51cc6adb1ae61 = L.popup({"maxWidth": "100%"});

        
            var html_b1aaf4e4e6f4440dbef411dd6d19b7cc = $(`<div id="html_b1aaf4e4e6f4440dbef411dd6d19b7cc" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://arxiv.org/abs/1811.00936">Acoustic features fusion using attentive multi-channel deep architecture</a><br></div>`)[0];
            popup_41fb60b657514caa9fb51cc6adb1ae61.setContent(html_b1aaf4e4e6f4440dbef411dd6d19b7cc);
        

        circle_marker_b2164e6c23ae487ba0d2c1f495d29ac4.bindPopup(popup_41fb60b657514caa9fb51cc6adb1ae61)
        ;

        
    
    
            var circle_marker_033bfec899dc408da9457bafabf7de6c = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_9ab7ad09783c4a89a4e155e3cfe3b50d = L.popup({"maxWidth": "100%"});

        
            var html_530335144c4b47239a727b4786248f91 = $(`<div id="html_530335144c4b47239a727b4786248f91" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="">An Empirical Study of Iterative Knowledge Distillation for Neural Network Compression</a><br></div>`)[0];
            popup_9ab7ad09783c4a89a4e155e3cfe3b50d.setContent(html_530335144c4b47239a727b4786248f91);
        

        circle_marker_033bfec899dc408da9457bafabf7de6c.bindPopup(popup_9ab7ad09783c4a89a4e155e3cfe3b50d)
        ;

        
    
    
            var circle_marker_b0634b0fad04433abfe16b72dea0d813 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a2537fc1d6e24c95af209541aca3f9e2 = L.popup({"maxWidth": "100%"});

        
            var html_5d32fd0a6b4749a5b4c069db4fddc4eb = $(`<div id="html_5d32fd0a6b4749a5b4c069db4fddc4eb" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s11042-019-08279-5.pdf">Analysis and classification of acoustic scenes with wavelet transform-based mel-scaled features</a><br></div>`)[0];
            popup_a2537fc1d6e24c95af209541aca3f9e2.setContent(html_5d32fd0a6b4749a5b4c069db4fddc4eb);
        

        circle_marker_b0634b0fad04433abfe16b72dea0d813.bindPopup(popup_a2537fc1d6e24c95af209541aca3f9e2)
        ;

        
    
    
            var circle_marker_85e8fde5f19144878643ebb6754147fd = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_52187cf9ff4641db826a689c001552a8 = L.popup({"maxWidth": "100%"});

        
            var html_72fea4c88465476fae43cce29c18769b = $(`<div id="html_72fea4c88465476fae43cce29c18769b" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S1051200418300046">Classification of audio scenes with novel features in a fused system framework</a><br></div>`)[0];
            popup_52187cf9ff4641db826a689c001552a8.setContent(html_72fea4c88465476fae43cce29c18769b);
        

        circle_marker_85e8fde5f19144878643ebb6754147fd.bindPopup(popup_52187cf9ff4641db826a689c001552a8)
        ;

        
    
    
            var circle_marker_d4ea8e1166d84bfdaa8ef657b4eb0cbc = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2818fa5530564454b1001817cebcbef3 = L.popup({"maxWidth": "100%"});

        
            var html_93c7febd0dd44c70bc73159d97247619 = $(`<div id="html_93c7febd0dd44c70bc73159d97247619" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S095741741830229X">Classification of vocal and non-vocal segments in audio clips using genetic algorithm based feature selection (GAFS)</a><br></div>`)[0];
            popup_2818fa5530564454b1001817cebcbef3.setContent(html_93c7febd0dd44c70bc73159d97247619);
        

        circle_marker_d4ea8e1166d84bfdaa8ef657b4eb0cbc.bindPopup(popup_2818fa5530564454b1001817cebcbef3)
        ;

        
    
    
            var circle_marker_390b358e5eea468f965259be30c6561f = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d2781c9f64e944169ab3b717a9e30b87 = L.popup({"maxWidth": "100%"});

        
            var html_610ff30173044af6898ab99d4bba0a4c = $(`<div id="html_610ff30173044af6898ab99d4bba0a4c" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s12065-019-00283-w.pdf">Data augmentation for cancer classification in oncogenomics: an improved KNN based approach</a><br></div>`)[0];
            popup_d2781c9f64e944169ab3b717a9e30b87.setContent(html_610ff30173044af6898ab99d4bba0a4c);
        

        circle_marker_390b358e5eea468f965259be30c6561f.bindPopup(popup_d2781c9f64e944169ab3b717a9e30b87)
        ;

        
    
    
            var circle_marker_9c4fe27e87384eed8ba405e04f30b208 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7cef655ed5ea469ab6cb092d685677c4 = L.popup({"maxWidth": "100%"});

        
            var html_c83c5b170c0a41a9b5fa06c2a14d84c6 = $(`<div id="html_c83c5b170c0a41a9b5fa06c2a14d84c6" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s00500-019-04602-2.pdf">Data augmentation using MG-GAN for improved cancer classification on gene expression data</a><br></div>`)[0];
            popup_7cef655ed5ea469ab6cb092d685677c4.setContent(html_c83c5b170c0a41a9b5fa06c2a14d84c6);
        

        circle_marker_9c4fe27e87384eed8ba405e04f30b208.bindPopup(popup_7cef655ed5ea469ab6cb092d685677c4)
        ;

        
    
    
            var circle_marker_645511b606ea400dacf61a9c2c718425 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_003b0d0885604a39ace1721d104a815e = L.popup({"maxWidth": "100%"});

        
            var html_92542461d2fc4a75b9a091bd40f0bfb1 = $(`<div id="html_92542461d2fc4a75b9a091bd40f0bfb1" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60765">Deep Multi-view Features from Raw Audio for Acoustic Scene Classification</a><br></div>`)[0];
            popup_003b0d0885604a39ace1721d104a815e.setContent(html_92542461d2fc4a75b9a091bd40f0bfb1);
        

        circle_marker_645511b606ea400dacf61a9c2c718425.bindPopup(popup_003b0d0885604a39ace1721d104a815e)
        ;

        
    
    
            var circle_marker_465cdb1ad76e420bb2e66ed618efcd29 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_548def03c7df4490a5e1687ab7b273af = L.popup({"maxWidth": "100%"});

        
            var html_03ee468809f14be7bb672a04fafc94be = $(`<div id="html_03ee468809f14be7bb672a04fafc94be" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://arxiv.org/abs/1708.05826">Ensemble Of Deep Neural Networks For Acoustic Scene Classification</a><br></div>`)[0];
            popup_548def03c7df4490a5e1687ab7b273af.setContent(html_03ee468809f14be7bb672a04fafc94be);
        

        circle_marker_465cdb1ad76e420bb2e66ed618efcd29.bindPopup(popup_548def03c7df4490a5e1687ab7b273af)
        ;

        
    
    
            var circle_marker_4feee973905744b986d2bf6144fabec0 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_140cb7e0c46444ecaac3ff85748add0c = L.popup({"maxWidth": "100%"});

        
            var html_79ab1bda459f41faa25ee0fcfc6e8f89 = $(`<div id="html_79ab1bda459f41faa25ee0fcfc6e8f89" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3322240">Environmental audio scene and sound event recognition for autonomous surveillance: A survey and comparative studies</a><br></div>`)[0];
            popup_140cb7e0c46444ecaac3ff85748add0c.setContent(html_79ab1bda459f41faa25ee0fcfc6e8f89);
        

        circle_marker_4feee973905744b986d2bf6144fabec0.bindPopup(popup_140cb7e0c46444ecaac3ff85748add0c)
        ;

        
    
    
            var circle_marker_5b70e08ddbd0451fa2e602af23c962f2 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_73d8c7eff9754ed5b8bf5c365912e364 = L.popup({"maxWidth": "100%"});

        
            var html_f1a94cf3747a41448225b5c38c921a82 = $(`<div id="html_f1a94cf3747a41448225b5c38c921a82" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8752027/">Generative Model Driven Representation Learning in a Hybrid Framework for Environmental Audio Scene and Sound Event Recognition</a><br></div>`)[0];
            popup_73d8c7eff9754ed5b8bf5c365912e364.setContent(html_f1a94cf3747a41448225b5c38c921a82);
        

        circle_marker_5b70e08ddbd0451fa2e602af23c962f2.bindPopup(popup_73d8c7eff9754ed5b8bf5c365912e364)
        ;

        
    
    
            var circle_marker_7479723bcdce4ed2a7df39c7f7a64e05 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_5f3d304c679241ebafd953121dc16354 = L.popup({"maxWidth": "100%"});

        
            var html_5dad7d0ceb894d848547a6045ac8af0d = $(`<div id="html_5dad7d0ceb894d848547a6045ac8af0d" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8721416/">Input Fusion of MFCC and SCMC Features for Acoustic Scene Classification using DNN</a><br></div>`)[0];
            popup_5f3d304c679241ebafd953121dc16354.setContent(html_5dad7d0ceb894d848547a6045ac8af0d);
        

        circle_marker_7479723bcdce4ed2a7df39c7f7a64e05.bindPopup(popup_5f3d304c679241ebafd953121dc16354)
        ;

        
    
    
            var circle_marker_7810a9c9f2f141668b49c959c4951340 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a33658921f7f4abbbf817335585ce9c2 = L.popup({"maxWidth": "100%"});

        
            var html_5d0e5a9d71594650bb56511fff3f271b = $(`<div id="html_5d0e5a9d71594650bb56511fff3f271b" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240631">Learning and fusing multimodal deep features for acoustic scene categorization</a><br></div>`)[0];
            popup_a33658921f7f4abbbf817335585ce9c2.setContent(html_5d0e5a9d71594650bb56511fff3f271b);
        

        circle_marker_7810a9c9f2f141668b49c959c4951340.bindPopup(popup_a33658921f7f4abbbf817335585ce9c2)
        ;

        
    
    
            var circle_marker_7847541c889447f9beea441a461680b3 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_956fd2511a984052bec0b708dd8f809e = L.popup({"maxWidth": "100%"});

        
            var html_eb3933376c93414e9e6bd74145c277c4 = $(`<div id="html_eb3933376c93414e9e6bd74145c277c4" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s10772-020-09681-3.pdf">Pattern analysis based acoustic signal processing: a survey of the state-of-art</a><br></div>`)[0];
            popup_956fd2511a984052bec0b708dd8f809e.setContent(html_eb3933376c93414e9e6bd74145c277c4);
        

        circle_marker_7847541c889447f9beea441a461680b3.bindPopup(popup_956fd2511a984052bec0b708dd8f809e)
        ;

        
    
    
            var circle_marker_95e2fac333c948a6b9bf6522b929fda2 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8c62167d36244d79be12d23a6d0b3bbb = L.popup({"maxWidth": "100%"});

        
            var html_a7e7fe0a606b4cc4ad242c59d6e189e1 = $(`<div id="html_a7e7fe0a606b4cc4ad242c59d6e189e1" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://www.researchgate.net/profile/Himanshu_Agrawal8/publication/336025224_Data_augmentation_for_cancer_classification_in_oncogenomics_an_improved_KNN_based_approach/links/5d92c634458515202b7778ff/Data-augmentation-for-cancer-classification-in-oncogenomics-an-improved-KNN-based-approach.pdf">Poonam Chaudhari, Himanshu Agarwal</a><br></div>`)[0];
            popup_8c62167d36244d79be12d23a6d0b3bbb.setContent(html_a7e7fe0a606b4cc4ad242c59d6e189e1);
        

        circle_marker_95e2fac333c948a6b9bf6522b929fda2.bindPopup(popup_8c62167d36244d79be12d23a6d0b3bbb)
        ;

        
    
    
            var circle_marker_5de65a643b9840788779328d26e78caf = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8d7c088291f84fe38af7138e908a0c99 = L.popup({"maxWidth": "100%"});

        
            var html_dadc321ef90c4d2da5e0102ef71159b1 = $(`<div id="html_dadc321ef90c4d2da5e0102ef71159b1" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://www.aclweb.org/anthology/2020.ecnlp-1.7/">Semi-Supervised Iterative Approach for Domain-Specific Complaint Detection in Social Media</a><br></div>`)[0];
            popup_8d7c088291f84fe38af7138e908a0c99.setContent(html_dadc321ef90c4d2da5e0102ef71159b1);
        

        circle_marker_5de65a643b9840788779328d26e78caf.bindPopup(popup_8d7c088291f84fe38af7138e908a0c99)
        ;

        
    
    
            var circle_marker_360f76c12e0f4cdd8791c98c52e6c038 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ac4337f8a4694694aaa2588306b1c2bf = L.popup({"maxWidth": "100%"});

        
            var html_88c95c5ef6d0440ea2067ffd009e2a32 = $(`<div id="html_88c95c5ef6d0440ea2067ffd009e2a32" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8593164/">Vocal and Non-vocal Segmentation based on the Analysis of Formant Structure</a><br></div>`)[0];
            popup_ac4337f8a4694694aaa2588306b1c2bf.setContent(html_88c95c5ef6d0440ea2067ffd009e2a32);
        

        circle_marker_360f76c12e0f4cdd8791c98c52e6c038.bindPopup(popup_ac4337f8a4694694aaa2588306b1c2bf)
        ;

        
    
    
            var circle_marker_32e90663a3b448b2894d16f29a73357a = L.circleMarker(
                [23.59829785, 120.83536313817521],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_cabb4d9e889b43dbba674ec0a7140e75 = L.popup({"maxWidth": "100%"});

        
            var html_0b9cd656c904476e976ab6c670d3062d = $(`<div id="html_0b9cd656c904476e976ab6c670d3062d" style="width: 100.0%; height: 100.0%;">Country : Taiwan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8326315/">Acoustic scene classification using convolutional neural networks and multi-scale multi-feature extraction</a><br></div>`)[0];
            popup_cabb4d9e889b43dbba674ec0a7140e75.setContent(html_0b9cd656c904476e976ab6c670d3062d);
        

        circle_marker_32e90663a3b448b2894d16f29a73357a.bindPopup(popup_cabb4d9e889b43dbba674ec0a7140e75)
        ;

        
    
    
            var circle_marker_616f4e23f7ed4e11a2e2550ac38888e0 = L.circleMarker(
                [23.59829785, 120.83536313817521],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_04c866ae3f2848698e314380d1f30340 = L.popup({"maxWidth": "100%"});

        
            var html_e39a567cdc6843a5aa0bc3e695c6772a = $(`<div id="html_e39a567cdc6843a5aa0bc3e695c6772a" style="width: 100.0%; height: 100.0%;">Country : Taiwan<br>                         Paper : <a href="http://dcase.community/documents/challenge2017/technical_reports/DCASE2017_Dang_209.pdf">Deep learning for DCASE2017 challenge</a><br></div>`)[0];
            popup_04c866ae3f2848698e314380d1f30340.setContent(html_e39a567cdc6843a5aa0bc3e695c6772a);
        

        circle_marker_616f4e23f7ed4e11a2e2550ac38888e0.bindPopup(popup_04c866ae3f2848698e314380d1f30340)
        ;

        
    
    
            var circle_marker_7abfb08bc6c54e54b4900117b48f6437 = L.circleMarker(
                [23.59829785, 120.83536313817521],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c4ff6d0b780a48508a0b9f59e3e6029a = L.popup({"maxWidth": "100%"});

        
            var html_276f2fe8341b4ba9be5f3c120309ccfe = $(`<div id="html_276f2fe8341b4ba9be5f3c120309ccfe" style="width: 100.0%; height: 100.0%;">Country : Taiwan<br>                         Paper : <a href="http://archives.ismir.net/ismir2019/paper/000104.pdf">Generating Structured Drum Pattern Using Variational Autoencoder and Self-similarity Matrix.</a><br></div>`)[0];
            popup_c4ff6d0b780a48508a0b9f59e3e6029a.setContent(html_276f2fe8341b4ba9be5f3c120309ccfe);
        

        circle_marker_7abfb08bc6c54e54b4900117b48f6437.bindPopup(popup_c4ff6d0b780a48508a0b9f59e3e6029a)
        ;

        
    
    
            var circle_marker_6a9c20ea01634bc287150d7cd25142e1 = L.circleMarker(
                [23.59829785, 120.83536313817521],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ff34b53a31e648eaba235a2858788637 = L.popup({"maxWidth": "100%"});

        
            var html_1f812ad72ed24fa38564f2945e36447b = $(`<div id="html_1f812ad72ed24fa38564f2945e36447b" style="width: 100.0%; height: 100.0%;">Country : Taiwan<br>                         Paper : <a href="https://www.ijcai.org/Proceedings/2018/0463.pdf">Learning to Recognize Transient Sound Events using Attentional Supervision.</a><br></div>`)[0];
            popup_ff34b53a31e648eaba235a2858788637.setContent(html_1f812ad72ed24fa38564f2945e36447b);
        

        circle_marker_6a9c20ea01634bc287150d7cd25142e1.bindPopup(popup_ff34b53a31e648eaba235a2858788637)
        ;

        
    
    
            var circle_marker_ab5ab6bedcf847eb854ac75c2878dcf9 = L.circleMarker(
                [52.215933, 19.134422],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ea187fab469b4c07b6b37530fb379e05 = L.popup({"maxWidth": "100%"});

        
            var html_aed7b62cb97f41749f7fbd22c7dad552 = $(`<div id="html_aed7b62cb97f41749f7fbd22c7dad552" style="width: 100.0%; height: 100.0%;">Country : Poland<br>                         Paper : <a href="https://www.karolpiczak.com/papers/Piczak2017-DCASE.pdf">The details that matter: Frequency resolution of spectrograms in acoustic scene classification</a><br></div>`)[0];
            popup_ea187fab469b4c07b6b37530fb379e05.setContent(html_aed7b62cb97f41749f7fbd22c7dad552);
        

        circle_marker_ab5ab6bedcf847eb854ac75c2878dcf9.bindPopup(popup_ea187fab469b4c07b6b37530fb379e05)
        ;

        
    
    
            var circle_marker_c4b779f3dd684951a2a64c325451f3e1 = L.circleMarker(
                [22.2793278, 114.1628131],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_63709d6176e348458340decd1f9e18f5 = L.popup({"maxWidth": "100%"});

        
            var html_aaff616f5de34956a28c9845378393ad = $(`<div id="html_aaff616f5de34956a28c9845378393ad" style="width: 100.0%; height: 100.0%;">Country : Hong Kong<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683490/">Enhancing sound texture in CNN-based acoustic scene classification</a><br></div>`)[0];
            popup_63709d6176e348458340decd1f9e18f5.setContent(html_aaff616f5de34956a28c9845378393ad);
        

        circle_marker_c4b779f3dd684951a2a64c325451f3e1.bindPopup(popup_63709d6176e348458340decd1f9e18f5)
        ;

        
    
    
            var circle_marker_da5c97fb53bd49fca198a519fe6642b7 = L.circleMarker(
                [22.2793278, 114.1628131],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_157075dee0474563aeed38e364842467 = L.popup({"maxWidth": "100%"});

        
            var html_c05fa28df59a4c759aa8f30ed79136a5 = $(`<div id="html_c05fa28df59a4c759aa8f30ed79136a5" style="width: 100.0%; height: 100.0%;">Country : Hong Kong<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8462168/">Reducing model complexity for DNN based large-scale audio classification</a><br></div>`)[0];
            popup_157075dee0474563aeed38e364842467.setContent(html_c05fa28df59a4c759aa8f30ed79136a5);
        

        circle_marker_da5c97fb53bd49fca198a519fe6642b7.bindPopup(popup_157075dee0474563aeed38e364842467)
        ;

        
    
    
            var circle_marker_ed75e449bae64ac3bc7bc592124449b8 = L.circleMarker(
                [38.9953683, 21.9877132],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_f91ab7d4710d4633a3b1e1e9ad6d7471 = L.popup({"maxWidth": "100%"});

        
            var html_b47db03e217844509db1e05edcbba390 = $(`<div id="html_b47db03e217844509db1e05edcbba390" style="width: 100.0%; height: 100.0%;">Country : Greece<br>                         Paper : <a href="https://dora.dmu.ac.uk/handle/2086/15000">Acoustic scene classification: From a hybrid classifier to deep learning</a><br></div>`)[0];
            popup_f91ab7d4710d4633a3b1e1e9ad6d7471.setContent(html_b47db03e217844509db1e05edcbba390);
        

        circle_marker_ed75e449bae64ac3bc7bc592124449b8.bindPopup(popup_f91ab7d4710d4633a3b1e1e9ad6d7471)
        ;

        
    
    
            var circle_marker_18abf5a27f3042be88079e2e9ec90721 = L.circleMarker(
                [38.9953683, 21.9877132],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_eb1e42697e5b46e180ca6499938cdbb9 = L.popup({"maxWidth": "100%"});

        
            var html_9975d4aa46f94793b3450bd76e27300e = $(`<div id="html_9975d4aa46f94793b3450bd76e27300e" style="width: 100.0%; height: 100.0%;">Country : Greece<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0952197619302052">Audio content analysis for unobtrusive event detection in smart homes</a><br></div>`)[0];
            popup_eb1e42697e5b46e180ca6499938cdbb9.setContent(html_9975d4aa46f94793b3450bd76e27300e);
        

        circle_marker_18abf5a27f3042be88079e2e9ec90721.bindPopup(popup_eb1e42697e5b46e180ca6499938cdbb9)
        ;

        
    
    
            var circle_marker_e23d29fd90bf4facac98ba177b0fcde1 = L.circleMarker(
                [38.9953683, 21.9877132],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0227498f379b41908b07499dd8bb7e0e = L.popup({"maxWidth": "100%"});

        
            var html_b48d1cc258674503b05728b6d159cb90 = $(`<div id="html_b48d1cc258674503b05728b6d159cb90" style="width: 100.0%; height: 100.0%;">Country : Greece<br>                         Paper : <a href="https://www.mdpi.com/1424-8220/19/22/4837">Deep learning on multi sensor data for counter UAV applications—a systematic review</a><br></div>`)[0];
            popup_0227498f379b41908b07499dd8bb7e0e.setContent(html_b48d1cc258674503b05728b6d159cb90);
        

        circle_marker_e23d29fd90bf4facac98ba177b0fcde1.bindPopup(popup_0227498f379b41908b07499dd8bb7e0e)
        ;

        
    
    
            var circle_marker_353da7a07b7a48c1bce2bf01684e1645 = L.circleMarker(
                [38.9953683, 21.9877132],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_4ad13e471bec403aa7b78b5cd692ebc9 = L.popup({"maxWidth": "100%"});

        
            var html_63b0737ec1e24221a40a7e24b9f42650 = $(`<div id="html_63b0737ec1e24221a40a7e24b9f42650" style="width: 100.0%; height: 100.0%;">Country : Greece<br>                         Paper : <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Tsiami_STAViS_Spatio-Temporal_AudioVisual_Saliency_Network_CVPR_2020_paper.html">STAViS: Spatio-Temporal AudioVisual Saliency Network</a><br></div>`)[0];
            popup_4ad13e471bec403aa7b78b5cd692ebc9.setContent(html_63b0737ec1e24221a40a7e24b9f42650);
        

        circle_marker_353da7a07b7a48c1bce2bf01684e1645.bindPopup(popup_4ad13e471bec403aa7b78b5cd692ebc9)
        ;

        
    
    
            var circle_marker_c0ac5d69703c4cb98ef8d482afa221f1 = L.circleMarker(
                [55.670249, 10.3333283],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_5f3b56ade6724e949527803f9e09a0a2 = L.popup({"maxWidth": "100%"});

        
            var html_3de0411a1f394b9bbc06d6488026007e = $(`<div id="html_3de0411a1f394b9bbc06d6488026007e" style="width: 100.0%; height: 100.0%;">Country : Denmark<br>                         Paper : <a href="https://dora.dmu.ac.uk/handle/2086/15000">Acoustic scene classification: From a hybrid classifier to deep learning</a><br></div>`)[0];
            popup_5f3b56ade6724e949527803f9e09a0a2.setContent(html_3de0411a1f394b9bbc06d6488026007e);
        

        circle_marker_c0ac5d69703c4cb98ef8d482afa221f1.bindPopup(popup_5f3b56ade6724e949527803f9e09a0a2)
        ;

        
    
    
            var circle_marker_ee2be7277379418da68ae8d2253966f2 = L.circleMarker(
                [55.670249, 10.3333283],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7df51a4ec87b47659d45048c6e0db19b = L.popup({"maxWidth": "100%"});

        
            var html_894431128d11491f9ca28bcc3e109dcf = $(`<div id="html_894431128d11491f9ca28bcc3e109dcf" style="width: 100.0%; height: 100.0%;">Country : Denmark<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09231-w">Trends in content-based recommendation</a><br></div>`)[0];
            popup_7df51a4ec87b47659d45048c6e0db19b.setContent(html_894431128d11491f9ca28bcc3e109dcf);
        

        circle_marker_ee2be7277379418da68ae8d2253966f2.bindPopup(popup_7df51a4ec87b47659d45048c6e0db19b)
        ;

        
    
    
            var circle_marker_69eb6c7b0fc9450e900a8b13066e9e5f = L.circleMarker(
                [52.5001698, 5.7480821],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b27614985104435084d4ef31b512612e = L.popup({"maxWidth": "100%"});

        
            var html_12e45d0ef14f4e5ea8dd2dfb06bf8588 = $(`<div id="html_12e45d0ef14f4e5ea8dd2dfb06bf8588" style="width: 100.0%; height: 100.0%;">Country : Netherlands<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7927482/">DCAR: A discriminative and compact audio representation for audio processing</a><br></div>`)[0];
            popup_b27614985104435084d4ef31b512612e.setContent(html_12e45d0ef14f4e5ea8dd2dfb06bf8588);
        

        circle_marker_69eb6c7b0fc9450e900a8b13066e9e5f.bindPopup(popup_b27614985104435084d4ef31b512612e)
        ;

        
    
    
            var circle_marker_15bafd414eb349d99149af838f1f6af3 = L.circleMarker(
                [52.5001698, 5.7480821],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_38e03326398048ed986108f3cc7effdb = L.popup({"maxWidth": "100%"});

        
            var html_f888b2539d6f403ab87d04af3a1ff2d7 = $(`<div id="html_f888b2539d6f403ab87d04af3a1ff2d7" style="width: 100.0%; height: 100.0%;">Country : Netherlands<br>                         Paper : <a href="">K-shot learning of acoustic context</a><br></div>`)[0];
            popup_38e03326398048ed986108f3cc7effdb.setContent(html_f888b2539d6f403ab87d04af3a1ff2d7);
        

        circle_marker_15bafd414eb349d99149af838f1f6af3.bindPopup(popup_38e03326398048ed986108f3cc7effdb)
        ;

        
    
    
            var circle_marker_f6828587a69d49b287e42c3225b63b02 = L.circleMarker(
                [52.5001698, 5.7480821],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_af7ef59847d844848edd3ab0d5e90d42 = L.popup({"maxWidth": "100%"});

        
            var html_5502f07752804373a46b8a78409482d8 = $(`<div id="html_5502f07752804373a46b8a78409482d8" style="width: 100.0%; height: 100.0%;">Country : Netherlands<br>                         Paper : <a href="">Linking open public datasets for multifaceted music discovery</a><br></div>`)[0];
            popup_af7ef59847d844848edd3ab0d5e90d42.setContent(html_5502f07752804373a46b8a78409482d8);
        

        circle_marker_f6828587a69d49b287e42c3225b63b02.bindPopup(popup_af7ef59847d844848edd3ab0d5e90d42)
        ;

        
    
    
            var circle_marker_baba982d45014480897dc39496708449 = L.circleMarker(
                [38.9597594, 34.9249653],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ef5f83178798430cb8457557f518be5c = L.popup({"maxWidth": "100%"});

        
            var html_f1c74c214a1843f19267ad6a8fbaf93e = $(`<div id="html_f1c74c214a1843f19267ad6a8fbaf93e" style="width: 100.0%; height: 100.0%;">Country : Turkey<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9052658/">A New Deep CNN Model for Environmental Sound Classification</a><br></div>`)[0];
            popup_ef5f83178798430cb8457557f518be5c.setContent(html_f1c74c214a1843f19267ad6a8fbaf93e);
        

        circle_marker_baba982d45014480897dc39496708449.bindPopup(popup_ef5f83178798430cb8457557f518be5c)
        ;

        
    
    
            var circle_marker_062777b57df841a7a2833f78430825d2 = L.circleMarker(
                [38.9597594, 34.9249653],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_d58276d25f0e4b89b5aacd3d2c57ab38 = L.popup({"maxWidth": "100%"});

        
            var html_0b187b162be741d5a3697b6045527109 = $(`<div id="html_0b187b162be741d5a3697b6045527109" style="width: 100.0%; height: 100.0%;">Country : Turkey<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s10462-018-9654-y">A review on deep learning for recommender systems: challenges and remedies</a><br></div>`)[0];
            popup_d58276d25f0e4b89b5aacd3d2c57ab38.setContent(html_0b187b162be741d5a3697b6045527109);
        

        circle_marker_062777b57df841a7a2833f78430825d2.bindPopup(popup_d58276d25f0e4b89b5aacd3d2c57ab38)
        ;

        
    
    
            var circle_marker_d301a1f357904852aff6a89a38daa5cc = L.circleMarker(
                [38.9597594, 34.9249653],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e35b3aa34a04492184b07d0aa6c0a1a4 = L.popup({"maxWidth": "100%"});

        
            var html_30e896a5cfff46e1bc2d3f2cc4447cee = $(`<div id="html_30e896a5cfff46e1bc2d3f2cc4447cee" style="width: 100.0%; height: 100.0%;">Country : Turkey<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8665547/">Acoustic Scene Classification Using Spatial Pyramid Pooling with Convolutional Neural Networks</a><br></div>`)[0];
            popup_e35b3aa34a04492184b07d0aa6c0a1a4.setContent(html_30e896a5cfff46e1bc2d3f2cc4447cee);
        

        circle_marker_d301a1f357904852aff6a89a38daa5cc.bindPopup(popup_e35b3aa34a04492184b07d0aa6c0a1a4)
        ;

        
    
    
            var circle_marker_6af4b305fe214bdabc51858f3d555d13 = L.circleMarker(
                [38.9597594, 34.9249653],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e6f089a5bf6b438b913fa4ed7b94184c = L.popup({"maxWidth": "100%"});

        
            var html_73d32272cf224d48b5dcbb8a5fca40ca = $(`<div id="html_73d32272cf224d48b5dcbb8a5fca40ca" style="width: 100.0%; height: 100.0%;">Country : Turkey<br>                         Paper : <a href="http://etd.lib.metu.edu.tr/upload/12619947/index.pdf">An Integrated use of vectorial approach with analytical and synthetic approaches: a teaching experiment with eleventh grade students on quadrilaterals</a><br></div>`)[0];
            popup_e6f089a5bf6b438b913fa4ed7b94184c.setContent(html_73d32272cf224d48b5dcbb8a5fca40ca);
        

        circle_marker_6af4b305fe214bdabc51858f3d555d13.bindPopup(popup_e6f089a5bf6b438b913fa4ed7b94184c)
        ;

        
    
    
            var circle_marker_31c815076ca44bebabd33ef0aad7bd44 = L.circleMarker(
                [30.3308401, 71.247499],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1f4f9b53dde14b4aab06735ce9615fa4 = L.popup({"maxWidth": "100%"});

        
            var html_cbf7f1794b7b4b09af3594ab1921a5f8 = $(`<div id="html_cbf7f1794b7b4b09af3594ab1921a5f8" style="width: 100.0%; height: 100.0%;">Country : Pakistan<br>                         Paper : <a href="http://asrjetsjournal.org/index.php/American_Scientific_Journal/article/view/4169">An improved acoustic scene classification method using convolutional neural networks (CNNs)</a><br></div>`)[0];
            popup_1f4f9b53dde14b4aab06735ce9615fa4.setContent(html_cbf7f1794b7b4b09af3594ab1921a5f8);
        

        circle_marker_31c815076ca44bebabd33ef0aad7bd44.bindPopup(popup_1f4f9b53dde14b4aab06735ce9615fa4)
        ;

        
    
    
            var circle_marker_94c559044ac44281a3c5dbc0608305d8 = L.circleMarker(
                [30.3308401, 71.247499],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a7d85e259d5a4a8fba1b061e130e1d2d = L.popup({"maxWidth": "100%"});

        
            var html_76b0ee8e131a4a6f99acb28a497ccf1c = $(`<div id="html_76b0ee8e131a4a6f99acb28a497ccf1c" style="width: 100.0%; height: 100.0%;">Country : Pakistan<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Hussain_166.pdf">Improved acoustic scene classification with DNN and CNN</a><br></div>`)[0];
            popup_a7d85e259d5a4a8fba1b061e130e1d2d.setContent(html_76b0ee8e131a4a6f99acb28a497ccf1c);
        

        circle_marker_94c559044ac44281a3c5dbc0608305d8.bindPopup(popup_a7d85e259d5a4a8fba1b061e130e1d2d)
        ;

        
    
    
            var circle_marker_9e08d271d0504bf5b7611f30c8ea5be5 = L.circleMarker(
                [49.4871968, 31.2718321],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_aad8bf4571aa4816a093c4ee1ccc9f76 = L.popup({"maxWidth": "100%"});

        
            var html_eba0da2f3ad44ba6ae41bc1f6c7c7256 = $(`<div id="html_eba0da2f3ad44ba6ae41bc1f6c7c7256" style="width: 100.0%; height: 100.0%;">Country : UAE<br>                         Paper : <a href="https://arxiv.org/abs/1811.00936">Acoustic features fusion using attentive multi-channel deep architecture</a><br></div>`)[0];
            popup_aad8bf4571aa4816a093c4ee1ccc9f76.setContent(html_eba0da2f3ad44ba6ae41bc1f6c7c7256);
        

        circle_marker_9e08d271d0504bf5b7611f30c8ea5be5.bindPopup(popup_aad8bf4571aa4816a093c4ee1ccc9f76)
        ;

        
    
    
            var circle_marker_5053c32f5fa14bd4b3f34cca6c2f682c = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_51755d34fbf54aa4901c0f45f461bb6f = L.popup({"maxWidth": "100%"});

        
            var html_af0dcd00bf114a52992e7564d0eaa0a6 = $(`<div id="html_af0dcd00bf114a52992e7564d0eaa0a6" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s00034-019-01094-1">A survey: Neural network-based deep learning for acoustic event detection</a><br></div>`)[0];
            popup_51755d34fbf54aa4901c0f45f461bb6f.setContent(html_af0dcd00bf114a52992e7564d0eaa0a6);
        

        circle_marker_5053c32f5fa14bd4b3f34cca6c2f682c.bindPopup(popup_51755d34fbf54aa4901c0f45f461bb6f)
        ;

        
    
    
            var circle_marker_a59dd6e2b8d94e99b9605d7890892da5 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_6dbbca20993f457eb98267e35541ec30 = L.popup({"maxWidth": "100%"});

        
            var html_523e52f6287546cb927c14ea3eb477b2 = $(`<div id="html_523e52f6287546cb927c14ea3eb477b2" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053274/">Acoustic scene classification using deep residual networks with late fusion of separated high and low frequency paths</a><br></div>`)[0];
            popup_6dbbca20993f457eb98267e35541ec30.setContent(html_523e52f6287546cb927c14ea3eb477b2);
        

        circle_marker_a59dd6e2b8d94e99b9605d7890892da5.bindPopup(popup_6dbbca20993f457eb98267e35541ec30)
        ;

        
    
    
            var circle_marker_437042c56d4b4e2aba738be1b4b57dc2 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b789004667f243339197a44a401d047f = L.popup({"maxWidth": "100%"});

        
            var html_4512100a505146f2a503bbd8f37a2a2d = $(`<div id="html_4512100a505146f2a503bbd8f37a2a2d" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://arxiv.org/abs/1901.06125">Cold-start playlist recommendation with multitask learning</a><br></div>`)[0];
            popup_b789004667f243339197a44a401d047f.setContent(html_4512100a505146f2a503bbd8f37a2a2d);
        

        circle_marker_437042c56d4b4e2aba738be1b4b57dc2.bindPopup(popup_b789004667f243339197a44a401d047f)
        ;

        
    
    
            var circle_marker_4b970f51159149b08aeebdfe2313e4f2 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_285faaa5ffe8467a80b35c757780d7d8 = L.popup({"maxWidth": "100%"});

        
            var html_7231dd1f377e457eb5168a359c3fd2a1 = $(`<div id="html_7231dd1f377e457eb5168a359c3fd2a1" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s00500-019-04602-2.pdf">Data augmentation using MG-GAN for improved cancer classification on gene expression data</a><br></div>`)[0];
            popup_285faaa5ffe8467a80b35c757780d7d8.setContent(html_7231dd1f377e457eb5168a359c3fd2a1);
        

        circle_marker_4b970f51159149b08aeebdfe2313e4f2.bindPopup(popup_285faaa5ffe8467a80b35c757780d7d8)
        ;

        
    
    
            var circle_marker_da6184946a4b47f386a3a1f63065f883 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_a57ef62b657a4abda3da21472eb1a87a = L.popup({"maxWidth": "100%"});

        
            var html_9b8ba60649f846d780ffb0088da918eb = $(`<div id="html_9b8ba60649f846d780ffb0088da918eb" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://journals.sagepub.com/doi/abs/10.1177/2059204319893179">Encouraging Attention and Exploration in a Hybrid Recommender System for Libraries of Unfamiliar Music</a><br></div>`)[0];
            popup_a57ef62b657a4abda3da21472eb1a87a.setContent(html_9b8ba60649f846d780ffb0088da918eb);
        

        circle_marker_da6184946a4b47f386a3a1f63065f883.bindPopup(popup_a57ef62b657a4abda3da21472eb1a87a)
        ;

        
    
    
            var circle_marker_5d83de5495d34683b0c5f5f9147d37e4 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_e4cf397ae5f74d55a9d4c934ff4aeef6 = L.popup({"maxWidth": "100%"});

        
            var html_7103ceba50df42d0957a1bd2bee56c43 = $(`<div id="html_7103ceba50df42d0957a1bd2bee56c43" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://openreview.net/forum?id=HJgEe1SKPr">GAN-based Gaussian Mixture Model Responsibility Learning</a><br></div>`)[0];
            popup_e4cf397ae5f74d55a9d4c934ff4aeef6.setContent(html_7103ceba50df42d0957a1bd2bee56c43);
        

        circle_marker_5d83de5495d34683b0c5f5f9147d37e4.bindPopup(popup_e4cf397ae5f74d55a9d4c934ff4aeef6)
        ;

        
    
    
            var circle_marker_d5ce5fce10bf49fab70f39c9bc6ce2b6 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0ba11e8794154de492e90a4d7c013e27 = L.popup({"maxWidth": "100%"});

        
            var html_7b63873c36cf407ebfb27b183e2e030b = $(`<div id="html_7b63873c36cf407ebfb27b183e2e030b" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9103031/">Sound Event Detection Using Multiple Optimized Kernels</a><br></div>`)[0];
            popup_0ba11e8794154de492e90a4d7c013e27.setContent(html_7b63873c36cf407ebfb27b183e2e030b);
        

        circle_marker_d5ce5fce10bf49fab70f39c9bc6ce2b6.bindPopup(popup_0ba11e8794154de492e90a4d7c013e27)
        ;

        
    
    
            var circle_marker_083260f6b36841caa49bcd7a34fdbd67 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_2b012e9b19ef4a3f919dd3acec34aefe = L.popup({"maxWidth": "100%"});

        
            var html_c5e2a0b6a192461682336e7fcd3c6a79 = $(`<div id="html_c5e2a0b6a192461682336e7fcd3c6a79" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://www.ingentaconnect.com/contentone/ince/ncej/2020/00000068/00000004/art00004">Squeak and rattle noise classification using radial basis function neural networks</a><br></div>`)[0];
            popup_2b012e9b19ef4a3f919dd3acec34aefe.setContent(html_c5e2a0b6a192461682336e7fcd3c6a79);
        

        circle_marker_083260f6b36841caa49bcd7a34fdbd67.bindPopup(popup_2b012e9b19ef4a3f919dd3acec34aefe)
        ;

        
    
    
            var circle_marker_cf688fc077fe4cf7ab656cff5400b741 = L.circleMarker(
                [24.4768783, 90.2932426],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_05cd3e5f112c43fa9bea138b163128d6 = L.popup({"maxWidth": "100%"});

        
            var html_75787f771c8d4044846c0c4b2cd67623 = $(`<div id="html_75787f771c8d4044846c0c4b2cd67623" style="width: 100.0%; height: 100.0%;">Country : Bangladesh<br>                         Paper : <a href="https://arxiv.org/abs/1811.05540">Native Language Identification using i-vector</a><br></div>`)[0];
            popup_05cd3e5f112c43fa9bea138b163128d6.setContent(html_75787f771c8d4044846c0c4b2cd67623);
        

        circle_marker_cf688fc077fe4cf7ab656cff5400b741.bindPopup(popup_05cd3e5f112c43fa9bea138b163128d6)
        ;

        
    
    
            var circle_marker_8bff9550f6304d088db8881fc47af689 = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_1507e96c09fd46c0b82f409800922cd6 = L.popup({"maxWidth": "100%"});

        
            var html_732a9b28e9154f38bd44f1c4d93597a3 = $(`<div id="html_732a9b28e9154f38bd44f1c4d93597a3" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://arxiv.org/abs/1904.11914">I-vector Based Features Embedding for Heart Sound Classification</a><br></div>`)[0];
            popup_1507e96c09fd46c0b82f409800922cd6.setContent(html_732a9b28e9154f38bd44f1c4d93597a3);
        

        circle_marker_8bff9550f6304d088db8881fc47af689.bindPopup(popup_1507e96c09fd46c0b82f409800922cd6)
        ;

        
    
    
            var circle_marker_05a71e783917467e9c555d2bc620c72c = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_305dd176d8404e978f170387c0c23df1 = L.popup({"maxWidth": "100%"});

        
            var html_e219b96c912140eebf65beecf8792333 = $(`<div id="html_e219b96c912140eebf65beecf8792333" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877452/">Movie genome recommender: A novel recommender system based on multimedia content</a><br></div>`)[0];
            popup_305dd176d8404e978f170387c0c23df1.setContent(html_e219b96c912140eebf65beecf8792333);
        

        circle_marker_05a71e783917467e9c555d2bc620c72c.bindPopup(popup_305dd176d8404e978f170387c0c23df1)
        ;

        
    
    
            var circle_marker_a3e97902cb9341dcb9f858c491d0276e = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_8263cad7982d4ea790a10cfd1f1d9cea = L.popup({"maxWidth": "100%"});

        
            var html_56581ba39a2c46e29102a2402e7695ec = $(`<div id="html_56581ba39a2c46e29102a2402e7695ec" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3301275.3302266">Prediction of music pairwise preferences from facial expressions</a><br></div>`)[0];
            popup_8263cad7982d4ea790a10cfd1f1d9cea.setContent(html_56581ba39a2c46e29102a2402e7695ec);
        

        circle_marker_a3e97902cb9341dcb9f858c491d0276e.bindPopup(popup_8263cad7982d4ea790a10cfd1f1d9cea)
        ;

        
    
    
            var circle_marker_603821b051274fd196fe4223899a08c2 = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_754608bb8d914e11aa4a7822caf0efb5 = L.popup({"maxWidth": "100%"});

        
            var html_40fe7f6f814d4d3997a3d42687fc7e0c = $(`<div id="html_40fe7f6f814d4d3997a3d42687fc7e0c" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://www.researchgate.net/profile/Mohammad_Adiban3/publication/336312145_Statistical_Feature_Embedding_for_Heart_Sound_Classification/links/5d9b32da92851c2f70f2b9ca/Statistical-Feature-Embedding-for-Heart-Sound-Classification.pdf">Statistical based features embedding for heart sound classification</a><br></div>`)[0];
            popup_754608bb8d914e11aa4a7822caf0efb5.setContent(html_40fe7f6f814d4d3997a3d42687fc7e0c);
        

        circle_marker_603821b051274fd196fe4223899a08c2.bindPopup(popup_754608bb8d914e11aa4a7822caf0efb5)
        ;

        
    
    
            var circle_marker_a030433551cb4887a7f27b0be1b45ed5 = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ed87074e89794b26a2a507d4e9d3acad = L.popup({"maxWidth": "100%"});

        
            var html_cd93942be2864c38a25f370b79ee5ac0 = $(`<div id="html_cd93942be2864c38a25f370b79ee5ac0" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://content.sciendo.com/view/journals/jee/70/4/article-p259.xml">Statistical feature embedding for heart sound classification</a><br></div>`)[0];
            popup_ed87074e89794b26a2a507d4e9d3acad.setContent(html_cd93942be2864c38a25f370b79ee5ac0);
        

        circle_marker_a030433551cb4887a7f27b0be1b45ed5.bindPopup(popup_ed87074e89794b26a2a507d4e9d3acad)
        ;

        
    
    
            var circle_marker_82a66b09d197498d9a756009393e472a = L.circleMarker(
                [40.0332629, -7.8896263],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_fc2423b8a27a466097bc4e492d951bae = L.popup({"maxWidth": "100%"});

        
            var html_7cd1b197064c4e53a22893e96982d8c0 = $(`<div id="html_7cd1b197064c4e53a22893e96982d8c0" style="width: 100.0%; height: 100.0%;">Country : Portugal<br>                         Paper : <a href="http://www.journals.isel.pt/index.php/i-ETC/article/view/35">Automatic Acoustic Scene Classification</a><br></div>`)[0];
            popup_fc2423b8a27a466097bc4e492d951bae.setContent(html_7cd1b197064c4e53a22893e96982d8c0);
        

        circle_marker_82a66b09d197498d9a756009393e472a.bindPopup(popup_fc2423b8a27a466097bc4e492d951bae)
        ;

        
    
    
            var circle_marker_4c86ead7ab8f41d8a756186505788ded = L.circleMarker(
                [-10.3333333, -53.2],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_7ea77bff9eac4786aea3fd9326aeeb33 = L.popup({"maxWidth": "100%"});

        
            var html_04905479072a432eb3d070a52d8d4359 = $(`<div id="html_04905479072a432eb3d070a52d8d4359" style="width: 100.0%; height: 100.0%;">Country : Brazil<br>                         Paper : <a href="https://www.soundeffects.dk/article/view/115027">An acoustemology of streaming media and information and communication technologies</a><br></div>`)[0];
            popup_7ea77bff9eac4786aea3fd9326aeeb33.setContent(html_04905479072a432eb3d070a52d8d4359);
        

        circle_marker_4c86ead7ab8f41d8a756186505788ded.bindPopup(popup_7ea77bff9eac4786aea3fd9326aeeb33)
        ;

        
    
    
            var circle_marker_7fde9358ec764ae4a33f1cd4f557e4d3 = L.circleMarker(
                [-10.3333333, -53.2],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_27b65f44c9864323a57ae190194b74fa = L.popup({"maxWidth": "100%"});

        
            var html_9080f3ea52974355915f1f77fa324591 = $(`<div id="html_9080f3ea52974355915f1f77fa324591" style="width: 100.0%; height: 100.0%;">Country : Brazil<br>                         Paper : <a href="https://sol.sbc.org.br/index.php/sbcup/article/view/11227">Arquitetura embarcável para detecção de eventos sonoros utilizando inteligência artificial</a><br></div>`)[0];
            popup_27b65f44c9864323a57ae190194b74fa.setContent(html_9080f3ea52974355915f1f77fa324591);
        

        circle_marker_7fde9358ec764ae4a33f1cd4f557e4d3.bindPopup(popup_27b65f44c9864323a57ae190194b74fa)
        ;

        
    
    
            var circle_marker_8124d1414d3f45f895d98408b1190fee = L.circleMarker(
                [26.2540493, 29.2675469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_95b2809b9b634a478cbabc592a012a54 = L.popup({"maxWidth": "100%"});

        
            var html_b47eb3cf59254e6e92db97e5690a5cca = $(`<div id="html_b47eb3cf59254e6e92db97e5690a5cca" style="width: 100.0%; height: 100.0%;">Country : Egypt<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-44289-7_51">Improving the Data Quality of the MovieLens Dataset Using Dimensionality Reduction Techniques</a><br></div>`)[0];
            popup_95b2809b9b634a478cbabc592a012a54.setContent(html_b47eb3cf59254e6e92db97e5690a5cca);
        

        circle_marker_8124d1414d3f45f895d98408b1190fee.bindPopup(popup_95b2809b9b634a478cbabc592a012a54)
        ;

        
    
    
            var circle_marker_888fca8090f940399d3b60dad59140bc = L.circleMarker(
                [52.865196, -7.9794599],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_0ef7711655604613bc39beb8156f18a3 = L.popup({"maxWidth": "100%"});

        
            var html_f4b7d2c9198843708290fa76c10932bd = $(`<div id="html_f4b7d2c9198843708290fa76c10932bd" style="width: 100.0%; height: 100.0%;">Country : Ireland<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3359555.3359563">Leveraging laziness, browsing-pattern aware stacked models for sequential accommodation learning to rank</a><br></div>`)[0];
            popup_0ef7711655604613bc39beb8156f18a3.setContent(html_f4b7d2c9198843708290fa76c10932bd);
        

        circle_marker_888fca8090f940399d3b60dad59140bc.bindPopup(popup_0ef7711655604613bc39beb8156f18a3)
        ;

        
    
    
            var circle_marker_6eb709a79f0d44418aac8d19d4899484 = L.circleMarker(
                [9.6000359, 7.9999721],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_07391dac4a9441dcb68f9f4378bcda1a = L.popup({"maxWidth": "100%"});

        
            var html_1e8ee601ac5b4a60b8d20d2c22c21a97 = $(`<div id="html_1e8ee601ac5b4a60b8d20d2c22c21a97" style="width: 100.0%; height: 100.0%;">Country : Nigeria<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11192-020-03642-y">Research paper recommender system based on public contextual metadata</a><br></div>`)[0];
            popup_07391dac4a9441dcb68f9f4378bcda1a.setContent(html_1e8ee601ac5b4a60b8d20d2c22c21a97);
        

        circle_marker_6eb709a79f0d44418aac8d19d4899484.bindPopup(popup_07391dac4a9441dcb68f9f4378bcda1a)
        ;

        
    
    
            var circle_marker_b3c94f3b401f42879af1a25250cf00bb = L.circleMarker(
                [4.5693754, 102.2656823],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_35950b723b23427eb14b3bc31ab065a2 = L.popup({"maxWidth": "100%"});

        
            var html_ef7b7a56d4bd444aa4592934bf470c80 = $(`<div id="html_ef7b7a56d4bd444aa4592934bf470c80" style="width: 100.0%; height: 100.0%;">Country : Malaysia<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11192-020-03642-y">Research paper recommender system based on public contextual metadata</a><br></div>`)[0];
            popup_35950b723b23427eb14b3bc31ab065a2.setContent(html_ef7b7a56d4bd444aa4592934bf470c80);
        

        circle_marker_b3c94f3b401f42879af1a25250cf00bb.bindPopup(popup_35950b723b23427eb14b3bc31ab065a2)
        ;

        
    
    
            var circle_marker_5c91cd843e3a457f855e37a3240550db = L.circleMarker(
                [25.6242618, 42.3528328],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_b968d0c4294b4e63be213d22edf16b10 = L.popup({"maxWidth": "100%"});

        
            var html_bbc1085af10c418ba641d8ddbc39284f = $(`<div id="html_bbc1085af10c418ba641d8ddbc39284f" style="width: 100.0%; height: 100.0%;">Country : Saudi Arabia<br>                         Paper : <a href="https://www.mdpi.com/2076-3417/9/22/4866">Aroma Release of Olfactory Displays Based on Audio-Visual Content</a><br></div>`)[0];
            popup_b968d0c4294b4e63be213d22edf16b10.setContent(html_bbc1085af10c418ba641d8ddbc39284f);
        

        circle_marker_5c91cd843e3a457f855e37a3240550db.bindPopup(popup_b968d0c4294b4e63be213d22edf16b10)
        ;

        
    
    
            var circle_marker_16c61490994b42b587e61d62fc564f8e = L.circleMarker(
                [45.8133113, 14.4808369],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_28f1de4d639b4408bf40ac504fd8b326 = L.popup({"maxWidth": "100%"});

        
            var html_8da3424999cd49848e985d18484e632d = $(`<div id="html_8da3424999cd49848e985d18484e632d" style="width: 100.0%; height: 100.0%;">Country : Slovenia<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3301275.3302266">Prediction of music pairwise preferences from facial expressions</a><br></div>`)[0];
            popup_28f1de4d639b4408bf40ac504fd8b326.setContent(html_8da3424999cd49848e985d18484e632d);
        

        circle_marker_16c61490994b42b587e61d62fc564f8e.bindPopup(popup_28f1de4d639b4408bf40ac504fd8b326)
        ;

        
    
    
            var circle_marker_d7d3c1731c864f5bb35a4b846ad2f0e2 = L.circleMarker(
                [59.6749712, 14.5208584],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_c1cdf2819eaa478190adb6337eccf3e7 = L.popup({"maxWidth": "100%"});

        
            var html_18e1124a1bac4bbd9efe37a659fe9f2e = $(`<div id="html_18e1124a1bac4bbd9efe37a659fe9f2e" style="width: 100.0%; height: 100.0%;">Country : Sweden<br>                         Paper : <a href="https://books.google.com/books?hl=de&lr=&id=6jvEDwAAQBAJ&oi=fnd&pg=PA223&ots=FUoGquoOv0&sig=3RyQmnd8MD1YU2VZq8Kbbhl_ezU">User Awareness in Music Recommender Systems</a><br></div>`)[0];
            popup_c1cdf2819eaa478190adb6337eccf3e7.setContent(html_18e1124a1bac4bbd9efe37a659fe9f2e);
        

        circle_marker_d7d3c1731c864f5bb35a4b846ad2f0e2.bindPopup(popup_c1cdf2819eaa478190adb6337eccf3e7)
        ;

        
    
    
            var circle_marker_ae74006feae649edb06be062bacc3d22 = L.circleMarker(
                [-31.7613365, -71.3187697],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_fed02479aba5438f9de49785c4283dfe);
        
    
        var popup_ea34426a84ab4a0cb1bd47cc5fd40206 = L.popup({"maxWidth": "100%"});

        
            var html_7812a3cc9b384142ad33ba8134bfb51f = $(`<div id="html_7812a3cc9b384142ad33ba8134bfb51f" style="width: 100.0%; height: 100.0%;">Country : Chile<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8979409/">Artificial Generation of Partial Discharge Sources Through an Algorithm Based on Deep Convolutional Generative Adversarial Networks</a><br></div>`)[0];
            popup_ea34426a84ab4a0cb1bd47cc5fd40206.setContent(html_7812a3cc9b384142ad33ba8134bfb51f);
        

        circle_marker_ae74006feae649edb06be062bacc3d22.bindPopup(popup_ea34426a84ab4a0cb1bd47cc5fd40206)
        ;

        
    
</script>
