<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    
        <script>
            L_NO_TOUCH = false;
            L_DISABLE_3D = false;
        </script>
    
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.6.0/dist/leaflet.js"></script>
    <script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.6.0/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
            <meta name="viewport" content="width=device-width,
                initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
            <style>
                #map_66d1e0e2cd214bedb7a43b1d14a97a8c {
                    position: relative;
                    width: 100.0%;
                    height: 100.0%;
                    left: 0.0%;
                    top: 0.0%;
                }
            </style>
        
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/leaflet.markercluster.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.Default.css"/>
</head>
<body>    
    
            <div class="folium-map" id="map_66d1e0e2cd214bedb7a43b1d14a97a8c" ></div>
        
</body>
<script>    
    
            var map_66d1e0e2cd214bedb7a43b1d14a97a8c = L.map(
                "map_66d1e0e2cd214bedb7a43b1d14a97a8c",
                {
                    center: [0, 0],
                    crs: L.CRS.EPSG3857,
                    zoom: 1,
                    zoomControl: true,
                    preferCanvas: false,
                }
            );

            

        
    
            var tile_layer_eb88532b7d654a3fb532ea64b321a7e1 = L.tileLayer(
                "https://cartodb-basemaps-{s}.global.ssl.fastly.net/light_all/{z}/{x}/{y}.png",
                {"attribution": "\u0026copy; \u003ca href=\"http://www.openstreetmap.org/copyright\"\u003eOpenStreetMap\u003c/a\u003e contributors \u0026copy; \u003ca href=\"http://cartodb.com/attributions\"\u003eCartoDB\u003c/a\u003e, CartoDB \u003ca href =\"http://cartodb.com/attributions\"\u003eattributions\u003c/a\u003e", "detectRetina": false, "maxNativeZoom": 18, "maxZoom": 18, "minZoom": 0, "noWrap": false, "opacity": 1, "subdomains": "abc", "tms": false}
            ).addTo(map_66d1e0e2cd214bedb7a43b1d14a97a8c);
        
    
            var marker_cluster_7a7253add80b4b16a3615265ee4b1de0 = L.markerClusterGroup(
                {}
            );
            map_66d1e0e2cd214bedb7a43b1d14a97a8c.addLayer(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
            var circle_marker_f51f145e06324b87a957e7ccbdba75b2 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_aab84b1593804434a7a35a67d1648e28 = L.popup({"maxWidth": "100%"});

        
            var html_d9426b45e05045dba2c18118d98c40ae = $(`<div id="html_d9426b45e05045dba2c18118d98c40ae" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/2008.03350">A Joint Framework for Audio Tagging and Weakly Supervised Acoustic Event Detection Using DenseNet with Global Average Pooling</a><br></div>`)[0];
            popup_aab84b1593804434a7a35a67d1648e28.setContent(html_d9426b45e05045dba2c18118d98c40ae);
        

        circle_marker_f51f145e06324b87a957e7ccbdba75b2.bindPopup(popup_aab84b1593804434a7a35a67d1648e28)
        ;

        
    
    
            var circle_marker_84be975159b84479aec473e84e7a9989 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f14b8ed5200c4152939502ba9f021edc = L.popup({"maxWidth": "100%"});

        
            var html_45be3eb699e64b55a09be4c11b771028 = $(`<div id="html_45be3eb699e64b55a09be4c11b771028" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1903.07714">A RAD approach to deep mixture models</a><br></div>`)[0];
            popup_f14b8ed5200c4152939502ba9f021edc.setContent(html_45be3eb699e64b55a09be4c11b771028);
        

        circle_marker_84be975159b84479aec473e84e7a9989.bindPopup(popup_f14b8ed5200c4152939502ba9f021edc)
        ;

        
    
    
            var circle_marker_9c880e5980df48e2927b244fde4057b3 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e0bb59c4379d492f932104638ed3224a = L.popup({"maxWidth": "100%"});

        
            var html_32c9ca150d934accbb54375c9e516e0e = $(`<div id="html_32c9ca150d934accbb54375c9e516e0e" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7952131/">A comparison of deep learning methods for environmental sound detection</a><br></div>`)[0];
            popup_e0bb59c4379d492f932104638ed3224a.setContent(html_32c9ca150d934accbb54375c9e516e0e);
        

        circle_marker_9c880e5980df48e2927b244fde4057b3.bindPopup(popup_e0bb59c4379d492f932104638ed3224a)
        ;

        
    
    
            var circle_marker_79118148f0404d7bba63e74f79590b87 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ba0a5c5c039648a4bda914ad74bf6b17 = L.popup({"maxWidth": "100%"});

        
            var html_98ede219e0f54d73858f8059a76bae53 = $(`<div id="html_98ede219e0f54d73858f8059a76bae53" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8883750/">A low-cost driver and passenger activity detection system based on deep learning and multiple sensor fusion</a><br></div>`)[0];
            popup_ba0a5c5c039648a4bda914ad74bf6b17.setContent(html_98ede219e0f54d73858f8059a76bae53);
        

        circle_marker_79118148f0404d7bba63e74f79590b87.bindPopup(popup_ba0a5c5c039648a4bda914ad74bf6b17)
        ;

        
    
    
            var circle_marker_433307e10aae4ca3a925f8189faa9c32 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_565b3af2a95a40198a6d36024380568b = L.popup({"maxWidth": "100%"});

        
            var html_d3800bd69e504bddbfe20a90ec5cea12 = $(`<div id="html_d3800bd69e504bddbfe20a90ec5cea12" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://search.ieice.org/bin/summary.php?id=e100-d_12_3041">A novel discriminative feature extraction for acoustic scene classification using rnn based source separation</a><br></div>`)[0];
            popup_565b3af2a95a40198a6d36024380568b.setContent(html_d3800bd69e504bddbfe20a90ec5cea12);
        

        circle_marker_433307e10aae4ca3a925f8189faa9c32.bindPopup(popup_565b3af2a95a40198a6d36024380568b)
        ;

        
    
    
            var circle_marker_7d27b8ad573f41fea7a4905d4ea5dab2 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7f30e02780464e269739213c9784e06c = L.popup({"maxWidth": "100%"});

        
            var html_4be49720289c48388a6b7aec464d20dc = $(`<div id="html_4be49720289c48388a6b7aec464d20dc" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Zhao_161.pdf">ADSC submission for DCASE 2017: Acoustic scene classification using deep residual convolutional neural networks</a><br></div>`)[0];
            popup_7f30e02780464e269739213c9784e06c.setContent(html_4be49720289c48388a6b7aec464d20dc);
        

        circle_marker_7d27b8ad573f41fea7a4905d4ea5dab2.bindPopup(popup_7f30e02780464e269739213c9784e06c)
        ;

        
    
    
            var circle_marker_283e6d70acf9450989d8f1fcdee1960d = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_36626f41d70a4c40b3be4721c4828943 = L.popup({"maxWidth": "100%"});

        
            var html_5cc764587bb349828a6d967e3b128785 = $(`<div id="html_5cc764587bb349828a6d967e3b128785" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1811.06669">Aclnet: efficient end-to-end audio classification cnn</a><br></div>`)[0];
            popup_36626f41d70a4c40b3be4721c4828943.setContent(html_5cc764587bb349828a6d967e3b128785);
        

        circle_marker_283e6d70acf9450989d8f1fcdee1960d.bindPopup(popup_36626f41d70a4c40b3be4721c4828943)
        ;

        
    
    
            var circle_marker_9cc6f0d3b51d4f6db0e56735e674b2bc = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c673db36063c4d43b6af22918d99ea97 = L.popup({"maxWidth": "100%"});

        
            var html_2aa502cfad7548c9a332acaccd1a5fdc = $(`<div id="html_2aa502cfad7548c9a332acaccd1a5fdc" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.researchgate.net/profile/Rakib_Hyder/publication/319185344_Acoustic_Scene_Classification_Using_a_CNN-SuperVector_System_Trained_with_Auditory_and_Spectrogram_Image_Features/links/5a0210e84585155c96cb478d/Acoustic-Scene-Classification-Using-a-CNN-SuperVector-System-Trained-with-Auditory-and-Spectrogram-Image-Features.pdf">Acoustic Scene Classification Using a CNN-SuperVector System Trained with Auditory and Spectrogram Image Features.</a><br></div>`)[0];
            popup_c673db36063c4d43b6af22918d99ea97.setContent(html_2aa502cfad7548c9a332acaccd1a5fdc);
        

        circle_marker_9cc6f0d3b51d4f6db0e56735e674b2bc.bindPopup(popup_c673db36063c4d43b6af22918d99ea97)
        ;

        
    
    
            var circle_marker_a963cb861c36499b813a77b5c5df0ed6 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ab9d267794bc452a9d1bd23632e87f6d = L.popup({"maxWidth": "100%"});

        
            var html_76857c3b82324ec48db27ce1002dc8e6 = $(`<div id="html_76857c3b82324ec48db27ce1002dc8e6" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8897625/">Adaptive multi-scale detection of acoustic events</a><br></div>`)[0];
            popup_ab9d267794bc452a9d1bd23632e87f6d.setContent(html_76857c3b82324ec48db27ce1002dc8e6);
        

        circle_marker_a963cb861c36499b813a77b5c5df0ed6.bindPopup(popup_ab9d267794bc452a9d1bd23632e87f6d)
        ;

        
    
    
            var circle_marker_f1538cfbbd974f57a1721a965c93eb0e = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_08450cdeac2544179aa66b1202500201 = L.popup({"maxWidth": "100%"});

        
            var html_6dd7085d8c9741998dbc00fee14f685b = $(`<div id="html_6dd7085d8c9741998dbc00fee14f685b" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3331184.3331234">Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continuation</a><br></div>`)[0];
            popup_08450cdeac2544179aa66b1202500201.setContent(html_6dd7085d8c9741998dbc00fee14f685b);
        

        circle_marker_f1538cfbbd974f57a1721a965c93eb0e.bindPopup(popup_08450cdeac2544179aa66b1202500201)
        ;

        
    
    
            var circle_marker_d5a97abfbd5a441985c4dce2d93cd1c2 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1910d7b1e80c479d9d4ee139dda70df2 = L.popup({"maxWidth": "100%"});

        
            var html_909abaeea3334bf68ece17bceaf02062 = $(`<div id="html_909abaeea3334bf68ece17bceaf02062" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/2008.00107">An Acoustic Segment Model Based Segment Unit Selection Approach to Acoustic Scene Classification with Partial Utterances</a><br></div>`)[0];
            popup_1910d7b1e80c479d9d4ee139dda70df2.setContent(html_909abaeea3334bf68ece17bceaf02062);
        

        circle_marker_d5a97abfbd5a441985c4dce2d93cd1c2.bindPopup(popup_1910d7b1e80c479d9d4ee139dda70df2)
        ;

        
    
    
            var circle_marker_b44906eda40e4ee793180b19e8869cea = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6c7873c627784ce6b337d051bbbe11c2 = L.popup({"maxWidth": "100%"});

        
            var html_265b7e44b76b46959d0ab64ecee123ec = $(`<div id="html_265b7e44b76b46959d0ab64ecee123ec" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://tomcollinsresearch.net/pdf/dasEtAlSMC2018.pdf">Analyzing and Classifying Guitarists from Rock Guitar Solo Tablature</a><br></div>`)[0];
            popup_6c7873c627784ce6b337d051bbbe11c2.setContent(html_265b7e44b76b46959d0ab64ecee123ec);
        

        circle_marker_b44906eda40e4ee793180b19e8869cea.bindPopup(popup_6c7873c627784ce6b337d051bbbe11c2)
        ;

        
    
    
            var circle_marker_8d4e482b16724ba5b687961476914412 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4608e33090ef41f2972009b55f787109 = L.popup({"maxWidth": "100%"});

        
            var html_693c7544e5d9423cb7c8de41cccb12b9 = $(`<div id="html_693c7544e5d9423cb7c8de41cccb12b9" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0440.PDF">Attention Based CLDNNs for Short-Duration Acoustic Scene Classification.</a><br></div>`)[0];
            popup_4608e33090ef41f2972009b55f787109.setContent(html_693c7544e5d9423cb7c8de41cccb12b9);
        

        circle_marker_8d4e482b16724ba5b687961476914412.bindPopup(popup_4608e33090ef41f2972009b55f787109)
        ;

        
    
    
            var circle_marker_589f3eb539c9463196ec84236523a541 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5f2ae769a9964d049b2e5ed7461fb7ee = L.popup({"maxWidth": "100%"});

        
            var html_12ebec9886b447499209306b86c2b954 = $(`<div id="html_12ebec9886b447499209306b86c2b954" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1906.02975">Audio tagging with noisy labels and minimal supervision</a><br></div>`)[0];
            popup_5f2ae769a9964d049b2e5ed7461fb7ee.setContent(html_12ebec9886b447499209306b86c2b954);
        

        circle_marker_589f3eb539c9463196ec84236523a541.bindPopup(popup_5f2ae769a9964d049b2e5ed7461fb7ee)
        ;

        
    
    
            var circle_marker_8e3dcfb38f7d401e81b774126696423e = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ad540601047c49398f2dd46bcc29dbf3 = L.popup({"maxWidth": "100%"});

        
            var html_89f983331e8f4ae998a930cff7367731 = $(`<div id="html_89f983331e8f4ae998a930cff7367731" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://dcase.community/documents/challenge2017/technical_reports/DCASE2017_Hasan_167.pdf">Buet Bosch consortium (B2C) acoustic scene classification systems for DCASE 2017 challenge</a><br></div>`)[0];
            popup_ad540601047c49398f2dd46bcc29dbf3.setContent(html_89f983331e8f4ae998a930cff7367731);
        

        circle_marker_8e3dcfb38f7d401e81b774126696423e.bindPopup(popup_ad540601047c49398f2dd46bcc29dbf3)
        ;

        
    
    
            var circle_marker_e2cad14e89de4d6bb70959a6a0147140 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6c83e78d182141d2b35432fe9a592428 = L.popup({"maxWidth": "100%"});

        
            var html_5a08304608cd47d3a638b8c8b7164f60 = $(`<div id="html_5a08304608cd47d3a638b8c8b7164f60" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://jit.ndhu.edu.tw/article/view/2329">ChoseAmobile: A Web-based Recommendation System for Mobile Phone Products</a><br></div>`)[0];
            popup_6c83e78d182141d2b35432fe9a592428.setContent(html_5a08304608cd47d3a638b8c8b7164f60);
        

        circle_marker_e2cad14e89de4d6bb70959a6a0147140.bindPopup(popup_6c83e78d182141d2b35432fe9a592428)
        ;

        
    
    
            var circle_marker_96f645f063474000a784b6cb847270cf = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8aede30056dc4111b6921e45e8143c59 = L.popup({"maxWidth": "100%"});

        
            var html_ed471347d16c4574b15521976c06f5d9 = $(`<div id="html_ed471347d16c4574b15521976c06f5d9" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1901.06125">Cold-start playlist recommendation with multitask learning</a><br></div>`)[0];
            popup_8aede30056dc4111b6921e45e8143c59.setContent(html_ed471347d16c4574b15521976c06f5d9);
        

        circle_marker_96f645f063474000a784b6cb847270cf.bindPopup(popup_8aede30056dc4111b6921e45e8143c59)
        ;

        
    
    
            var circle_marker_ae50a6c585154ee9b6799eb355d6e45a = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4cb33df1ff0b46929f237313ab9b2a22 = L.popup({"maxWidth": "100%"});

        
            var html_63c3855bdf6b490bbc23aa738688dc2d = $(`<div id="html_63c3855bdf6b490bbc23aa738688dc2d" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7966291/">Convolutional gated recurrent neural network incorporating spatial features for audio tagging</a><br></div>`)[0];
            popup_4cb33df1ff0b46929f237313ab9b2a22.setContent(html_63c3855bdf6b490bbc23aa738688dc2d);
        

        circle_marker_ae50a6c585154ee9b6799eb355d6e45a.bindPopup(popup_4cb33df1ff0b46929f237313ab9b2a22)
        ;

        
    
    
            var circle_marker_982ec4b953204e8cb297676c9ab4071a = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_38b5562f436f4c8492ef3479880fb5ae = L.popup({"maxWidth": "100%"});

        
            var html_6ea6bfffd9734abd9a7630ed7411a435 = $(`<div id="html_6ea6bfffd9734abd9a7630ed7411a435" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://tel.archives-ouvertes.fr/tel-01559667/">Convolutional operators in the time-frequency domain</a><br></div>`)[0];
            popup_38b5562f436f4c8492ef3479880fb5ae.setContent(html_6ea6bfffd9734abd9a7630ed7411a435);
        

        circle_marker_982ec4b953204e8cb297676c9ab4071a.bindPopup(popup_38b5562f436f4c8492ef3479880fb5ae)
        ;

        
    
    
            var circle_marker_b37073b98bdf449eb94ebdb6a1afc035 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_40c8c4ca31674b13a42db27966575972 = L.popup({"maxWidth": "100%"});

        
            var html_68d5c91084574414bfb45d2d93ef7b68 = $(`<div id="html_68d5c91084574414bfb45d2d93ef7b68" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s13735-018-0154-2">Current challenges and visions in music recommender systems research</a><br></div>`)[0];
            popup_40c8c4ca31674b13a42db27966575972.setContent(html_68d5c91084574414bfb45d2d93ef7b68);
        

        circle_marker_b37073b98bdf449eb94ebdb6a1afc035.bindPopup(popup_40c8c4ca31674b13a42db27966575972)
        ;

        
    
    
            var circle_marker_bdf39e2d5246490b9d67ca2ba6ef7f9d = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7cddae75cb0844f7b732d4a3edf4b27d = L.popup({"maxWidth": "100%"});

        
            var html_e4fa6a56d4b346d08fba3f27a6ab5748 = $(`<div id="html_e4fa6a56d4b346d08fba3f27a6ab5748" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7927482/">DCAR: A discriminative and compact audio representation for audio processing</a><br></div>`)[0];
            popup_7cddae75cb0844f7b732d4a3edf4b27d.setContent(html_e4fa6a56d4b346d08fba3f27a6ab5748);
        

        circle_marker_bdf39e2d5246490b9d67ca2ba6ef7f9d.bindPopup(popup_7cddae75cb0844f7b732d4a3edf4b27d)
        ;

        
    
    
            var circle_marker_a889c9e7521e44f28c3c74574a2e8a02 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_54d7a4c885b04d69abebe3e3684bdc44 = L.popup({"maxWidth": "100%"});

        
            var html_a9b26c94e51b46b0867aac9bff49d379 = $(`<div id="html_a9b26c94e51b46b0867aac9bff49d379" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1808.00773">DCASE 2018 challenge surrey cross-task convolutional neural network baseline</a><br></div>`)[0];
            popup_54d7a4c885b04d69abebe3e3684bdc44.setContent(html_a9b26c94e51b46b0867aac9bff49d379);
        

        circle_marker_a889c9e7521e44f28c3c74574a2e8a02.bindPopup(popup_54d7a4c885b04d69abebe3e3684bdc44)
        ;

        
    
    
            var circle_marker_af888c57a470401fb788b32946201c80 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ebe4af79233f4260a94915c293688698 = L.popup({"maxWidth": "100%"});

        
            var html_5709b1056f494fbfb706db552aa7afca = $(`<div id="html_5709b1056f494fbfb706db552aa7afca" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Diverse_Image_Generation_via_Self-Conditioned_GANs_CVPR_2020_paper.html">Diverse Image Generation via Self-Conditioned GANs</a><br></div>`)[0];
            popup_ebe4af79233f4260a94915c293688698.setContent(html_5709b1056f494fbfb706db552aa7afca);
        

        circle_marker_af888c57a470401fb788b32946201c80.bindPopup(popup_ebe4af79233f4260a94915c293688698)
        ;

        
    
    
            var circle_marker_44b37da55004450cadaf7e66a7cb50a1 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f36664b43f3e4d5c8b6fa3929cf70bf0 = L.popup({"maxWidth": "100%"});

        
            var html_585900585b694709b20f629d490314f1 = $(`<div id="html_585900585b694709b20f629d490314f1" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683514/">Domain mismatch robust acoustic scene classification using channel information conversion</a><br></div>`)[0];
            popup_f36664b43f3e4d5c8b6fa3929cf70bf0.setContent(html_585900585b694709b20f629d490314f1);
        

        circle_marker_44b37da55004450cadaf7e66a7cb50a1.bindPopup(popup_f36664b43f3e4d5c8b6fa3929cf70bf0)
        ;

        
    
    
            var circle_marker_e363ee78794f4247b3e5727269bad6aa = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e47e34ee3b104deb8ebe11bab20e7eb6 = L.popup({"maxWidth": "100%"});

        
            var html_f1243adb2f52441ea7da8ba02abd875b = $(`<div id="html_f1243adb2f52441ea7da8ba02abd875b" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1807.09902">General-purpose tagging of freesound audio with audioset labels: Task description, dataset, and baseline</a><br></div>`)[0];
            popup_e47e34ee3b104deb8ebe11bab20e7eb6.setContent(html_f1243adb2f52441ea7da8ba02abd875b);
        

        circle_marker_e363ee78794f4247b3e5727269bad6aa.bindPopup(popup_e47e34ee3b104deb8ebe11bab20e7eb6)
        ;

        
    
    
            var circle_marker_a8362a5abcd8451fb0fbed422874fc32 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_90ed1408f49643e496a6b35826cee218 = L.popup({"maxWidth": "100%"});

        
            var html_3ce7e9cd41cd4d0eaae26f33181a7b15 = $(`<div id="html_3ce7e9cd41cd4d0eaae26f33181a7b15" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://archives.ismir.net/ismir2019/paper/000104.pdf">Generating Structured Drum Pattern Using Variational Autoencoder and Self-similarity Matrix.</a><br></div>`)[0];
            popup_90ed1408f49643e496a6b35826cee218.setContent(html_3ce7e9cd41cd4d0eaae26f33181a7b15);
        

        circle_marker_a8362a5abcd8451fb0fbed422874fc32.bindPopup(popup_90ed1408f49643e496a6b35826cee218)
        ;

        
    
    
            var circle_marker_0dd39a3911f044b2a715b0508e74c861 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0ec8b989e8344a5fad11bc4d2871e4d0 = L.popup({"maxWidth": "100%"});

        
            var html_683fe2899e254f1e82d3aee6d9c7efa8 = $(`<div id="html_683fe2899e254f1e82d3aee6d9c7efa8" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Mun_213.pdf">Generative adversarial network based acoustic scene training set augmentation and selection using SVM hyper-plane</a><br></div>`)[0];
            popup_0ec8b989e8344a5fad11bc4d2871e4d0.setContent(html_683fe2899e254f1e82d3aee6d9c7efa8);
        

        circle_marker_0dd39a3911f044b2a715b0508e74c861.bindPopup(popup_0ec8b989e8344a5fad11bc4d2871e4d0)
        ;

        
    
    
            var circle_marker_ea45a189626d4f6ea40b4976488881bb = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_094d9a41f37e4ada9914de83ae3bf845 = L.popup({"maxWidth": "100%"});

        
            var html_7cd314cc738b4fa38f06f90817d8f7ad = $(`<div id="html_7cd314cc738b4fa38f06f90817d8f7ad" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1808.05340">Genre-agnostic key classification with convolutional neural networks</a><br></div>`)[0];
            popup_094d9a41f37e4ada9914de83ae3bf845.setContent(html_7cd314cc738b4fa38f06f90817d8f7ad);
        

        circle_marker_ea45a189626d4f6ea40b4976488881bb.bindPopup(popup_094d9a41f37e4ada9914de83ae3bf845)
        ;

        
    
    
            var circle_marker_069bc7d1dc154b04898ff950baf7cdf3 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_50e8882dae9b4eb9b3e7632700166d42 = L.popup({"maxWidth": "100%"});

        
            var html_f97530343e8c482e934c1a08e587861e = $(`<div id="html_f97530343e8c482e934c1a08e587861e" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.researchgate.net/profile/Filip_Korzeniowski/publication/331276600_Harmonic_Analysis_of_Musical_Audio_using_Deep_Neural_Networks/links/5c6fcc2fa6fdcc471591c504/Harmonic-Analysis-of-Musical-Audio-using-Deep-Neural-Networks.pdf">Harmonic Analysis of Musical Audio using Deep Neural Networks</a><br></div>`)[0];
            popup_50e8882dae9b4eb9b3e7632700166d42.setContent(html_f97530343e8c482e934c1a08e587861e);
        

        circle_marker_069bc7d1dc154b04898ff950baf7cdf3.bindPopup(popup_50e8882dae9b4eb9b3e7632700166d42)
        ;

        
    
    
            var circle_marker_f74dfa28b9804d52839b2623dcd67037 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fa6e869f88e540f28bf67070675458a3 = L.popup({"maxWidth": "100%"});

        
            var html_88ac09715ea0440aa6a3d4d569625322 = $(`<div id="html_88ac09715ea0440aa6a3d4d569625322" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://ieomsociety.org/toronto2019/papers/361.pdf">Humans' Perceptions of Handwritten Digits Generated by a Generative Adversarial Network</a><br></div>`)[0];
            popup_fa6e869f88e540f28bf67070675458a3.setContent(html_88ac09715ea0440aa6a3d4d569625322);
        

        circle_marker_f74dfa28b9804d52839b2623dcd67037.bindPopup(popup_fa6e869f88e540f28bf67070675458a3)
        ;

        
    
    
            var circle_marker_7daef4b2e76a4430ab80907a6647592a = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_14214cfcace14adcbb9674663691e603 = L.popup({"maxWidth": "100%"});

        
            var html_b34fa33d022944e6b1d446884ee88f23 = $(`<div id="html_b34fa33d022944e6b1d446884ee88f23" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0957417419301794">Hybrid feature-based analysis of video's affective content using protagonist detection</a><br></div>`)[0];
            popup_14214cfcace14adcbb9674663691e603.setContent(html_b34fa33d022944e6b1d446884ee88f23);
        

        circle_marker_7daef4b2e76a4430ab80907a6647592a.bindPopup(popup_14214cfcace14adcbb9674663691e603)
        ;

        
    
    
            var circle_marker_b1171845070447d3a74bf0ecd7a68aed = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_aab68dd0f1c242638edbc6b3268ba8e0 = L.popup({"maxWidth": "100%"});

        
            var html_67ca3c7af36a4c9db56df2d65ee0fbf7 = $(`<div id="html_67ca3c7af36a4c9db56df2d65ee0fbf7" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Kothinti_79.pdf">INTEGRATED BOTTOM-UP AND TOP-DOWN INFERENCE FOR SOUND EVENT DETECTION Technical Report</a><br></div>`)[0];
            popup_aab68dd0f1c242638edbc6b3268ba8e0.setContent(html_67ca3c7af36a4c9db56df2d65ee0fbf7);
        

        circle_marker_b1171845070447d3a74bf0ecd7a68aed.bindPopup(popup_aab68dd0f1c242638edbc6b3268ba8e0)
        ;

        
    
    
            var circle_marker_c72322bf73a74825bbfb959434a2de3d = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a3e24bc96c554fc0a9137bbb4af7f21a = L.popup({"maxWidth": "100%"});

        
            var html_4383adaef57c4f43917b1d1d42595a3d = $(`<div id="html_4383adaef57c4f43917b1d1d42595a3d" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://journals.sagepub.com/doi/abs/10.1177/1475921720923147">If structure can exclaim: a novel robotic-assisted percussion method for spatial bolt-ball joint looseness detection</a><br></div>`)[0];
            popup_a3e24bc96c554fc0a9137bbb4af7f21a.setContent(html_4383adaef57c4f43917b1d1d42595a3d);
        

        circle_marker_c72322bf73a74825bbfb959434a2de3d.bindPopup(popup_a3e24bc96c554fc0a9137bbb4af7f21a)
        ;

        
    
    
            var circle_marker_796f39cd9f9b47e2899426322d2ad2b1 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7ca7f6090b2548408833cedfceea423f = L.popup({"maxWidth": "100%"});

        
            var html_d1d9b8e277b741f6994f4a000f607aa4 = $(`<div id="html_d1d9b8e277b741f6994f4a000f607aa4" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/2009.01759">Intra-Utterance Similarity Preserving Knowledge Distillation for Audio Tagging</a><br></div>`)[0];
            popup_7ca7f6090b2548408833cedfceea423f.setContent(html_d1d9b8e277b741f6994f4a000f607aa4);
        

        circle_marker_796f39cd9f9b47e2899426322d2ad2b1.bindPopup(popup_7ca7f6090b2548408833cedfceea423f)
        ;

        
    
    
            var circle_marker_f017f2be7bcb4995804b5d5419d9dfe6 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8ab9e1c07670435f9fe26fbfcd6ee704 = L.popup({"maxWidth": "100%"});

        
            var html_dac852cdbf394f979ded9eadc278120d = $(`<div id="html_dac852cdbf394f979ded9eadc278120d" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Kothinti_90.pdf">JOINT ACOUSTIC AND CLASS INFERENCE FOR WEAKLY SUPERVISED SOUND EVENT DETECTION Technical Report Sandeep Kothinti1, Keisuke Imoto2 …</a><br></div>`)[0];
            popup_8ab9e1c07670435f9fe26fbfcd6ee704.setContent(html_dac852cdbf394f979ded9eadc278120d);
        

        circle_marker_f017f2be7bcb4995804b5d5419d9dfe6.bindPopup(popup_8ab9e1c07670435f9fe26fbfcd6ee704)
        ;

        
    
    
            var circle_marker_582ca4438bdb4a1e80687ecbdfdbab7a = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c3185338f32544b19b610f92ceefaf07 = L.popup({"maxWidth": "100%"});

        
            var html_4e23fffdf6c04bf2aac954425a36cbe4 = $(`<div id="html_4e23fffdf6c04bf2aac954425a36cbe4" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682772/">Joint acoustic and class inference for weakly supervised sound event detection</a><br></div>`)[0];
            popup_c3185338f32544b19b610f92ceefaf07.setContent(html_4e23fffdf6c04bf2aac954425a36cbe4);
        

        circle_marker_582ca4438bdb4a1e80687ecbdfdbab7a.bindPopup(popup_c3185338f32544b19b610f92ceefaf07)
        ;

        
    
    
            var circle_marker_c45427c438f44dd6a60507a2b3185ed1 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d7434dd7469041d6b27a17a0b7f56c2f = L.popup({"maxWidth": "100%"});

        
            var html_e10da80108bc4f98ba18fb42c7e27bcf = $(`<div id="html_e10da80108bc4f98ba18fb42c7e27bcf" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ccrma.stanford.edu/~urinieto/MARL/publications/ISMIR2020_MoodPrediction.pdf">MOOD CLASSIFICATION USING LISTENING DATA</a><br></div>`)[0];
            popup_d7434dd7469041d6b27a17a0b7f56c2f.setContent(html_e10da80108bc4f98ba18fb42c7e27bcf);
        

        circle_marker_c45427c438f44dd6a60507a2b3185ed1.bindPopup(popup_d7434dd7469041d6b27a17a0b7f56c2f)
        ;

        
    
    
            var circle_marker_753629065bb54c4b904c6c26dc1f0477 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_555c73906755417f94b1b746f4a7ac03 = L.popup({"maxWidth": "100%"});

        
            var html_689dfd2d88f549d38e5820c1cd192c6a = $(`<div id="html_689dfd2d88f549d38e5820c1cd192c6a" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8925176/">Multi-scale convolutional recurrent neural network with ensemble method for weakly labeled sound event detection</a><br></div>`)[0];
            popup_555c73906755417f94b1b746f4a7ac03.setContent(html_689dfd2d88f549d38e5820c1cd192c6a);
        

        circle_marker_753629065bb54c4b904c6c26dc1f0477.bindPopup(popup_555c73906755417f94b1b746f4a7ac03)
        ;

        
    
    
            var circle_marker_672fb6f32266458a96c9084c7d763ffb = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_629ee600579743809be56df19d1a615b = L.popup({"maxWidth": "100%"});

        
            var html_81de0618eda24578b60d46ef08d5f67c = $(`<div id="html_81de0618eda24578b60d46ef08d5f67c" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://escholarship.org/uc/item/6mm160gq">Neural network based representation learning and modeling for speech and speaker recognition</a><br></div>`)[0];
            popup_629ee600579743809be56df19d1a615b.setContent(html_81de0618eda24578b60d46ef08d5f67c);
        

        circle_marker_672fb6f32266458a96c9084c7d763ffb.bindPopup(popup_629ee600579743809be56df19d1a615b)
        ;

        
    
    
            var circle_marker_48d576473b8f4763813bb16b57abe801 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f1a873d2ca274749b4c3a5d03174ee9b = L.popup({"maxWidth": "100%"});

        
            var html_e2e462a62c4e4162b3a470885ece1b42 = $(`<div id="html_e2e462a62c4e4162b3a470885ece1b42" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/2007.09284">Optimal Bayesian estimation of Gaussian mixtures with growing number of components</a><br></div>`)[0];
            popup_f1a873d2ca274749b4c3a5d03174ee9b.setContent(html_e2e462a62c4e4162b3a470885ece1b42);
        

        circle_marker_48d576473b8f4763813bb16b57abe801.bindPopup(popup_f1a873d2ca274749b4c3a5d03174ee9b)
        ;

        
    
    
            var circle_marker_bf8a98c1f174491aaf560e7f33dccf7f = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_47d036817dd344a8b61b3b4a14db6577 = L.popup({"maxWidth": "100%"});

        
            var html_bb895ee143ab4d35bc7c032bde75b30f = $(`<div id="html_bb895ee143ab4d35bc7c032bde75b30f" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.theses.fr/2017PSLEE012">Opérateurs convolutionnels dans le plan temps-fréquence</a><br></div>`)[0];
            popup_47d036817dd344a8b61b3b4a14db6577.setContent(html_bb895ee143ab4d35bc7c032bde75b30f);
        

        circle_marker_bf8a98c1f174491aaf560e7f33dccf7f.bindPopup(popup_47d036817dd344a8b61b3b4a14db6577)
        ;

        
    
    
            var circle_marker_d3a311588fc24260807de72100d62926 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6c4f6086e87e4e0fb2a692cac9ff3bd0 = L.popup({"maxWidth": "100%"});

        
            var html_37df208d379f48efbba591d24f244fbb = $(`<div id="html_37df208d379f48efbba591d24f244fbb" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="">Polyphonic sound event detection with weak labeling</a><br></div>`)[0];
            popup_6c4f6086e87e4e0fb2a692cac9ff3bd0.setContent(html_37df208d379f48efbba591d24f244fbb);
        

        circle_marker_d3a311588fc24260807de72100d62926.bindPopup(popup_6c4f6086e87e4e0fb2a692cac9ff3bd0)
        ;

        
    
    
            var circle_marker_b3f24efae900419cb285bc8186651a99 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b61102e5596448dea2dedfe2f24e5c3e = L.popup({"maxWidth": "100%"});

        
            var html_8da868a607434e89b68d8f137828c912 = $(`<div id="html_8da868a607434e89b68d8f137828c912" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1908.06708">Recommender Systems Fairness Evaluation via Generalized Cross Entropy</a><br></div>`)[0];
            popup_b61102e5596448dea2dedfe2f24e5c3e.setContent(html_8da868a607434e89b68d8f137828c912);
        

        circle_marker_b3f24efae900419cb285bc8186651a99.bindPopup(popup_b61102e5596448dea2dedfe2f24e5c3e)
        ;

        
    
    
            var circle_marker_e2cb767556874d2ab24e1dfbd3ba11fd = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f41033a8ca3e42d1b01fa20b7aadd2c6 = L.popup({"maxWidth": "100%"});

        
            var html_3f8cda44ffc348cb8b4547ae5b7738ab = $(`<div id="html_3f8cda44ffc348cb8b4547ae5b7738ab" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/2008.00110">Relational Teacher Student Learning with Neural Label Embedding for Device Adaptation in Acoustic Scene Classification</a><br></div>`)[0];
            popup_f41033a8ca3e42d1b01fa20b7aadd2c6.setContent(html_3f8cda44ffc348cb8b4547ae5b7738ab);
        

        circle_marker_e2cb767556874d2ab24e1dfbd3ba11fd.bindPopup(popup_f41033a8ca3e42d1b01fa20b7aadd2c6)
        ;

        
    
    
            var circle_marker_cfe93de264e148ec9ee253dc0d465a3c = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_587fdeb59052403c8e17498c1dca1b21 = L.popup({"maxWidth": "100%"});

        
            var html_9cc4cae5e42a4d0c9110a1c0ab2f338a = $(`<div id="html_9cc4cae5e42a4d0c9110a1c0ab2f338a" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://www.aclweb.org/anthology/2020.ecnlp-1.7/">Semi-Supervised Iterative Approach for Domain-Specific Complaint Detection in Social Media</a><br></div>`)[0];
            popup_587fdeb59052403c8e17498c1dca1b21.setContent(html_9cc4cae5e42a4d0c9110a1c0ab2f338a);
        

        circle_marker_cfe93de264e148ec9ee253dc0d465a3c.bindPopup(popup_587fdeb59052403c8e17498c1dca1b21)
        ;

        
    
    
            var circle_marker_3c0a75b9d9664b4b97d794d82d09e4db = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e80fc09af8754ba89eb2aa38a358584d = L.popup({"maxWidth": "100%"});

        
            var html_8877543106ff4af18b902f147ed90ed7 = $(`<div id="html_8877543106ff4af18b902f147ed90ed7" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-319-63450-0_13">Sound analysis in smart cities</a><br></div>`)[0];
            popup_e80fc09af8754ba89eb2aa38a358584d.setContent(html_8877543106ff4af18b902f147ed90ed7);
        

        circle_marker_3c0a75b9d9664b4b97d794d82d09e4db.bindPopup(popup_e80fc09af8754ba89eb2aa38a358584d)
        ;

        
    
    
            var circle_marker_0f1c0bbea847420da156580e5ce0407b = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ee94174860a343119b785eb0db77bfae = L.popup({"maxWidth": "100%"});

        
            var html_898343c72de847bea55bac2852ba771a = $(`<div id="html_898343c72de847bea55bac2852ba771a" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://openreview.net/forum?id=HkGv2NMTjQ">Sound event classification using ontology-based neural networks</a><br></div>`)[0];
            popup_ee94174860a343119b785eb0db77bfae.setContent(html_898343c72de847bea55bac2852ba771a);
        

        circle_marker_0f1c0bbea847420da156580e5ce0407b.bindPopup(popup_ee94174860a343119b785eb0db77bfae)
        ;

        
    
    
            var circle_marker_8886db56345f44319590141c9a4eb27a = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f032cf73e3c64cfd8e1f70308762d917 = L.popup({"maxWidth": "100%"});

        
            var html_66ca7bda7a634417b2ad39bc803470dc = $(`<div id="html_66ca7bda7a634417b2ad39bc803470dc" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60771">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</a><br></div>`)[0];
            popup_f032cf73e3c64cfd8e1f70308762d917.setContent(html_66ca7bda7a634417b2ad39bc803470dc);
        

        circle_marker_8886db56345f44319590141c9a4eb27a.bindPopup(popup_f032cf73e3c64cfd8e1f70308762d917)
        ;

        
    
    
            var circle_marker_938a705b44dd453f9c226857f005e186 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_011ac40929cf41b7af183149b0b89a67 = L.popup({"maxWidth": "100%"});

        
            var html_e040d7cccd7e49f6b47e84ddbf3d3b34 = $(`<div id="html_e040d7cccd7e49f6b47e84ddbf3d3b34" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9054478/">Sound event detection in synthetic domestic environments</a><br></div>`)[0];
            popup_011ac40929cf41b7af183149b0b89a67.setContent(html_e040d7cccd7e49f6b47e84ddbf3d3b34);
        

        circle_marker_938a705b44dd453f9c226857f005e186.bindPopup(popup_011ac40929cf41b7af183149b0b89a67)
        ;

        
    
    
            var circle_marker_a864b2ff21d54db2b7f5d8eeb55ef736 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1728aac414da401aa1d4adb44d4ad943 = L.popup({"maxWidth": "100%"});

        
            var html_5f1019bd6add4d65b6e4a7a7a88f64f2 = $(`<div id="html_5f1019bd6add4d65b6e4a7a7a88f64f2" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8673582/">Sound event detection in the DCASE 2017 challenge</a><br></div>`)[0];
            popup_1728aac414da401aa1d4adb44d4ad943.setContent(html_5f1019bd6add4d65b6e4a7a7a88f64f2);
        

        circle_marker_a864b2ff21d54db2b7f5d8eeb55ef736.bindPopup(popup_1728aac414da401aa1d4adb44d4ad943)
        ;

        
    
    
            var circle_marker_faee2077b47946b8bbb382107e3ed002 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7c1f4f91a685489b901255b70684d9be = L.popup({"maxWidth": "100%"});

        
            var html_3a6de89e400d4fc499ef78df0482a10c = $(`<div id="html_3a6de89e400d4fc499ef78df0482a10c" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9165887/">Sound event detection of weakly labelled data with CNN-transformer and automatic threshold optimization</a><br></div>`)[0];
            popup_7c1f4f91a685489b901255b70684d9be.setContent(html_3a6de89e400d4fc499ef78df0482a10c);
        

        circle_marker_faee2077b47946b8bbb382107e3ed002.bindPopup(popup_7c1f4f91a685489b901255b70684d9be)
        ;

        
    
    
            var circle_marker_070d2ca0f4e4446abad22da3150dc3d8 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_16193b0621684da980b35cc73b49bb01 = L.popup({"maxWidth": "100%"});

        
            var html_93e0c1bcb9aa49ddb255c0ea4486cebc = $(`<div id="html_93e0c1bcb9aa49ddb255c0ea4486cebc" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://arxiv.org/abs/1803.01164">The history began from alexnet: A comprehensive survey on deep learning approaches</a><br></div>`)[0];
            popup_16193b0621684da980b35cc73b49bb01.setContent(html_93e0c1bcb9aa49ddb255c0ea4486cebc);
        

        circle_marker_070d2ca0f4e4446abad22da3150dc3d8.bindPopup(popup_16193b0621684da980b35cc73b49bb01)
        ;

        
    
    
            var circle_marker_5ea7561d0be14ae68154960c42ccc2ab = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c362f27608494372ae8dcc03f3c6af59 = L.popup({"maxWidth": "100%"});

        
            var html_afe0a72567c84345a417ae39a3acc180 = $(`<div id="html_afe0a72567c84345a417ae39a3acc180" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="">Uncertainty aware multimodal activity recognition with Bayesian inference.</a><br></div>`)[0];
            popup_c362f27608494372ae8dcc03f3c6af59.setContent(html_afe0a72567c84345a417ae39a3acc180);
        

        circle_marker_5ea7561d0be14ae68154960c42ccc2ab.bindPopup(popup_c362f27608494372ae8dcc03f3c6af59)
        ;

        
    
    
            var circle_marker_801ca5e19f01407db764723ed9e07a7c = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1f44101cb3554cf1aac48ff1fad9747d = L.popup({"maxWidth": "100%"});

        
            var html_83a6f05780dd43b18a43d18619ef395b = $(`<div id="html_83a6f05780dd43b18a43d18619ef395b" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Subedar_Uncertainty-Aware_Audiovisual_Activity_Recognition_Using_Deep_Bayesian_Variational_Inference_ICCV_2019_paper.html">Uncertainty-aware audiovisual activity recognition using deep bayesian variational inference</a><br></div>`)[0];
            popup_1f44101cb3554cf1aac48ff1fad9747d.setContent(html_83a6f05780dd43b18a43d18619ef395b);
        

        circle_marker_801ca5e19f01407db764723ed9e07a7c.bindPopup(popup_1f44101cb3554cf1aac48ff1fad9747d)
        ;

        
    
    
            var circle_marker_f4ef37172cf84c0ba056c035b46c4341 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e195025c665b4bef8937a22ae8e8f020 = L.popup({"maxWidth": "100%"});

        
            var html_479fe50ec6c14c6ead95dac0ebc6e94c = $(`<div id="html_479fe50ec6c14c6ead95dac0ebc6e94c" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8937231/">Unsupervised adversarial domain adaptation based on the wasserstein distance for acoustic scene classification</a><br></div>`)[0];
            popup_e195025c665b4bef8937a22ae8e8f020.setContent(html_479fe50ec6c14c6ead95dac0ebc6e94c);
        

        circle_marker_f4ef37172cf84c0ba056c035b46c4341.bindPopup(popup_e195025c665b4bef8937a22ae8e8f020)
        ;

        
    
    
            var circle_marker_1bb3b3875ff34669864dbcf21707b1be = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5eab9730f0a740dbba0d7059dff3d517 = L.popup({"maxWidth": "100%"});

        
            var html_7679784aa1fd44f8bd74ec7ad64208d7 = $(`<div id="html_7679784aa1fd44f8bd74ec7ad64208d7" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7952190/">Very deep convolutional neural networks for raw waveforms</a><br></div>`)[0];
            popup_5eab9730f0a740dbba0d7059dff3d517.setContent(html_7679784aa1fd44f8bd74ec7ad64208d7);
        

        circle_marker_1bb3b3875ff34669864dbcf21707b1be.bindPopup(popup_5eab9730f0a740dbba0d7059dff3d517)
        ;

        
    
    
            var circle_marker_ea64143724c04c54b91458aaecc6364b = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_55e0529d8752418c8be82783b900198c = L.popup({"maxWidth": "100%"});

        
            var html_1bc78dd04b9f499ba2ed104202356aad = $(`<div id="html_1bc78dd04b9f499ba2ed104202356aad" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://deepai.org/publication/weakly-labelled-audioset-classification-with-attention-neural-networks">Weakly labelled audioset classification with attention neural networks</a><br></div>`)[0];
            popup_55e0529d8752418c8be82783b900198c.setContent(html_1bc78dd04b9f499ba2ed104202356aad);
        

        circle_marker_ea64143724c04c54b91458aaecc6364b.bindPopup(popup_55e0529d8752418c8be82783b900198c)
        ;

        
    
    
            var circle_marker_63b1d5da942d4289bcda9c1128dc0093 = L.circleMarker(
                [39.7837304, -100.4458825],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9995bac2d796441e953ad6bb5b5d2b7c = L.popup({"maxWidth": "100%"});

        
            var html_4ba442578a274a1889627dbc25304e0c = $(`<div id="html_4ba442578a274a1889627dbc25304e0c" style="width: 100.0%; height: 100.0%;">Country : US<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8777125/">Weakly labelled audioset tagging with attention neural networks</a><br></div>`)[0];
            popup_9995bac2d796441e953ad6bb5b5d2b7c.setContent(html_4ba442578a274a1889627dbc25304e0c);
        

        circle_marker_63b1d5da942d4289bcda9c1128dc0093.bindPopup(popup_9995bac2d796441e953ad6bb5b5d2b7c)
        ;

        
    
    
            var circle_marker_3b9c8c6280ed454c87c07a87dc0a87f3 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fc817027f89745f49575a1944da2f0d9 = L.popup({"maxWidth": "100%"});

        
            var html_40c4710bebfc42feadea624a5b055d48 = $(`<div id="html_40c4710bebfc42feadea624a5b055d48" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7966035/">A convolutional neural network approach for acoustic scene classification</a><br></div>`)[0];
            popup_fc817027f89745f49575a1944da2f0d9.setContent(html_40c4710bebfc42feadea624a5b055d48);
        

        circle_marker_3b9c8c6280ed454c87c07a87dc0a87f3.bindPopup(popup_fc817027f89745f49575a1944da2f0d9)
        ;

        
    
    
            var circle_marker_18700abfb9dc4eb684ed02911457b050 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_11fe37cb86104d3b91af3ea6531c96c7 = L.popup({"maxWidth": "100%"});

        
            var html_98fe575fe8044538b4553ad0b486be55 = $(`<div id="html_98fe575fe8044538b4553ad0b486be55" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="http://archive.nyu.edu/handle/2451/60751">Acoustic scene classification in DCASE 2019 challenge: Closed and open set classification and data mismatch setups</a><br></div>`)[0];
            popup_11fe37cb86104d3b91af3ea6531c96c7.setContent(html_98fe575fe8044538b4553ad0b486be55);
        

        circle_marker_18700abfb9dc4eb684ed02911457b050.bindPopup(popup_11fe37cb86104d3b91af3ea6531c96c7)
        ;

        
    
    
            var circle_marker_05a89e315f284148818fa2ea3d458170 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_816e5bca91a54570b2c9716dd7ab18d1 = L.popup({"maxWidth": "100%"});

        
            var html_1ba85e3036144ca3b46a610557d4d077 = $(`<div id="html_1ba85e3036144ca3b46a610557d4d077" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="http://dcase.community/documents/challenge2017/technical_reports/DCASE2017_Jallet_140.pdf">Acoustic scene classification using convolutional recurrent neural networks</a><br></div>`)[0];
            popup_816e5bca91a54570b2c9716dd7ab18d1.setContent(html_1ba85e3036144ca3b46a610557d4d077);
        

        circle_marker_05a89e315f284148818fa2ea3d458170.bindPopup(popup_816e5bca91a54570b2c9716dd7ab18d1)
        ;

        
    
    
            var circle_marker_bf08bca5292e444688890183b894102c = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7f4cdecb49e44482947c853ad5703e2c = L.popup({"maxWidth": "100%"});

        
            var html_8ad40e8b1ac74a24827664aa3ce49400 = $(`<div id="html_8ad40e8b1ac74a24827664aa3ce49400" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8521242/">Acoustic scene classification: an overview of DCASE 2017 challenge entries</a><br></div>`)[0];
            popup_7f4cdecb49e44482947c853ad5703e2c.setContent(html_8ad40e8b1ac74a24827664aa3ce49400);
        

        circle_marker_bf08bca5292e444688890183b894102c.bindPopup(popup_7f4cdecb49e44482947c853ad5703e2c)
        ;

        
    
    
            var circle_marker_421acd38998646d3b17634c94a357209 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a523faa588814b12a08c71d7eaafd548 = L.popup({"maxWidth": "100%"});

        
            var html_0f63cca5de7a4269a21ebf6eac2dd5f7 = $(`<div id="html_0f63cca5de7a4269a21ebf6eac2dd5f7" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://arxiv.org/abs/2002.05033">Active Learning for Sound Event Detection</a><br></div>`)[0];
            popup_a523faa588814b12a08c71d7eaafd548.setContent(html_0f63cca5de7a4269a21ebf6eac2dd5f7);
        

        circle_marker_421acd38998646d3b17634c94a357209.bindPopup(popup_a523faa588814b12a08c71d7eaafd548)
        ;

        
    
    
            var circle_marker_6b283a9f27ee4d3ab3655c0af0da9844 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3b2859d2e51448dabf6799450951f4bf = L.popup({"maxWidth": "100%"});

        
            var html_658714f15083466d9286598523d9182d = $(`<div id="html_658714f15083466d9286598523d9182d" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8170047/">Assessment of human and machine performance in acoustic scene classification: DCASE 2016 case study</a><br></div>`)[0];
            popup_3b2859d2e51448dabf6799450951f4bf.setContent(html_658714f15083466d9286598523d9182d);
        

        circle_marker_6b283a9f27ee4d3ab3655c0af0da9844.bindPopup(popup_3b2859d2e51448dabf6799450951f4bf)
        ;

        
    
    
            var circle_marker_468ed260a0c1473390d49daa1780fab3 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e343ce0669e74fbc8a22076cf5f5e873 = L.popup({"maxWidth": "100%"});

        
            var html_b1bc1427dd1f42c5999da31f9c0d9c37 = $(`<div id="html_b1bc1427dd1f42c5999da31f9c0d9c37" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8123864/">Detection and classification of acoustic scenes and events: Outcome of the DCASE 2016 challenge</a><br></div>`)[0];
            popup_e343ce0669e74fbc8a22076cf5f5e873.setContent(html_b1bc1427dd1f42c5999da31f9c0d9c37);
        

        circle_marker_468ed260a0c1473390d49daa1780fab3.bindPopup(popup_e343ce0669e74fbc8a22076cf5f5e873)
        ;

        
    
    
            var circle_marker_9201647a5f784e73a93fb980ab7859b9 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_904a4ad0c4734ee39fd313c6674a47f4 = L.popup({"maxWidth": "100%"});

        
            var html_107195aab1f048e0aa3cd5de45fcbb7c = $(`<div id="html_107195aab1f048e0aa3cd5de45fcbb7c" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9057638/">Glottal Source Information for Pathological Voice Detection</a><br></div>`)[0];
            popup_904a4ad0c4734ee39fd313c6674a47f4.setContent(html_107195aab1f048e0aa3cd5de45fcbb7c);
        

        circle_marker_9201647a5f784e73a93fb980ab7859b9.bindPopup(popup_904a4ad0c4734ee39fd313c6674a47f4)
        ;

        
    
    
            var circle_marker_6c4aa330e975403c91eba2d752daf2d4 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_626aa3c13bf4475e91365b69a1a2de3c = L.popup({"maxWidth": "100%"});

        
            var html_9c15a9f585694739a784ef3ac3558845 = $(`<div id="html_9c15a9f585694739a784ef3ac3558845" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8673582/">Sound event detection in the DCASE 2017 challenge</a><br></div>`)[0];
            popup_626aa3c13bf4475e91365b69a1a2de3c.setContent(html_9c15a9f585694739a784ef3ac3558845);
        

        circle_marker_6c4aa330e975403c91eba2d752daf2d4.bindPopup(popup_626aa3c13bf4475e91365b69a1a2de3c)
        ;

        
    
    
            var circle_marker_8c9c45d40b58475c86ca32a7b8208135 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fa83fb5304a84e3f8fb3a8af858541f3 = L.popup({"maxWidth": "100%"});

        
            var html_5144286870934754b8a21f16146b21ce = $(`<div id="html_5144286870934754b8a21f16146b21ce" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682858/">Sound event envelope estimation in polyphonic mixtures</a><br></div>`)[0];
            popup_fa83fb5304a84e3f8fb3a8af858541f3.setContent(html_5144286870934754b8a21f16146b21ce);
        

        circle_marker_8c9c45d40b58475c86ca32a7b8208135.bindPopup(popup_fa83fb5304a84e3f8fb3a8af858541f3)
        ;

        
    
    
            var circle_marker_49fb94c861ba418a838a6437e2343289 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a1e519676ea749359f776b35f803e9e6 = L.popup({"maxWidth": "100%"});

        
            var html_4a173bf157894788b39fb7ec9cc9fb0c = $(`<div id="html_4a173bf157894788b39fb7ec9cc9fb0c" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8937231/">Unsupervised adversarial domain adaptation based on the wasserstein distance for acoustic scene classification</a><br></div>`)[0];
            popup_a1e519676ea749359f776b35f803e9e6.setContent(html_4a173bf157894788b39fb7ec9cc9fb0c);
        

        circle_marker_49fb94c861ba418a838a6437e2343289.bindPopup(popup_a1e519676ea749359f776b35f803e9e6)
        ;

        
    
    
            var circle_marker_f986fcb2917a46b599bbdb587122b840 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8aaabc980e574bc19c03bf5b8d498354 = L.popup({"maxWidth": "100%"});

        
            var html_3979cd5e0e9c44568e517b26c8184585 = $(`<div id="html_3979cd5e0e9c44568e517b26c8184585" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://arxiv.org/abs/1808.05777">Unsupervised adversarial domain adaptation for acoustic scene classification</a><br></div>`)[0];
            popup_8aaabc980e574bc19c03bf5b8d498354.setContent(html_3979cd5e0e9c44568e517b26c8184585);
        

        circle_marker_f986fcb2917a46b599bbdb587122b840.bindPopup(popup_8aaabc980e574bc19c03bf5b8d498354)
        ;

        
    
    
            var circle_marker_f3d2f6b8e6b9404593e9f333c748d3c4 = L.circleMarker(
                [63.2467777, 25.9209164],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_972a20d8be2a4b6eb4ac2ee90df288a9 = L.popup({"maxWidth": "100%"});

        
            var html_2174616310314fe3aa61acbf72d722c0 = $(`<div id="html_2174616310314fe3aa61acbf72d722c0" style="width: 100.0%; height: 100.0%;">Country : Finland<br>                         Paper : <a href="https://opus.bibliothek.uni-augsburg.de/opus4/files/45063/DCASE_2017+-+Sequence.pdf">Workshop (DCASE2017)</a><br></div>`)[0];
            popup_972a20d8be2a4b6eb4ac2ee90df288a9.setContent(html_2174616310314fe3aa61acbf72d722c0);
        

        circle_marker_f3d2f6b8e6b9404593e9f333c748d3c4.bindPopup(popup_972a20d8be2a4b6eb4ac2ee90df288a9)
        ;

        
    
    
            var circle_marker_7e620bb4856f4baaafb582ca261f43b6 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f44b08834c0b49628bbb93c51b8f87c5 = L.popup({"maxWidth": "100%"});

        
            var html_6f0873c8a5c9488b890589ee6284f00d = $(`<div id="html_6f0873c8a5c9488b890589ee6284f00d" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-981-13-8707-4_8">A Comparison of Attention Mechanisms of Convolutional Neural Network in Weakly Labeled Audio Tagging</a><br></div>`)[0];
            popup_f44b08834c0b49628bbb93c51b8f87c5.setContent(html_6f0873c8a5c9488b890589ee6284f00d);
        

        circle_marker_7e620bb4856f4baaafb582ca261f43b6.bindPopup(popup_f44b08834c0b49628bbb93c51b8f87c5)
        ;

        
    
    
            var circle_marker_1acf7b7523034f4084e1cb1525ae0930 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_07c4de2ac3d04f04b4d953cf027df7f6 = L.popup({"maxWidth": "100%"});

        
            var html_b1e37a19d8944b67978082be2545fd9e = $(`<div id="html_b1e37a19d8944b67978082be2545fd9e" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.researchgate.net/profile/Lam_Pham4/publication/335829391_A_Robust_Framework_for_Acoustic_Scene_Classification/links/5dc03975a6fdcc2128011ee7/A-Robust-Framework-for-Acoustic-Scene-Classification.pdf">A Robust Framework for Acoustic Scene Classification.</a><br></div>`)[0];
            popup_07c4de2ac3d04f04b4d953cf027df7f6.setContent(html_b1e37a19d8944b67978082be2545fd9e);
        

        circle_marker_1acf7b7523034f4084e1cb1525ae0930.bindPopup(popup_07c4de2ac3d04f04b4d953cf027df7f6)
        ;

        
    
    
            var circle_marker_a8cb2a79ab2d47cabb6c9a6855c013f2 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c90164c9ae864b5093801f60b12a029f = L.popup({"maxWidth": "100%"});

        
            var html_88c44df23bc447e59bdb0fbe891825dd = $(`<div id="html_88c44df23bc447e59bdb0fbe891825dd" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9052995/">A framework for the robust evaluation of sound event detection</a><br></div>`)[0];
            popup_c90164c9ae864b5093801f60b12a029f.setContent(html_88c44df23bc447e59bdb0fbe891825dd);
        

        circle_marker_a8cb2a79ab2d47cabb6c9a6855c013f2.bindPopup(popup_c90164c9ae864b5093801f60b12a029f)
        ;

        
    
    
            var circle_marker_53f241c424084720a7a8f85bc7948b31 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e0081db5a412469d9e45bda93f2935bf = L.popup({"maxWidth": "100%"});

        
            var html_79ff07d1cf7747388174e906c0db3d8d = $(`<div id="html_79ff07d1cf7747388174e906c0db3d8d" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553225/">A fusion of deep convolutional generative adversarial networks and sequence to sequence autoencoders for acoustic scene classification</a><br></div>`)[0];
            popup_e0081db5a412469d9e45bda93f2935bf.setContent(html_79ff07d1cf7747388174e906c0db3d8d);
        

        circle_marker_53f241c424084720a7a8f85bc7948b31.bindPopup(popup_e0081db5a412469d9e45bda93f2935bf)
        ;

        
    
    
            var circle_marker_2da13d2798884b6e90c2580e5603707f = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3dc7737fc5f04aa7bff0d8fbcf64dc7a = L.popup({"maxWidth": "100%"});

        
            var html_67cfb010f2d04e4388364de6ed27a6ac = $(`<div id="html_67cfb010f2d04e4388364de6ed27a6ac" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1481.pdf">ASe: Acoustic Scene Embedding Using Deep Archetypal Analysis and GMM.</a><br></div>`)[0];
            popup_3dc7737fc5f04aa7bff0d8fbcf64dc7a.setContent(html_67cfb010f2d04e4388364de6ed27a6ac);
        

        circle_marker_2da13d2798884b6e90c2580e5603707f.bindPopup(popup_3dc7737fc5f04aa7bff0d8fbcf64dc7a)
        ;

        
    
    
            var circle_marker_d88325f759174a448972ef0a61895c34 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_86f779c84215422aa2b068600f6d7915 = L.popup({"maxWidth": "100%"});

        
            var html_028d58db9a0d43babf62c5ae169fc2c5 = $(`<div id="html_028d58db9a0d43babf62c5ae169fc2c5" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/2007.12391">Artificial Intelligence in the Creative Industries: A Review</a><br></div>`)[0];
            popup_86f779c84215422aa2b068600f6d7915.setContent(html_028d58db9a0d43babf62c5ae169fc2c5);
        

        circle_marker_d88325f759174a448972ef0a61895c34.bindPopup(popup_86f779c84215422aa2b068600f6d7915)
        ;

        
    
    
            var circle_marker_c51ab4738af740b8894f19530a881145 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1f669101627e46d7806ffdeff0c34b10 = L.popup({"maxWidth": "100%"});

        
            var html_bfeccd62abff4772961dd3ccdcc0e54e = $(`<div id="html_bfeccd62abff4772961dd3ccdcc0e54e" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/1703.04770">Audio scene classification with deep recurrent neural networks</a><br></div>`)[0];
            popup_1f669101627e46d7806ffdeff0c34b10.setContent(html_bfeccd62abff4772961dd3ccdcc0e54e);
        

        circle_marker_c51ab4738af740b8894f19530a881145.bindPopup(popup_1f669101627e46d7806ffdeff0c34b10)
        ;

        
    
    
            var circle_marker_8ba828daeeb5459ab513f322c9c20771 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_95fd4d900f2e4a9e93561b84403f36ed = L.popup({"maxWidth": "100%"});

        
            var html_a3b997ef378e456f9420afd0d290139d = $(`<div id="html_a3b997ef378e456f9420afd0d290139d" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/1811.01095">Beyond equal-length snippets: How long is sufficient to recognize an audio scene?</a><br></div>`)[0];
            popup_95fd4d900f2e4a9e93561b84403f36ed.setContent(html_a3b997ef378e456f9420afd0d290139d);
        

        circle_marker_8ba828daeeb5459ab513f322c9c20771.bindPopup(popup_95fd4d900f2e4a9e93561b84403f36ed)
        ;

        
    
    
            var circle_marker_6fe9623fe0624a7a834c42cccde43a75 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_29af849aca6748a3ba7a46bdb7ea8f3d = L.popup({"maxWidth": "100%"});

        
            var html_419814af165b49fdb84822a47017de92 = $(`<div id="html_419814af165b49fdb84822a47017de92" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7966291/">Convolutional gated recurrent neural network incorporating spatial features for audio tagging</a><br></div>`)[0];
            popup_29af849aca6748a3ba7a46bdb7ea8f3d.setContent(html_419814af165b49fdb84822a47017de92);
        

        circle_marker_6fe9623fe0624a7a834c42cccde43a75.bindPopup(popup_29af849aca6748a3ba7a46bdb7ea8f3d)
        ;

        
    
    
            var circle_marker_0fa4d2bdbeaa46b18e6ce76befa7f54d = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_10c242fd9b4e4b549a69b295c03c37bf = L.popup({"maxWidth": "100%"});

        
            var html_bd301ddb12544d0997f447e5e4d7ccf7 = $(`<div id="html_bd301ddb12544d0997f447e5e4d7ccf7" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/1808.00773">DCASE 2018 challenge surrey cross-task convolutional neural network baseline</a><br></div>`)[0];
            popup_10c242fd9b4e4b549a69b295c03c37bf.setContent(html_bd301ddb12544d0997f447e5e4d7ccf7);
        

        circle_marker_0fa4d2bdbeaa46b18e6ce76befa7f54d.bindPopup(popup_10c242fd9b4e4b549a69b295c03c37bf)
        ;

        
    
    
            var circle_marker_7065194e87304bba8eda274d72f4a427 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d379c35fd5e24cbeb7e6fe3e30c689a0 = L.popup({"maxWidth": "100%"});

        
            var html_909b9829204f4a449bcaebf13962414e = $(`<div id="html_909b9829204f4a449bcaebf13962414e" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/2007.12864">DD-CNN: Depthwise Disout Convolutional Neural Network for Low-complexity Acoustic Scene Classification</a><br></div>`)[0];
            popup_d379c35fd5e24cbeb7e6fe3e30c689a0.setContent(html_909b9829204f4a449bcaebf13962414e);
        

        circle_marker_7065194e87304bba8eda274d72f4a427.bindPopup(popup_d379c35fd5e24cbeb7e6fe3e30c689a0)
        ;

        
    
    
            var circle_marker_3fab402793cd43048a944a5dc873fd9b = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a89dbd70ce884dbaa3b7219fcd8984bd = L.popup({"maxWidth": "100%"});

        
            var html_068b2e0e0cdb4fd196cf61de518e6726 = $(`<div id="html_068b2e0e0cdb4fd196cf61de518e6726" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8123864/">Detection and classification of acoustic scenes and events: Outcome of the DCASE 2016 challenge</a><br></div>`)[0];
            popup_a89dbd70ce884dbaa3b7219fcd8984bd.setContent(html_068b2e0e0cdb4fd196cf61de518e6726);
        

        circle_marker_3fab402793cd43048a944a5dc873fd9b.bindPopup(popup_a89dbd70ce884dbaa3b7219fcd8984bd)
        ;

        
    
    
            var circle_marker_881eda183b2a402db225e9a3021098e0 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cb8babe955ad4de4a18f2f2017702997 = L.popup({"maxWidth": "100%"});

        
            var html_2c012a05bd3a46aeaf7d1feb825a9876 = $(`<div id="html_2c012a05bd3a46aeaf7d1feb825a9876" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.mdpi.com/239784">EigenScape: A database of spatial acoustic scene recordings</a><br></div>`)[0];
            popup_cb8babe955ad4de4a18f2f2017702997.setContent(html_2c012a05bd3a46aeaf7d1feb825a9876);
        

        circle_marker_881eda183b2a402db225e9a3021098e0.bindPopup(popup_cb8babe955ad4de4a18f2f2017702997)
        ;

        
    
    
            var circle_marker_ac36be77f9dd4e07af615fd6923898eb = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0f74cb524f914279bfcd70970c0986a9 = L.popup({"maxWidth": "100%"});

        
            var html_72783dfa88934bf3965a2715e4930128 = $(`<div id="html_72783dfa88934bf3965a2715e4930128" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="http://web.pkusz.edu.cn/adsp/files/2020/08/Interspeech2020_%E7%8E%8B%E8%B5%AB%E9%BA%9F_Environmental-Sound-Classification-with-Parallel-Temporal-spectral-Attention.pdf">Environmental sound classification with parallel temporal-spectral attention</a><br></div>`)[0];
            popup_0f74cb524f914279bfcd70970c0986a9.setContent(html_72783dfa88934bf3965a2715e4930128);
        

        circle_marker_ac36be77f9dd4e07af615fd6923898eb.bindPopup(popup_0f74cb524f914279bfcd70970c0986a9)
        ;

        
    
    
            var circle_marker_b2848851461f405a9e9b910e9169b2fe = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bb3062f66864499d95cdd26672dab37f = L.popup({"maxWidth": "100%"});

        
            var html_0e4cc7b088674b1e884cc8a06bad32c0 = $(`<div id="html_0e4cc7b088674b1e884cc8a06bad32c0" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://asa.scitation.org/doi/abs/10.1121/1.5111059">General audio tagging with ensembling convolutional neural networks and statistical features</a><br></div>`)[0];
            popup_bb3062f66864499d95cdd26672dab37f.setContent(html_0e4cc7b088674b1e884cc8a06bad32c0);
        

        circle_marker_b2848851461f405a9e9b910e9169b2fe.bindPopup(popup_bb3062f66864499d95cdd26672dab37f)
        ;

        
    
    
            var circle_marker_bfc500b7b0f4438fb745544ecc3af2f8 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_36e0dea44ead44bca086dcd45f5898fa = L.popup({"maxWidth": "100%"});

        
            var html_8ae37695d47c45a0acc3d884030324f1 = $(`<div id="html_8ae37695d47c45a0acc3d884030324f1" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="http://epubs.surrey.ac.uk/849923/">General-purpose audio tagging from noisy labels using convolutional neural networks</a><br></div>`)[0];
            popup_36e0dea44ead44bca086dcd45f5898fa.setContent(html_8ae37695d47c45a0acc3d884030324f1);
        

        circle_marker_bfc500b7b0f4438fb745544ecc3af2f8.bindPopup(popup_36e0dea44ead44bca086dcd45f5898fa)
        ;

        
    
    
            var circle_marker_cf99570b589143faabd3cb82ef421c5d = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_66ef0d1660504bc095dbc09e7ce4534d = L.popup({"maxWidth": "100%"});

        
            var html_db08e4e70d9042caa3cde909373e8127 = $(`<div id="html_db08e4e70d9042caa3cde909373e8127" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933052/">Improved audio scene classification based on label-tree embeddings and convolutional neural networks</a><br></div>`)[0];
            popup_66ef0d1660504bc095dbc09e7ce4534d.setContent(html_db08e4e70d9042caa3cde909373e8127);
        

        circle_marker_cf99570b589143faabd3cb82ef421c5d.bindPopup(popup_66ef0d1660504bc095dbc09e7ce4534d)
        ;

        
    
    
            var circle_marker_719ec87a6d694d04b9fcad8dbe1002ec = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_86951ac06e08452c8efa1237052c2320 = L.popup({"maxWidth": "100%"});

        
            var html_5df4fbc044874efe94aeebd273bd858c = $(`<div id="html_5df4fbc044874efe94aeebd273bd858c" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9115871/">Learning Hierarchy Aware Embedding From Raw Audio for Acoustic Scene Classification</a><br></div>`)[0];
            popup_86951ac06e08452c8efa1237052c2320.setContent(html_5df4fbc044874efe94aeebd273bd858c);
        

        circle_marker_719ec87a6d694d04b9fcad8dbe1002ec.bindPopup(popup_86951ac06e08452c8efa1237052c2320)
        ;

        
    
    
            var circle_marker_e98a17b9e1d74f6696dbe95a9e69538b = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_576b0e4583a345c98bd883c816d2790a = L.popup({"maxWidth": "100%"});

        
            var html_655afab96b83471fb9c229a519b19028 = $(`<div id="html_655afab96b83471fb9c229a519b19028" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="">Linking open public datasets for multifaceted music discovery</a><br></div>`)[0];
            popup_576b0e4583a345c98bd883c816d2790a.setContent(html_655afab96b83471fb9c229a519b19028);
        

        circle_marker_e98a17b9e1d74f6696dbe95a9e69538b.bindPopup(popup_576b0e4583a345c98bd883c816d2790a)
        ;

        
    
    
            var circle_marker_0c347c9aa92747d884c247530fb561b7 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7a9262633b904561999067b7f1931165 = L.popup({"maxWidth": "100%"});

        
            var html_4c937ca64eb14d16b4450a3172de1afc = $(`<div id="html_4c937ca64eb14d16b4450a3172de1afc" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/1707.04678">Lyrics-based music genre classification using a hierarchical attention network</a><br></div>`)[0];
            popup_7a9262633b904561999067b7f1931165.setContent(html_4c937ca64eb14d16b4450a3172de1afc);
        

        circle_marker_0c347c9aa92747d884c247530fb561b7.bindPopup(popup_7a9262633b904561999067b7f1931165)
        ;

        
    
    
            var circle_marker_78c33fd0300d4602832b1fa602d3b0dc = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2459faddaf364b4994c456f44119a1a5 = L.popup({"maxWidth": "100%"});

        
            var html_15a1e8e1050843ffa7fbf513d584112a = $(`<div id="html_15a1e8e1050843ffa7fbf513d584112a" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3356590.3356611">Modelling Musical Similarity for Drum Patterns: A Perceptual Evaluation</a><br></div>`)[0];
            popup_2459faddaf364b4994c456f44119a1a5.setContent(html_15a1e8e1050843ffa7fbf513d584112a);
        

        circle_marker_78c33fd0300d4602832b1fa602d3b0dc.bindPopup(popup_2459faddaf364b4994c456f44119a1a5)
        ;

        
    
    
            var circle_marker_a8b8bda43c7742e8af074470edc32683 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_525a7a054c7945ee8f4a114150e91be2 = L.popup({"maxWidth": "100%"});

        
            var html_5d1e49dd865046d9b61998791eb40c4a = $(`<div id="html_5d1e49dd865046d9b61998791eb40c4a" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.jstage.jst.go.jp/article/transinf/E102.D/10/E102.D_2019EDL8062/_article/-char/ja/">Multi Model-Based Distillation for Sound Event Detection</a><br></div>`)[0];
            popup_525a7a054c7945ee8f4a114150e91be2.setContent(html_5d1e49dd865046d9b61998791eb40c4a);
        

        circle_marker_a8b8bda43c7742e8af074470edc32683.bindPopup(popup_525a7a054c7945ee8f4a114150e91be2)
        ;

        
    
    
            var circle_marker_4c0ddc6fa2ba485b9288a7d6890ed93d = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d6b085aef3814d40aac12ae979c47213 = L.popup({"maxWidth": "100%"});

        
            var html_0da68e0f1a074780940592a2daeb71fc = $(`<div id="html_0da68e0f1a074780940592a2daeb71fc" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/2002.04502">Robust Acoustic Scene Classification using a Multi-Spectrogram Encoder-Decoder Framework</a><br></div>`)[0];
            popup_d6b085aef3814d40aac12ae979c47213.setContent(html_0da68e0f1a074780940592a2daeb71fc);
        

        circle_marker_4c0ddc6fa2ba485b9288a7d6890ed93d.bindPopup(popup_d6b085aef3814d40aac12ae979c47213)
        ;

        
    
    
            var circle_marker_fb676571345c4c0585eb7529f8a49c76 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_01da12b9b06e450e9e3fc9b2e8e457aa = L.popup({"maxWidth": "100%"});

        
            var html_1d557af7aceb4b16bf0260286f4643bc = $(`<div id="html_1d557af7aceb4b16bf0260286f4643bc" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Amiriparian_173.pdf">Sequence to sequence autoencoders for unsupervised representation learning from audio</a><br></div>`)[0];
            popup_01da12b9b06e450e9e3fc9b2e8e457aa.setContent(html_1d557af7aceb4b16bf0260286f4643bc);
        

        circle_marker_fb676571345c4c0585eb7529f8a49c76.bindPopup(popup_01da12b9b06e450e9e3fc9b2e8e457aa)
        ;

        
    
    
            var circle_marker_1eda0b460f0f4befa435eac50fe2b8c7 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9d760ea0232b4bbfaadd615ff9c0c20d = L.popup({"maxWidth": "100%"});

        
            var html_55182fccb69e48658876b9f7c9de1a69 = $(`<div id="html_55182fccb69e48658876b9f7c9de1a69" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://www.academia.edu/download/56171528/Amiriparian17-SSC.PDF">Snore Sound Classification Using Image-Based Deep Spectrum Features.</a><br></div>`)[0];
            popup_9d760ea0232b4bbfaadd615ff9c0c20d.setContent(html_55182fccb69e48658876b9f7c9de1a69);
        

        circle_marker_1eda0b460f0f4befa435eac50fe2b8c7.bindPopup(popup_9d760ea0232b4bbfaadd615ff9c0c20d)
        ;

        
    
    
            var circle_marker_3db5d0311b784fcd8ed97743dd513e6a = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4169cddda5e6433b910f03102be019c8 = L.popup({"maxWidth": "100%"});

        
            var html_f2ce82a190b14317bbe0c1aae2bb5f91 = $(`<div id="html_f2ce82a190b14317bbe0c1aae2bb5f91" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://arxiv.org/abs/2005.12779">Sound Context Classification Basing on Join Learning Model and Multi-Spectrogram Features</a><br></div>`)[0];
            popup_4169cddda5e6433b910f03102be019c8.setContent(html_f2ce82a190b14317bbe0c1aae2bb5f91);
        

        circle_marker_3db5d0311b784fcd8ed97743dd513e6a.bindPopup(popup_4169cddda5e6433b910f03102be019c8)
        ;

        
    
    
            var circle_marker_a11d486c7bdd43e0affa195ca8b947ff = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2c478d6f6a764e739a20d8bf4a1c5863 = L.popup({"maxWidth": "100%"});

        
            var html_6202a1d0d2e84562a349557737b4ded3 = $(`<div id="html_6202a1d0d2e84562a349557737b4ded3" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9165887/">Sound event detection of weakly labelled data with CNN-transformer and automatic threshold optimization</a><br></div>`)[0];
            popup_2c478d6f6a764e739a20d8bf4a1c5863.setContent(html_6202a1d0d2e84562a349557737b4ded3);
        

        circle_marker_a11d486c7bdd43e0affa195ca8b947ff.bindPopup(popup_2c478d6f6a764e739a20d8bf4a1c5863)
        ;

        
    
    
            var circle_marker_070bf80548fd492a978540923d20a31d = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_28a2d3374bd84b25a4ab320fbcbbb6d6 = L.popup({"maxWidth": "100%"});

        
            var html_4c819df9690446a9b10b33cabddc0345 = $(`<div id="html_4c819df9690446a9b10b33cabddc0345" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="http://epubs.surrey.ac.uk/853328/">Sound event detection with weakly labelled data</a><br></div>`)[0];
            popup_28a2d3374bd84b25a4ab320fbcbbb6d6.setContent(html_4c819df9690446a9b10b33cabddc0345);
        

        circle_marker_070bf80548fd492a978540923d20a31d.bindPopup(popup_28a2d3374bd84b25a4ab320fbcbbb6d6)
        ;

        
    
    
            var circle_marker_d24bea981ed24644b0c5ac1a0d51030d = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_21a65956d6ff4192b609152eaef10091 = L.popup({"maxWidth": "100%"});

        
            var html_1d3f6245cc524310872498c340c97556 = $(`<div id="html_1d3f6245cc524310872498c340c97556" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8492416/">Transfer learning for wearable long-term social speech evaluations</a><br></div>`)[0];
            popup_21a65956d6ff4192b609152eaef10091.setContent(html_1d3f6245cc524310872498c340c97556);
        

        circle_marker_d24bea981ed24644b0c5ac1a0d51030d.bindPopup(popup_21a65956d6ff4192b609152eaef10091)
        ;

        
    
    
            var circle_marker_8717bffe628b480ab61941734074e923 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5b6c1ff1a43a4298b9c8c7168274c7c9 = L.popup({"maxWidth": "100%"});

        
            var html_8629817a454440a7acdbb29f733089e1 = $(`<div id="html_8629817a454440a7acdbb29f733089e1" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://deepai.org/publication/weakly-labelled-audioset-classification-with-attention-neural-networks">Weakly labelled audioset classification with attention neural networks</a><br></div>`)[0];
            popup_5b6c1ff1a43a4298b9c8c7168274c7c9.setContent(html_8629817a454440a7acdbb29f733089e1);
        

        circle_marker_8717bffe628b480ab61941734074e923.bindPopup(popup_5b6c1ff1a43a4298b9c8c7168274c7c9)
        ;

        
    
    
            var circle_marker_d8ac24e5f2724c9ab39a7d5990504408 = L.circleMarker(
                [54.7023545, -3.2765753],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c871ab2f15e74e73887869e13304a587 = L.popup({"maxWidth": "100%"});

        
            var html_415b2327ec954052a670af4d9c757686 = $(`<div id="html_415b2327ec954052a670af4d9c757686" style="width: 100.0%; height: 100.0%;">Country : UK<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8777125/">Weakly labelled audioset tagging with attention neural networks</a><br></div>`)[0];
            popup_c871ab2f15e74e73887869e13304a587.setContent(html_415b2327ec954052a670af4d9c757686);
        

        circle_marker_d8ac24e5f2724c9ab39a7d5990504408.bindPopup(popup_c871ab2f15e74e73887869e13304a587)
        ;

        
    
    
            var circle_marker_aac43dbb546f443aa3f8f34389799104 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5488495cd76b4e46b37e55f24aec5361 = L.popup({"maxWidth": "100%"});

        
            var html_1e2e337e049e454cb7269907c3d86f0e = $(`<div id="html_1e2e337e049e454cb7269907c3d86f0e" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1807.09208">A hybrid of deep audio feature and i-vector for artist recognition</a><br></div>`)[0];
            popup_5488495cd76b4e46b37e55f24aec5361.setContent(html_1e2e337e049e454cb7269907c3d86f0e);
        

        circle_marker_aac43dbb546f443aa3f8f34389799104.bindPopup(popup_5488495cd76b4e46b37e55f24aec5361)
        ;

        
    
    
            var circle_marker_2d119b3cff93416bb2d38c08cf19feaa = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b96027520a3545979912538b6a9065b0 = L.popup({"maxWidth": "100%"});

        
            var html_30b1bc5d11e74feeb399b800682bdb94 = $(`<div id="html_30b1bc5d11e74feeb399b800682bdb94" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://search.ieice.org/bin/summary.php?id=e100-d_12_3041">A novel discriminative feature extraction for acoustic scene classification using rnn based source separation</a><br></div>`)[0];
            popup_b96027520a3545979912538b6a9065b0.setContent(html_30b1bc5d11e74feeb399b800682bdb94);
        

        circle_marker_2d119b3cff93416bb2d38c08cf19feaa.bindPopup(popup_b96027520a3545979912538b6a9065b0)
        ;

        
    
    
            var circle_marker_e70d209a1d9e425b8dee88eb731f28c4 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_561ad0f810c34b1cac62b7ccea4d1875 = L.popup({"maxWidth": "100%"});

        
            var html_4f04efd1c659439da8d581645dbbb6b4 = $(`<div id="html_4f04efd1c659439da8d581645dbbb6b4" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1910.06784">Acoustic Scene Classification Based on a Large-margin Factorized CNN</a><br></div>`)[0];
            popup_561ad0f810c34b1cac62b7ccea4d1875.setContent(html_4f04efd1c659439da8d581645dbbb6b4);
        

        circle_marker_e70d209a1d9e425b8dee88eb731f28c4.bindPopup(popup_561ad0f810c34b1cac62b7ccea4d1875)
        ;

        
    
    
            var circle_marker_59820d8ca786469eb197056f3c5aba3b = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1501fd536a574af18ab079108b5683a1 = L.popup({"maxWidth": "100%"});

        
            var html_c25dad2ff5c749e19535a54eb4ebf281 = $(`<div id="html_c25dad2ff5c749e19535a54eb4ebf281" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/2003.09164">Acoustic Scene Classification using Audio Tagging</a><br></div>`)[0];
            popup_1501fd536a574af18ab079108b5683a1.setContent(html_c25dad2ff5c749e19535a54eb4ebf281);
        

        circle_marker_59820d8ca786469eb197056f3c5aba3b.bindPopup(popup_1501fd536a574af18ab079108b5683a1)
        ;

        
    
    
            var circle_marker_cc180829cbd24f23a21f68ff36ba7460 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9e7d57024c454a71be1afe2d737d8d3d = L.popup({"maxWidth": "100%"});

        
            var html_96ed4f4faf994f18bd1c177a0d23654f = $(`<div id="html_96ed4f4faf994f18bd1c177a0d23654f" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1904.10135">Acoustic scene classification using teacher-student learning with soft-labels</a><br></div>`)[0];
            popup_9e7d57024c454a71be1afe2d737d8d3d.setContent(html_96ed4f4faf994f18bd1c177a0d23654f);
        

        circle_marker_cc180829cbd24f23a21f68ff36ba7460.bindPopup(popup_9e7d57024c454a71be1afe2d737d8d3d)
        ;

        
    
    
            var circle_marker_799f6e6b46a54f81b6e80b86c08d79da = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a0cfc35810a9458893a4c85d69529370 = L.popup({"maxWidth": "100%"});

        
            var html_8e42e2467920487db00f89d9d4b7402c = $(`<div id="html_8e42e2467920487db00f89d9d4b7402c" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8282240/">An acoustic monitoring system and its field trials</a><br></div>`)[0];
            popup_a0cfc35810a9458893a4c85d69529370.setContent(html_8e42e2467920487db00f89d9d4b7402c);
        

        circle_marker_799f6e6b46a54f81b6e80b86c08d79da.bindPopup(popup_a0cfc35810a9458893a4c85d69529370)
        ;

        
    
    
            var circle_marker_fd3baf3764d147f6a35a32c97fdd507b = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3369b352b7214e069639bf8512b6d673 = L.popup({"maxWidth": "100%"});

        
            var html_8a6fb16d6a914d6fbe9e9866f37f5b84 = $(`<div id="html_8a6fb16d6a914d6fbe9e9866f37f5b84" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553423/">Anomalous sound event detection based on wavenet</a><br></div>`)[0];
            popup_3369b352b7214e069639bf8512b6d673.setContent(html_8a6fb16d6a914d6fbe9e9866f37f5b84);
        

        circle_marker_fd3baf3764d147f6a35a32c97fdd507b.bindPopup(popup_3369b352b7214e069639bf8512b6d673)
        ;

        
    
    
            var circle_marker_79c9b534b27c4fca846f4e21de135f53 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5fd8594f2b5a4df8b61f3cb6bee7f568 = L.popup({"maxWidth": "100%"});

        
            var html_dcb6fa2145dd41c98d353c39048e1766 = $(`<div id="html_dcb6fa2145dd41c98d353c39048e1766" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-319-93554-6_66">Complex Activity Recognition Using Polyphonic Sound Event Detection</a><br></div>`)[0];
            popup_5fd8594f2b5a4df8b61f3cb6bee7f568.setContent(html_dcb6fa2145dd41c98d353c39048e1766);
        

        circle_marker_79c9b534b27c4fca846f4e21de135f53.bindPopup(popup_5fd8594f2b5a4df8b61f3cb6bee7f568)
        ;

        
    
    
            var circle_marker_2c5d353cb0cb4b38b42b1d5b1676a363 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9026564528c7485bb189f4d09e7f745d = L.popup({"maxWidth": "100%"});

        
            var html_70a5d135f1c94629aff4d2b9e5afa262 = $(`<div id="html_70a5d135f1c94629aff4d2b9e5afa262" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Han_206.pdf">Convolutional neural networks with binaural representations and background subtraction for acoustic scene classification</a><br></div>`)[0];
            popup_9026564528c7485bb189f4d09e7f745d.setContent(html_70a5d135f1c94629aff4d2b9e5afa262);
        

        circle_marker_2c5d353cb0cb4b38b42b1d5b1676a363.bindPopup(popup_9026564528c7485bb189f4d09e7f745d)
        ;

        
    
    
            var circle_marker_128126ccc79c476f975700a44ef5a06c = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7f9319c0e13041daa43646e5a3836633 = L.popup({"maxWidth": "100%"});

        
            var html_9caa8fd22cdd4aa39145878275fdd9c8 = $(`<div id="html_9caa8fd22cdd4aa39145878275fdd9c8" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/2009.09642">DCASENET: A joint pre-trained deep neural network for detecting and classifying acoustic scenes and events</a><br></div>`)[0];
            popup_7f9319c0e13041daa43646e5a3836633.setContent(html_9caa8fd22cdd4aa39145878275fdd9c8);
        

        circle_marker_128126ccc79c476f975700a44ef5a06c.bindPopup(popup_7f9319c0e13041daa43646e5a3836633)
        ;

        
    
    
            var circle_marker_5f451c8a49be475c8e9a208df5f2896e = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_de587f0479f54229b257f63cd2017963 = L.popup({"maxWidth": "100%"});

        
            var html_77db563db54547b3845acdb3ae046ab4 = $(`<div id="html_77db563db54547b3845acdb3ae046ab4" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Jung_187.pdf">DNN-based audio scene classification for DCASE 2017: dual input features, balancing cost, and stochastic data duplication</a><br></div>`)[0];
            popup_de587f0479f54229b257f63cd2017963.setContent(html_77db563db54547b3845acdb3ae046ab4);
        

        circle_marker_5f451c8a49be475c8e9a208df5f2896e.bindPopup(popup_de587f0479f54229b257f63cd2017963)
        ;

        
    
    
            var circle_marker_7c2e8144a8a9430fb9b72893a2029fff = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a7845b1afdc0467eaa40cc62ad1230d5 = L.popup({"maxWidth": "100%"});

        
            var html_df777eecb3f84354b845c3580db0a326 = $(`<div id="html_df777eecb3f84354b845c3580db0a326" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Suh_101.pdf">Designing Acoustic Scene Classification Models with CNN Variants</a><br></div>`)[0];
            popup_a7845b1afdc0467eaa40cc62ad1230d5.setContent(html_df777eecb3f84354b845c3580db0a326);
        

        circle_marker_7c2e8144a8a9430fb9b72893a2029fff.bindPopup(popup_a7845b1afdc0467eaa40cc62ad1230d5)
        ;

        
    
    
            var circle_marker_214d05a7bcb6444883669320a7096750 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cd71ac492cc24d8a855a5a81d6143939 = L.popup({"maxWidth": "100%"});

        
            var html_99a900a8ef514836b1899f6ce23c722a = $(`<div id="html_99a900a8ef514836b1899f6ce23c722a" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7952181/">Detection of anomaly acoustic scenes based on a temporal dissimilarity model</a><br></div>`)[0];
            popup_cd71ac492cc24d8a855a5a81d6143939.setContent(html_99a900a8ef514836b1899f6ce23c722a);
        

        circle_marker_214d05a7bcb6444883669320a7096750.bindPopup(popup_cd71ac492cc24d8a855a5a81d6143939)
        ;

        
    
    
            var circle_marker_7616e877bcb1449e906ab6cec7d4a319 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_186acfb0b5ff4f94ae0141a75c5a39e9 = L.popup({"maxWidth": "100%"});

        
            var html_57a79bd174b94123a1f281e6feef17ce = $(`<div id="html_57a79bd174b94123a1f281e6feef17ce" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683514/">Domain mismatch robust acoustic scene classification using channel information conversion</a><br></div>`)[0];
            popup_186acfb0b5ff4f94ae0141a75c5a39e9.setContent(html_57a79bd174b94123a1f281e6feef17ce);
        

        circle_marker_7616e877bcb1449e906ab6cec7d4a319.bindPopup(popup_186acfb0b5ff4f94ae0141a75c5a39e9)
        ;

        
    
    
            var circle_marker_a2ef009840ed449a9d52ecb390ecc685 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1c3dbc9b10034d9ca3a5eebd864ce50e = L.popup({"maxWidth": "100%"});

        
            var html_3e147aa80a094a2e932ae78fb11475ae = $(`<div id="html_3e147aa80a094a2e932ae78fb11475ae" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Kim_21.pdf">GIST_WISENETAI AUDIO TAGGER BASED ON CONCATENATED RESIDUAL NETWORK FOR DCASE 2018 CHALLENGE TASK 2</a><br></div>`)[0];
            popup_1c3dbc9b10034d9ca3a5eebd864ce50e.setContent(html_3e147aa80a094a2e932ae78fb11475ae);
        

        circle_marker_a2ef009840ed449a9d52ecb390ecc685.bindPopup(popup_1c3dbc9b10034d9ca3a5eebd864ce50e)
        ;

        
    
    
            var circle_marker_7acd9b5b8d0a4565b6449724787779d6 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a77457c2e7ff4c2ead143bd89e5c7636 = L.popup({"maxWidth": "100%"});

        
            var html_b3244e7d7717439786c9745a914627bf = $(`<div id="html_b3244e7d7717439786c9745a914627bf" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Mun_213.pdf">Generative adversarial network based acoustic scene training set augmentation and selection using SVM hyper-plane</a><br></div>`)[0];
            popup_a77457c2e7ff4c2ead143bd89e5c7636.setContent(html_b3244e7d7717439786c9745a914627bf);
        

        circle_marker_7acd9b5b8d0a4565b6449724787779d6.bindPopup(popup_a77457c2e7ff4c2ead143bd89e5c7636)
        ;

        
    
    
            var circle_marker_15c90c3d7e3a4e548283fc53c5dfeadc = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bc8c295b35494c69aabd70fbd76991a6 = L.popup({"maxWidth": "100%"});

        
            var html_163982cbf68d4bafbb79caa279a36807 = $(`<div id="html_163982cbf68d4bafbb79caa279a36807" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://www.jstage.jst.go.jp/article/transinf/E101.D/12/E101.D_2018EDP7140/_article/-char/ja/">Hidden singer: Distinguishing imitation singers based on training with only the original song</a><br></div>`)[0];
            popup_bc8c295b35494c69aabd70fbd76991a6.setContent(html_163982cbf68d4bafbb79caa279a36807);
        

        circle_marker_15c90c3d7e3a4e548283fc53c5dfeadc.bindPopup(popup_bc8c295b35494c69aabd70fbd76991a6)
        ;

        
    
    
            var circle_marker_0e3a7ae474964bd2803fb5e8c0139620 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2f3821f19bf14a298c88d38b15fd197f = L.popup({"maxWidth": "100%"});

        
            var html_924ea5d8c223492590e73f53d052481e = $(`<div id="html_924ea5d8c223492590e73f53d052481e" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9186616/">Knowledge Distillation in Acoustic Scene Classification</a><br></div>`)[0];
            popup_2f3821f19bf14a298c88d38b15fd197f.setContent(html_924ea5d8c223492590e73f53d052481e);
        

        circle_marker_0e3a7ae474964bd2803fb5e8c0139620.bindPopup(popup_2f3821f19bf14a298c88d38b15fd197f)
        ;

        
    
    
            var circle_marker_a07e1dc29608403c8b8a4700b9511e3d = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0d28fcab6a4442e69cd9309e02048aed = L.popup({"maxWidth": "100%"});

        
            var html_26eec6c270194a92beae60f180c0dfae = $(`<div id="html_26eec6c270194a92beae60f180c0dfae" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Kim_87.pdf">MULTI-CHANNEL FEATURE USING INTER-CLASS AND INTER-DEVICE STANDARD DEVIATIONS FOR ACOUSTIC SCENE CLASSIFICATION</a><br></div>`)[0];
            popup_0d28fcab6a4442e69cd9309e02048aed.setContent(html_26eec6c270194a92beae60f180c0dfae);
        

        circle_marker_a07e1dc29608403c8b8a4700b9511e3d.bindPopup(popup_0d28fcab6a4442e69cd9309e02048aed)
        ;

        
    
    
            var circle_marker_5399e6d108ec4df191b5c5f857f5c4c6 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b4d06f564c47435597e0dd6039769efe = L.popup({"maxWidth": "100%"});

        
            var html_050bade570fc4f7693e4da9ad2d045bf = $(`<div id="html_050bade570fc4f7693e4da9ad2d045bf" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/2007.05191">Overcoming label noise in audio event detection using sequential labeling</a><br></div>`)[0];
            popup_b4d06f564c47435597e0dd6039769efe.setContent(html_050bade570fc4f7693e4da9ad2d045bf);
        

        circle_marker_5399e6d108ec4df191b5c5f857f5c4c6.bindPopup(popup_b4d06f564c47435597e0dd6039769efe)
        ;

        
    
    
            var circle_marker_8aef331db99845b09891173ef2b8bea7 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_db81dddd725b430ebcb397eba7da801a = L.popup({"maxWidth": "100%"});

        
            var html_b8c8956cc72746999bfb6af2ebb49391 = $(`<div id="html_b8c8956cc72746999bfb6af2ebb49391" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1710.06648">Representation learning of music using artist labels</a><br></div>`)[0];
            popup_db81dddd725b430ebcb397eba7da801a.setContent(html_b8c8956cc72746999bfb6af2ebb49391);
        

        circle_marker_8aef331db99845b09891173ef2b8bea7.bindPopup(popup_db81dddd725b430ebcb397eba7da801a)
        ;

        
    
    
            var circle_marker_0c37887f4789425391f14807f5c70caa = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4283ddd7017c40a98cce7d415434a577 = L.popup({"maxWidth": "100%"});

        
            var html_d7a7f7f9bcb6445c8fecb1d824bceac2 = $(`<div id="html_d7a7f7f9bcb6445c8fecb1d824bceac2" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683068/">Scene-dependent Anomalous Acoustic-event Detection Based on Conditional Wavenet and I-vector</a><br></div>`)[0];
            popup_4283ddd7017c40a98cce7d415434a577.setContent(html_d7a7f7f9bcb6445c8fecb1d824bceac2);
        

        circle_marker_0c37887f4789425391f14807f5c70caa.bindPopup(popup_4283ddd7017c40a98cce7d415434a577)
        ;

        
    
    
            var circle_marker_4641d1d65bdb423885e6a40e4055f64b = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3f64f2d72f96477a94aabcfc37db2a38 = L.popup({"maxWidth": "100%"});

        
            var html_8bcb8f891c7b4093ac63e8ea0138cf6f = $(`<div id="html_8bcb8f891c7b4093ac63e8ea0138cf6f" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Lim_77.pdf">Sound event detection in domestic environments using ensemble of convolutional recurrent neural networks</a><br></div>`)[0];
            popup_3f64f2d72f96477a94aabcfc37db2a38.setContent(html_8bcb8f891c7b4093ac63e8ea0138cf6f);
        

        circle_marker_4641d1d65bdb423885e6a40e4055f64b.bindPopup(popup_3f64f2d72f96477a94aabcfc37db2a38)
        ;

        
    
    
            var circle_marker_6e0dba8033d7454891aad7861b0d26e8 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9ddf1e89acea42dc8760c02b327d9f47 = L.popup({"maxWidth": "100%"});

        
            var html_d4e53ee5989b466e8095a6a1a4c4b8c3 = $(`<div id="html_d4e53ee5989b466e8095a6a1a4c4b8c3" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60743">SpecAugment for sound event detection in domestic environments using ensemble of convolutional recurrent neural networks</a><br></div>`)[0];
            popup_9ddf1e89acea42dc8760c02b327d9f47.setContent(html_d4e53ee5989b466e8095a6a1a4c4b8c3);
        

        circle_marker_6e0dba8033d7454891aad7861b0d26e8.bindPopup(popup_9ddf1e89acea42dc8760c02b327d9f47)
        ;

        
    
    
            var circle_marker_4331fe0971ec46f59b5b9d95a783105b = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a04103bf82944e6ea0fea336d3db264b = L.popup({"maxWidth": "100%"});

        
            var html_019cd7a144be48bb8c811b3d62ae283d = $(`<div id="html_019cd7a144be48bb8c811b3d62ae283d" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="https://arxiv.org/abs/1910.06790">Weakly Labeled Sound Event Detection using Tri-training and Adversarial Learning</a><br></div>`)[0];
            popup_a04103bf82944e6ea0fea336d3db264b.setContent(html_019cd7a144be48bb8c811b3d62ae283d);
        

        circle_marker_4331fe0971ec46f59b5b9d95a783105b.bindPopup(popup_a04103bf82944e6ea0fea336d3db264b)
        ;

        
    
    
            var circle_marker_1f808adf87d1490e81b8c61de2691794 = L.circleMarker(
                [36.638392, 127.6961188],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7bcf768fc2334a0cbefdaadf8826b93e = L.popup({"maxWidth": "100%"});

        
            var html_da53ee41f6f345e7980b74a2af690d09 = $(`<div id="html_da53ee41f6f345e7980b74a2af690d09" style="width: 100.0%; height: 100.0%;">Country : South Korea<br>                         Paper : <a href="http://dcase.community/documents/workshop2018/proceedings/DCASE2018Workshop_Lim_115.pdf">Weakly labeled semi-supervised sound event detection using crnn with inception module</a><br></div>`)[0];
            popup_7bcf768fc2334a0cbefdaadf8826b93e.setContent(html_da53ee41f6f345e7980b74a2af690d09);
        

        circle_marker_1f808adf87d1490e81b8c61de2691794.bindPopup(popup_7bcf768fc2334a0cbefdaadf8826b93e)
        ;

        
    
    
            var circle_marker_ced0695d806a4fe298ca67e8a3e421b5 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_acc0779930fb4c878ff28c663633cc67 = L.popup({"maxWidth": "100%"});

        
            var html_39925987dd3b4fc380b1cc8632890a71 = $(`<div id="html_39925987dd3b4fc380b1cc8632890a71" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/1909.12598">" Best-of-Many-Samples" Distribution Matching</a><br></div>`)[0];
            popup_acc0779930fb4c878ff28c663633cc67.setContent(html_39925987dd3b4fc380b1cc8632890a71);
        

        circle_marker_ced0695d806a4fe298ca67e8a3e421b5.bindPopup(popup_acc0779930fb4c878ff28c663633cc67)
        ;

        
    
    
            var circle_marker_fe8cc2a99f1740a2a1e3d9e947515ffb = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bfb80578003e4735bc558633a8165044 = L.popup({"maxWidth": "100%"});

        
            var html_ae78d75a1cd546faaeb8a3e7a5ae2a54 = $(`<div id="html_ae78d75a1cd546faaeb8a3e7a5ae2a54" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553225/">A fusion of deep convolutional generative adversarial networks and sequence to sequence autoencoders for acoustic scene classification</a><br></div>`)[0];
            popup_bfb80578003e4735bc558633a8165044.setContent(html_ae78d75a1cd546faaeb8a3e7a5ae2a54);
        

        circle_marker_fe8cc2a99f1740a2a1e3d9e947515ffb.bindPopup(popup_bfb80578003e4735bc558633a8165044)
        ;

        
    
    
            var circle_marker_830018039f5e45aa950f189a8b365b7d = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5627bd719f46455b8941c68bbe110cd5 = L.popup({"maxWidth": "100%"});

        
            var html_8ed6f29be2af45b08c7c49fed0225b7f = $(`<div id="html_8ed6f29be2af45b08c7c49fed0225b7f" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://www.mdpi.com/2076-3417/10/6/2020">A review of deep learning based methods for acoustic scene classification</a><br></div>`)[0];
            popup_5627bd719f46455b8941c68bbe110cd5.setContent(html_8ed6f29be2af45b08c7c49fed0225b7f);
        

        circle_marker_830018039f5e45aa950f189a8b365b7d.bindPopup(popup_5627bd719f46455b8941c68bbe110cd5)
        ;

        
    
    
            var circle_marker_bcd23046013045b6bad1f9a0c1bfa6fd = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2b08a5d1f55d425788fc122ac6a36feb = L.popup({"maxWidth": "100%"});

        
            var html_782fb3038bc14b869cc552067580fec0 = $(`<div id="html_782fb3038bc14b869cc552067580fec0" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/1703.04770">Audio scene classification with deep recurrent neural networks</a><br></div>`)[0];
            popup_2b08a5d1f55d425788fc122ac6a36feb.setContent(html_782fb3038bc14b869cc552067580fec0);
        

        circle_marker_bcd23046013045b6bad1f9a0c1bfa6fd.bindPopup(popup_2b08a5d1f55d425788fc122ac6a36feb)
        ;

        
    
    
            var circle_marker_7ef85df0f7ce4ca4b8fc5c101e48c212 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bac21606544041488c5d942046f8f65f = L.popup({"maxWidth": "100%"});

        
            var html_c201f9f6f52143e99452f772c21a47f0 = $(`<div id="html_c201f9f6f52143e99452f772c21a47f0" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/1811.01095">Beyond equal-length snippets: How long is sufficient to recognize an audio scene?</a><br></div>`)[0];
            popup_bac21606544041488c5d942046f8f65f.setContent(html_c201f9f6f52143e99452f772c21a47f0);
        

        circle_marker_7ef85df0f7ce4ca4b8fc5c101e48c212.bindPopup(popup_bac21606544041488c5d942046f8f65f)
        ;

        
    
    
            var circle_marker_4dd50e0e93db4fb1acc62f8a3fd40e57 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f9e1ce97e1e44c72b65dab7b6b7fa78f = L.popup({"maxWidth": "100%"});

        
            var html_7053c64c682b4a73bb12230ac9572d1a = $(`<div id="html_7053c64c682b4a73bb12230ac9572d1a" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9052950/">Beyond the Dcase 2017 Challenge on Rare Sound Event Detection: A Proposal for a More Realistic Training and Test Framework</a><br></div>`)[0];
            popup_f9e1ce97e1e44c72b65dab7b6b7fa78f.setContent(html_7053c64c682b4a73bb12230ac9572d1a);
        

        circle_marker_4dd50e0e93db4fb1acc62f8a3fd40e57.bindPopup(popup_f9e1ce97e1e44c72b65dab7b6b7fa78f)
        ;

        
    
    
            var circle_marker_1a0b7c60074e422c875845f7a8ef6b34 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6378d603f902488dac41e4b605c3318a = L.popup({"maxWidth": "100%"});

        
            var html_d7ee12ca7dcc4d819d01c0f7cf1c9a7d = $(`<div id="html_d7ee12ca7dcc4d819d01c0f7cf1c9a7d" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://mediatum.ub.tum.de/1463108">Deep representation learning techniques for audio signal processing</a><br></div>`)[0];
            popup_6378d603f902488dac41e4b605c3318a.setContent(html_d7ee12ca7dcc4d819d01c0f7cf1c9a7d);
        

        circle_marker_1a0b7c60074e422c875845f7a8ef6b34.bindPopup(popup_6378d603f902488dac41e4b605c3318a)
        ;

        
    
    
            var circle_marker_5a3ea90053764398b2bcee6bda5d1c55 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a808959fbbca4f90aba2f05080e957ba = L.popup({"maxWidth": "100%"});

        
            var html_89f87ad78966464bae459a1486fc3f6b = $(`<div id="html_89f87ad78966464bae459a1486fc3f6b" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933052/">Improved audio scene classification based on label-tree embeddings and convolutional neural networks</a><br></div>`)[0];
            popup_a808959fbbca4f90aba2f05080e957ba.setContent(html_89f87ad78966464bae459a1486fc3f6b);
        

        circle_marker_5a3ea90053764398b2bcee6bda5d1c55.bindPopup(popup_a808959fbbca4f90aba2f05080e957ba)
        ;

        
    
    
            var circle_marker_7588ff17825b47cbbf137c14082dca09 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_885a19071c6440ec9a27b74a0e025ee4 = L.popup({"maxWidth": "100%"});

        
            var html_5c180fde6b6549f780fdbe67be35058e = $(`<div id="html_5c180fde6b6549f780fdbe67be35058e" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60772">Open-set acoustic scene classification with deep convolutional autoencoders</a><br></div>`)[0];
            popup_885a19071c6440ec9a27b74a0e025ee4.setContent(html_5c180fde6b6549f780fdbe67be35058e);
        

        circle_marker_7588ff17825b47cbbf137c14082dca09.bindPopup(popup_885a19071c6440ec9a27b74a0e025ee4)
        ;

        
    
    
            var circle_marker_5b2c74e333664ae584d6cc5a3316e463 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_06f09c76310f4305a8a0831ef1374ee9 = L.popup({"maxWidth": "100%"});

        
            var html_6ac275b5514749deb1ce47d1699fc61f = $(`<div id="html_6ac275b5514749deb1ce47d1699fc61f" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7952262/">Reverberation-based feature extraction for acoustic scene classification</a><br></div>`)[0];
            popup_06f09c76310f4305a8a0831ef1374ee9.setContent(html_6ac275b5514749deb1ce47d1699fc61f);
        

        circle_marker_5b2c74e333664ae584d6cc5a3316e463.bindPopup(popup_06f09c76310f4305a8a0831ef1374ee9)
        ;

        
    
    
            var circle_marker_e64d9f2ff718429f94f8c452138ca18e = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_80ce1ef6ec254f648522bcbaa6e99d72 = L.popup({"maxWidth": "100%"});

        
            var html_302aefaa1a3c4ed7984140081c9142f6 = $(`<div id="html_302aefaa1a3c4ed7984140081c9142f6" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933059/">Robust detection of environmental sounds in binaural auditory scenes</a><br></div>`)[0];
            popup_80ce1ef6ec254f648522bcbaa6e99d72.setContent(html_302aefaa1a3c4ed7984140081c9142f6);
        

        circle_marker_e64d9f2ff718429f94f8c452138ca18e.bindPopup(popup_80ce1ef6ec254f648522bcbaa6e99d72)
        ;

        
    
    
            var circle_marker_6a5e6c99c46440108531ad679a4d43c6 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_58b08f6b73894eb0a14e54088da5b964 = L.popup({"maxWidth": "100%"});

        
            var html_d16a5b6ee7594a089893c50edc427305 = $(`<div id="html_d16a5b6ee7594a089893c50edc427305" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8901720/">Selective Hearing: A Machine Listening Perspective</a><br></div>`)[0];
            popup_58b08f6b73894eb0a14e54088da5b964.setContent(html_d16a5b6ee7594a089893c50edc427305);
        

        circle_marker_6a5e6c99c46440108531ad679a4d43c6.bindPopup(popup_58b08f6b73894eb0a14e54088da5b964)
        ;

        
    
    
            var circle_marker_fa108d31a81344519e5815dfc1291cf2 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ea8ce99e28654dc6a1e5c8f1055f622d = L.popup({"maxWidth": "100%"});

        
            var html_089f12efe977422aba7846bf222e7603 = $(`<div id="html_089f12efe977422aba7846bf222e7603" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Amiriparian_173.pdf">Sequence to sequence autoencoders for unsupervised representation learning from audio</a><br></div>`)[0];
            popup_ea8ce99e28654dc6a1e5c8f1055f622d.setContent(html_089f12efe977422aba7846bf222e7603);
        

        circle_marker_fa108d31a81344519e5815dfc1291cf2.bindPopup(popup_ea8ce99e28654dc6a1e5c8f1055f622d)
        ;

        
    
    
            var circle_marker_b59c4afe6ff847caa76b7cad35161d6c = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_56afc22b91da44a6823349b72fbeae2e = L.popup({"maxWidth": "100%"});

        
            var html_d44779cd31a74bd7a74e5a36ba6c1ee4 = $(`<div id="html_d44779cd31a74bd7a74e5a36ba6c1ee4" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://www.academia.edu/download/56171528/Amiriparian17-SSC.PDF">Snore Sound Classification Using Image-Based Deep Spectrum Features.</a><br></div>`)[0];
            popup_56afc22b91da44a6823349b72fbeae2e.setContent(html_d44779cd31a74bd7a74e5a36ba6c1ee4);
        

        circle_marker_b59c4afe6ff847caa76b7cad35161d6c.bindPopup(popup_56afc22b91da44a6823349b72fbeae2e)
        ;

        
    
    
            var circle_marker_2f96d6dc15404ed0895badb219c5a9e6 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1333dfe8c99641ce955b92414447c2cb = L.popup({"maxWidth": "100%"});

        
            var html_7076d26bf97446cf81ea20afbcd9d6d8 = $(`<div id="html_7076d26bf97446cf81ea20afbcd9d6d8" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://dl.gi.de/handle/20.500.12116/4113">Soundslike</a><br></div>`)[0];
            popup_1333dfe8c99641ce955b92414447c2cb.setContent(html_7076d26bf97446cf81ea20afbcd9d6d8);
        

        circle_marker_2f96d6dc15404ed0895badb219c5a9e6.bindPopup(popup_1333dfe8c99641ce955b92414447c2cb)
        ;

        
    
    
            var circle_marker_f92ab58f474b4493961f827c68a3b9db = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5e7370f63a7b4e68895599e5b4a3233f = L.popup({"maxWidth": "100%"});

        
            var html_bf65152a331642dcaf31dd64c717640f = $(`<div id="html_bf65152a331642dcaf31dd64c717640f" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/2008.04590">Surgical Mask Detection with Convolutional Neural Networks and Data Augmentations on Spectrograms</a><br></div>`)[0];
            popup_5e7370f63a7b4e68895599e5b4a3233f.setContent(html_bf65152a331642dcaf31dd64c717640f);
        

        circle_marker_f92ab58f474b4493961f827c68a3b9db.bindPopup(popup_5e7370f63a7b4e68895599e5b4a3233f)
        ;

        
    
    
            var circle_marker_bd89681abe5847ed91c82c79bac83202 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fd77a308d8a948c6b0635a02e4d13237 = L.popup({"maxWidth": "100%"});

        
            var html_507177aeb32246dab46a50e9668cdcf3 = $(`<div id="html_507177aeb32246dab46a50e9668cdcf3" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/334548980_The_2019_Multimedia_for_Recommender_System_Task_MovieREC_and_NewsREEL_at_MediaEval/links/5d30c3ac299bf1547cc258c1/The-2019-Multimedia-for-Recommender-System-Task-MovieREC-and-NewsREEL-at-MediaEval.pdf">The 2019 Multimedia for Recommender System Task: MovieREC and NewsREEL at MediaEval</a><br></div>`)[0];
            popup_fd77a308d8a948c6b0635a02e4d13237.setContent(html_507177aeb32246dab46a50e9668cdcf3);
        

        circle_marker_bd89681abe5847ed91c82c79bac83202.bindPopup(popup_fd77a308d8a948c6b0635a02e4d13237)
        ;

        
    
    
            var circle_marker_1fd5a5f6c8d04804aff3270275ecf137 = L.circleMarker(
                [51.0834196, 10.4234469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_db0e3446ca444268b318516904a2ae69 = L.popup({"maxWidth": "100%"});

        
            var html_a710ff9def7440b5965482bf31cc3fe0 = $(`<div id="html_a710ff9def7440b5965482bf31cc3fe0" style="width: 100.0%; height: 100.0%;">Country : Germany<br>                         Paper : <a href="https://arxiv.org/abs/2005.00145">Unsupervised Domain Adaptation for Acoustic Scene Classification Using Band-Wise Statistics Matching</a><br></div>`)[0];
            popup_db0e3446ca444268b318516904a2ae69.setContent(html_a710ff9def7440b5965482bf31cc3fe0);
        

        circle_marker_1fd5a5f6c8d04804aff3270275ecf137.bindPopup(popup_db0e3446ca444268b318516904a2ae69)
        ;

        
    
    
            var circle_marker_76a83849610e40b4a2f4cd830f9641c6 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0c2a81b25caf4636bf5d84d528f2ae7d = L.popup({"maxWidth": "100%"});

        
            var html_127d67dc992f4af2ac1131510a00bc36 = $(`<div id="html_127d67dc992f4af2ac1131510a00bc36" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://arxiv.org/abs/1806.07506">A simple fusion of deep and shallow learning for acoustic scene classification</a><br></div>`)[0];
            popup_0c2a81b25caf4636bf5d84d528f2ae7d.setContent(html_127d67dc992f4af2ac1131510a00bc36);
        

        circle_marker_76a83849610e40b4a2f4cd830f9641c6.bindPopup(popup_0c2a81b25caf4636bf5d84d528f2ae7d)
        ;

        
    
    
            var circle_marker_55157ca5791e40e0887347d591bfd311 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_df246b4ad77642bc8f0ff6694f2574b4 = L.popup({"maxWidth": "100%"});

        
            var html_c880f43faaf74a1f96be21330a8edb4c = $(`<div id="html_c880f43faaf74a1f96be21330a8edb4c" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://repositori.upf.edu/handle/10230/33454">Acoustic scene classification by ensembling gradient boosting machine and convolutional neural networks</a><br></div>`)[0];
            popup_df246b4ad77642bc8f0ff6694f2574b4.setContent(html_c880f43faaf74a1f96be21330a8edb4c);
        

        circle_marker_55157ca5791e40e0887347d591bfd311.bindPopup(popup_df246b4ad77642bc8f0ff6694f2574b4)
        ;

        
    
    
            var circle_marker_9078a6101df94bfa9b2c36d8363a3f6c = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_edcba03a57864b5887687adeadad2d69 = L.popup({"maxWidth": "100%"});

        
            var html_f29128e371274b37a8789514ba968464 = $(`<div id="html_f29128e371274b37a8789514ba968464" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Gong_189.pdf">Acoustic scene classification by fusing LightGBM and VGG-net multichannel predictions</a><br></div>`)[0];
            popup_edcba03a57864b5887687adeadad2d69.setContent(html_f29128e371274b37a8789514ba968464);
        

        circle_marker_9078a6101df94bfa9b2c36d8363a3f6c.bindPopup(popup_edcba03a57864b5887687adeadad2d69)
        ;

        
    
    
            var circle_marker_eba5ad7bd588419694ae1c0380d92d58 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0748c131a3734553afac9bd8056bf75d = L.popup({"maxWidth": "100%"});

        
            var html_d5c38dd9f14544a9b58b540a8b0f6f1e = $(`<div id="html_d5c38dd9f14544a9b58b540a8b0f6f1e" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://arxiv.org/abs/1906.02975">Audio tagging with noisy labels and minimal supervision</a><br></div>`)[0];
            popup_0748c131a3734553afac9bd8056bf75d.setContent(html_d5c38dd9f14544a9b58b540a8b0f6f1e);
        

        circle_marker_eba5ad7bd588419694ae1c0380d92d58.bindPopup(popup_0748c131a3734553afac9bd8056bf75d)
        ;

        
    
    
            var circle_marker_93fb081d34b9447c91e956170f3cabf1 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_799cc40bdf81495a89d2659fb4667406 = L.popup({"maxWidth": "100%"});

        
            var html_e7020eb5b979485d8952a9bb065e648d = $(`<div id="html_e7020eb5b979485d8952a9bb065e648d" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://e-archivo.uc3m.es/handle/10016/29786">Bayesian and echoic log-surprise for auditory saliency detection</a><br></div>`)[0];
            popup_799cc40bdf81495a89d2659fb4667406.setContent(html_e7020eb5b979485d8952a9bb065e648d);
        

        circle_marker_93fb081d34b9447c91e956170f3cabf1.bindPopup(popup_799cc40bdf81495a89d2659fb4667406)
        ;

        
    
    
            var circle_marker_27536c9d09194dcdb8686a747876ef04 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ce27a093aa1f4e569a7fd83fe1b9c472 = L.popup({"maxWidth": "100%"});

        
            var html_cb6f9b03f5ee41d9a1075d06f1edc33e = $(`<div id="html_cb6f9b03f5ee41d9a1075d06f1edc33e" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="">DJ-Running: An Emotion-based System for Recommending Spotify Songs to Runners.</a><br></div>`)[0];
            popup_ce27a093aa1f4e569a7fd83fe1b9c472.setContent(html_cb6f9b03f5ee41d9a1075d06f1edc33e);
        

        circle_marker_27536c9d09194dcdb8686a747876ef04.bindPopup(popup_ce27a093aa1f4e569a7fd83fe1b9c472)
        ;

        
    
    
            var circle_marker_1e912c521adb40159a77db0bf0a75a2f = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_96409961b97b4d558950311acb6c4552 = L.popup({"maxWidth": "100%"});

        
            var html_4b8489625a7d4224a81cb4f75a1d4c25 = $(`<div id="html_4b8489625a7d4224a81cb4f75a1d4c25" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://arxiv.org/abs/1807.09902">General-purpose tagging of freesound audio with audioset labels: Task description, dataset, and baseline</a><br></div>`)[0];
            popup_96409961b97b4d558950311acb6c4552.setContent(html_4b8489625a7d4224a81cb4f75a1d4c25);
        

        circle_marker_1e912c521adb40159a77db0bf0a75a2f.bindPopup(popup_96409961b97b4d558950311acb6c4552)
        ;

        
    
    
            var circle_marker_7d05633fe82543b5b61962fa9b14a044 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ffef5339e1a2461d84aed79ec2979a7b = L.popup({"maxWidth": "100%"});

        
            var html_b6d097b0238a45bdb22e316c7a74ddc3 = $(`<div id="html_b6d097b0238a45bdb22e316c7a74ddc3" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://ccrma.stanford.edu/~urinieto/MARL/publications/ISMIR2020_MoodPrediction.pdf">MOOD CLASSIFICATION USING LISTENING DATA</a><br></div>`)[0];
            popup_ffef5339e1a2461d84aed79ec2979a7b.setContent(html_b6d097b0238a45bdb22e316c7a74ddc3);
        

        circle_marker_7d05633fe82543b5b61962fa9b14a044.bindPopup(popup_ffef5339e1a2461d84aed79ec2979a7b)
        ;

        
    
    
            var circle_marker_4705fd8462174fa686b6c840d04a547b = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3b6c350355124d15a723295a512d2464 = L.popup({"maxWidth": "100%"});

        
            var html_d47aca11f6f74d4197958b1eb53bc101 = $(`<div id="html_d47aca11f6f74d4197958b1eb53bc101" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3298689.3347052">Music cold-start and long-tail recommendation: bias in deep representations</a><br></div>`)[0];
            popup_3b6c350355124d15a723295a512d2464.setContent(html_d47aca11f6f74d4197958b1eb53bc101);
        

        circle_marker_4705fd8462174fa686b6c840d04a547b.bindPopup(popup_3b6c350355124d15a723295a512d2464)
        ;

        
    
    
            var circle_marker_db48e96b637841e187a02c91d5e24d6f = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5f8a8c9e1c36454ba3ea4bb13edec592 = L.popup({"maxWidth": "100%"});

        
            var html_0b147c23927c4c50a9a443866ab4769c = $(`<div id="html_0b147c23927c4c50a9a443866ab4769c" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://arxiv.org/abs/1908.06708">Recommender Systems Fairness Evaluation via Generalized Cross Entropy</a><br></div>`)[0];
            popup_5f8a8c9e1c36454ba3ea4bb13edec592.setContent(html_0b147c23927c4c50a9a443866ab4769c);
        

        circle_marker_db48e96b637841e187a02c91d5e24d6f.bindPopup(popup_5f8a8c9e1c36454ba3ea4bb13edec592)
        ;

        
    
    
            var circle_marker_69b66d96dc994373ac3b5d3c76d2effc = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_96ae79207f3c4cc1a0c2adeb31cebad0 = L.popup({"maxWidth": "100%"});

        
            var html_1b7917ec25b24351be0bb75b1ca1a36e = $(`<div id="html_1b7917ec25b24351be0bb75b1ca1a36e" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682858/">Sound event envelope estimation in polyphonic mixtures</a><br></div>`)[0];
            popup_96ae79207f3c4cc1a0c2adeb31cebad0.setContent(html_1b7917ec25b24351be0bb75b1ca1a36e);
        

        circle_marker_69b66d96dc994373ac3b5d3c76d2effc.bindPopup(popup_96ae79207f3c4cc1a0c2adeb31cebad0)
        ;

        
    
    
            var circle_marker_77404311275f4af6b879a3400be44022 = L.circleMarker(
                [39.3262345, -4.8380649],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0649a41855144be98f4f285914a59001 = L.popup({"maxWidth": "100%"});

        
            var html_02d8e5d8f39b4c6bae809d580b894e43 = $(`<div id="html_02d8e5d8f39b4c6bae809d580b894e43" style="width: 100.0%; height: 100.0%;">Country : Spain<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Naranjo-Alcazar_34_t1.pdf">TASK 1 DCASE 2020: ASC WITH MISMATCH DEVICES AND REDUCED SIZE MODEL USING RESIDUAL SQUEEZE-EXCITATION CNNS</a><br></div>`)[0];
            popup_0649a41855144be98f4f285914a59001.setContent(html_02d8e5d8f39b4c6bae809d580b894e43);
        

        circle_marker_77404311275f4af6b879a3400be44022.bindPopup(popup_0649a41855144be98f4f285914a59001)
        ;

        
    
    
            var circle_marker_f89443e9ab3243e6b7ca0355f2a1bc50 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ccff0cf62825487692ffd9c705626551 = L.popup({"maxWidth": "100%"});

        
            var html_5b220331d0af4683a521776b92f8f269 = $(`<div id="html_5b220331d0af4683a521776b92f8f269" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-319-63450-0_4">Acoustic features for environmental sound analysis</a><br></div>`)[0];
            popup_ccff0cf62825487692ffd9c705626551.setContent(html_5b220331d0af4683a521776b92f8f269);
        

        circle_marker_f89443e9ab3243e6b7ca0355f2a1bc50.bindPopup(popup_ccff0cf62825487692ffd9c705626551)
        ;

        
    
    
            var circle_marker_34a7e3a4b3cd4c9db123e0b2b0816a97 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7b5c18d1d2c04aacb02905c9201ad8e9 = L.popup({"maxWidth": "100%"});

        
            var html_5336f35788344c9d9b882634c0b98164 = $(`<div id="html_5336f35788344c9d9b882634c0b98164" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://hal.archives-ouvertes.fr/tel-01912022/">Apprentissage de représentations pour l'analyse de scènes sonores</a><br></div>`)[0];
            popup_7b5c18d1d2c04aacb02905c9201ad8e9.setContent(html_5336f35788344c9d9b882634c0b98164);
        

        circle_marker_34a7e3a4b3cd4c9db123e0b2b0816a97.bindPopup(popup_7b5c18d1d2c04aacb02905c9201ad8e9)
        ;

        
    
    
            var circle_marker_8de0de84efb2481b91b336f90bbab340 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_53b35dcb734e44d0bcb9433bd6dfc68a = L.popup({"maxWidth": "100%"});

        
            var html_920922ae01414cb9a0f463afe6c90d15 = $(`<div id="html_920922ae01414cb9a0f463afe6c90d15" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877429/">Benchmark for Kitchen20, a daily life dataset for audio-based human action recognition</a><br></div>`)[0];
            popup_53b35dcb734e44d0bcb9433bd6dfc68a.setContent(html_920922ae01414cb9a0f463afe6c90d15);
        

        circle_marker_8de0de84efb2481b91b336f90bbab340.bindPopup(popup_53b35dcb734e44d0bcb9433bd6dfc68a)
        ;

        
    
    
            var circle_marker_b56b1e1af33f4866a643f60b422e2662 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_11972767b98e4d98af1337ff3be6c833 = L.popup({"maxWidth": "100%"});

        
            var html_e70a965c817441779c2ed18dfc00c16e = $(`<div id="html_e70a965c817441779c2ed18dfc00c16e" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8852143/">Cosine-similarity penalty to discriminate sound classes in weakly-supervised sound event detection</a><br></div>`)[0];
            popup_11972767b98e4d98af1337ff3be6c833.setContent(html_e70a965c817441779c2ed18dfc00c16e);
        

        circle_marker_b56b1e1af33f4866a643f60b422e2662.bindPopup(popup_11972767b98e4d98af1337ff3be6c833)
        ;

        
    
    
            var circle_marker_389a191d4d5d439d8faf9bc4f52d7395 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_980dc3da590441e9a22b05cfd57a3032 = L.popup({"maxWidth": "100%"});

        
            var html_81d9e7ced26e4644bcd8f00af9d5661f = $(`<div id="html_81d9e7ced26e4644bcd8f00af9d5661f" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://arxiv.org/abs/1810.01807">Disambiguating music artists at scale with audio metric learning</a><br></div>`)[0];
            popup_980dc3da590441e9a22b05cfd57a3032.setContent(html_81d9e7ced26e4644bcd8f00af9d5661f);
        

        circle_marker_389a191d4d5d439d8faf9bc4f52d7395.bindPopup(popup_980dc3da590441e9a22b05cfd57a3032)
        ;

        
    
    
            var circle_marker_e60a5d2029b14312b604e217703300f2 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f6b89df2725541199766f59385844cbc = L.popup({"maxWidth": "100%"});

        
            var html_6bae1eab467d45eda72ac00b16335b37 = $(`<div id="html_6bae1eab467d45eda72ac00b16335b37" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8937143/">Evaluation of post-processing algorithms for polyphonic sound event detection</a><br></div>`)[0];
            popup_f6b89df2725541199766f59385844cbc.setContent(html_6bae1eab467d45eda72ac00b16335b37);
        

        circle_marker_e60a5d2029b14312b604e217703300f2.bindPopup(popup_f6b89df2725541199766f59385844cbc)
        ;

        
    
    
            var circle_marker_f97e52850d254eab99feeacce2cbfc58 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e5681cecd465419eaf7ef12203798024 = L.popup({"maxWidth": "100%"});

        
            var html_813e6262307d45459d0e2ac4d903eab6 = $(`<div id="html_813e6262307d45459d0e2ac4d903eab6" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="http://www-poleia.lip6.fr/~cord/pdfs/publis/2018dcasecord.pdf">Exploring deep vision models for acoustic scene classification</a><br></div>`)[0];
            popup_e5681cecd465419eaf7ef12203798024.setContent(html_813e6262307d45459d0e2ac4d903eab6);
        

        circle_marker_f97e52850d254eab99feeacce2cbfc58.bindPopup(popup_e5681cecd465419eaf7ef12203798024)
        ;

        
    
    
            var circle_marker_5a441ebdf09f4d3a948443401ae13214 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bec92271586049a7aafca675e8df3c46 = L.popup({"maxWidth": "100%"});

        
            var html_60bcf7cbf1204f12b4dbc6705986f55b = $(`<div id="html_60bcf7cbf1204f12b4dbc6705986f55b" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933045/">Feature learning with matrix factorization applied to acoustic scene classification</a><br></div>`)[0];
            popup_bec92271586049a7aafca675e8df3c46.setContent(html_60bcf7cbf1204f12b4dbc6705986f55b);
        

        circle_marker_5a441ebdf09f4d3a948443401ae13214.bindPopup(popup_bec92271586049a7aafca675e8df3c46)
        ;

        
    
    
            var circle_marker_c6dde498e9b64e5185a8aab8cbe10854 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_caec1f2a82204b26a82016c33390c395 = L.popup({"maxWidth": "100%"});

        
            var html_e8f8f9168ac64910871c17a234532127 = $(`<div id="html_e8f8f9168ac64910871c17a234532127" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8168139/">Leveraging deep neural networks with nonnegative representations for improved environmental sound classification</a><br></div>`)[0];
            popup_caec1f2a82204b26a82016c33390c395.setContent(html_e8f8f9168ac64910871c17a234532127);
        

        circle_marker_c6dde498e9b64e5185a8aab8cbe10854.bindPopup(popup_caec1f2a82204b26a82016c33390c395)
        ;

        
    
    
            var circle_marker_1f4ef16867e24147bf7608f3dbbc943e = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_086fda260a0f425f9d2df6197578cf23 = L.popup({"maxWidth": "100%"});

        
            var html_17e8c9ff8f014fb0a36d40c7b5be535d = $(`<div id="html_17e8c9ff8f014fb0a36d40c7b5be535d" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Cances_69.pdf">Multi task learning and post processing optimization for sound event detection</a><br></div>`)[0];
            popup_086fda260a0f425f9d2df6197578cf23.setContent(html_17e8c9ff8f014fb0a36d40c7b5be535d);
        

        circle_marker_1f4ef16867e24147bf7608f3dbbc943e.bindPopup(popup_086fda260a0f425f9d2df6197578cf23)
        ;

        
    
    
            var circle_marker_afb795367775437cbf3a2981c98ff56c = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_500e5b0d6e4341ff9795e3aa458bf974 = L.popup({"maxWidth": "100%"});

        
            var html_baa3e5037106416894c9af4d05b853f8 = $(`<div id="html_baa3e5037106416894c9af4d05b853f8" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://hal.inria.fr/hal-01636627/">Nonnegative feature learning methods for acoustic scene classification</a><br></div>`)[0];
            popup_500e5b0d6e4341ff9795e3aa458bf974.setContent(html_baa3e5037106416894c9af4d05b853f8);
        

        circle_marker_afb795367775437cbf3a2981c98ff56c.bindPopup(popup_500e5b0d6e4341ff9795e3aa458bf974)
        ;

        
    
    
            var circle_marker_4fabe40a5c0148d9b69505217ffc941a = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_21eb1a73af2440178fd57e9436879cbe = L.popup({"maxWidth": "100%"});

        
            var html_bcfd76cd67f8480d862fc1f5eb31ddca = $(`<div id="html_bcfd76cd67f8480d862fc1f5eb31ddca" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://hal.telecom-paris.fr/hal-02934433/document">SHOULD WE CONSIDER THE USERS IN CONTEXTUAL MUSIC AUTO-TAGGING MODELS?</a><br></div>`)[0];
            popup_21eb1a73af2440178fd57e9436879cbe.setContent(html_bcfd76cd67f8480d862fc1f5eb31ddca);
        

        circle_marker_4fabe40a5c0148d9b69505217ffc941a.bindPopup(popup_21eb1a73af2440178fd57e9436879cbe)
        ;

        
    
    
            var circle_marker_69addfa2b7884636a51b74bff2199ea1 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_61c33c771cd6443e8a572d4d0b196b5c = L.popup({"maxWidth": "100%"});

        
            var html_846cd870c732499b93546652bce65f66 = $(`<div id="html_846cd870c732499b93546652bce65f66" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683774/">Semi-supervised triplet loss based learning of ambient audio embeddings</a><br></div>`)[0];
            popup_61c33c771cd6443e8a572d4d0b196b5c.setContent(html_846cd870c732499b93546652bce65f66);
        

        circle_marker_69addfa2b7884636a51b74bff2199ea1.bindPopup(popup_61c33c771cd6443e8a572d4d0b196b5c)
        ;

        
    
    
            var circle_marker_b9f8cd93a1774472b6ee564a1e17c394 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_652e4739554848c2ae93d52aa2c5a3c8 = L.popup({"maxWidth": "100%"});

        
            var html_033fc3ffea254d7fbe62c8c31047d495 = $(`<div id="html_033fc3ffea254d7fbe62c8c31047d495" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://hal.inria.fr/hal-02114652/">Sound event detection from partially annotated data: Trends and challenges</a><br></div>`)[0];
            popup_652e4739554848c2ae93d52aa2c5a3c8.setContent(html_033fc3ffea254d7fbe62c8c31047d495);
        

        circle_marker_b9f8cd93a1774472b6ee564a1e17c394.bindPopup(popup_652e4739554848c2ae93d52aa2c5a3c8)
        ;

        
    
    
            var circle_marker_21d67a2e4c11462c87b57a16a17eb614 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0b74e2df166643aea1d2baf66e6142bb = L.popup({"maxWidth": "100%"});

        
            var html_9f1f08f0607f424f9dcbe33292bacb30 = $(`<div id="html_9f1f08f0607f424f9dcbe33292bacb30" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://trepo.tuni.fi/bitstream/handle/10024/116599/DCASE_2018_proceedings.pdf?sequence=1#page=65">Sound event detection from weak annotations: weighted-gru versus multi-instance-learning</a><br></div>`)[0];
            popup_0b74e2df166643aea1d2baf66e6142bb.setContent(html_9f1f08f0607f424f9dcbe33292bacb30);
        

        circle_marker_21d67a2e4c11462c87b57a16a17eb614.bindPopup(popup_0b74e2df166643aea1d2baf66e6142bb)
        ;

        
    
    
            var circle_marker_bb0d5e6459f34e7aba4cf02788b320d0 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_dfc807d0e4a8462bb96dee97f0734cf2 = L.popup({"maxWidth": "100%"});

        
            var html_cd7f28758cca4a2e85a080d8c6abcb72 = $(`<div id="html_cd7f28758cca4a2e85a080d8c6abcb72" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60771">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</a><br></div>`)[0];
            popup_dfc807d0e4a8462bb96dee97f0734cf2.setContent(html_cd7f28758cca4a2e85a080d8c6abcb72);
        

        circle_marker_bb0d5e6459f34e7aba4cf02788b320d0.bindPopup(popup_dfc807d0e4a8462bb96dee97f0734cf2)
        ;

        
    
    
            var circle_marker_6472ba1e62264a88b85878270166b575 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_eeb5079308834fa4aa392fdfbc9846a6 = L.popup({"maxWidth": "100%"});

        
            var html_94b2683880e64c01bcc022fe32c788b5 = $(`<div id="html_94b2683880e64c01bcc022fe32c788b5" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9054478/">Sound event detection in synthetic domestic environments</a><br></div>`)[0];
            popup_eeb5079308834fa4aa392fdfbc9846a6.setContent(html_94b2683880e64c01bcc022fe32c788b5);
        

        circle_marker_6472ba1e62264a88b85878270166b575.bindPopup(popup_eeb5079308834fa4aa392fdfbc9846a6)
        ;

        
    
    
            var circle_marker_8806a990188d47dc9092e7534c87d1d3 = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_27c3840f11584d89958e133a39c305a4 = L.popup({"maxWidth": "100%"});

        
            var html_697eb663dfa345bb821e2183855e8cf3 = $(`<div id="html_697eb663dfa345bb821e2183855e8cf3" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://arxiv.org/abs/2007.03931">Training sound event detection on a heterogeneous dataset</a><br></div>`)[0];
            popup_27c3840f11584d89958e133a39c305a4.setContent(html_697eb663dfa345bb821e2183855e8cf3);
        

        circle_marker_8806a990188d47dc9092e7534c87d1d3.bindPopup(popup_27c3840f11584d89958e133a39c305a4)
        ;

        
    
    
            var circle_marker_221e936fcdc44bb29d953a366ecff81f = L.circleMarker(
                [46.603354, 1.8883335],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f466a3da63a544dd8d611c5a62ba8fb0 = L.popup({"maxWidth": "100%"});

        
            var html_599ea0b469384ea0a71d381e777fa44a = $(`<div id="html_599ea0b469384ea0a71d381e777fa44a" style="width: 100.0%; height: 100.0%;">Country : France<br>                         Paper : <a href="https://opus.bibliothek.uni-augsburg.de/opus4/files/45063/DCASE_2017+-+Sequence.pdf">Workshop (DCASE2017)</a><br></div>`)[0];
            popup_f466a3da63a544dd8d611c5a62ba8fb0.setContent(html_599ea0b469384ea0a71d381e777fa44a);
        

        circle_marker_221e936fcdc44bb29d953a366ecff81f.bindPopup(popup_f466a3da63a544dd8d611c5a62ba8fb0)
        ;

        
    
    
            var circle_marker_4784b2c203e64c189d1b1b6e16886855 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cf33167a9d164867920d93c28dd7ca74 = L.popup({"maxWidth": "100%"});

        
            var html_586e0262b0d24246b140e9b74a441d0c = $(`<div id="html_586e0262b0d24246b140e9b74a441d0c" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="http://www.academia.edu/download/62174595/ECAI_RAI_TCL20200223-31870-1j44ahp.pdf">A Knowledge-based System for the Dynamic Generation and Classification of Novel Contents in Multimedia Broadcasting</a><br></div>`)[0];
            popup_cf33167a9d164867920d93c28dd7ca74.setContent(html_586e0262b0d24246b140e9b74a441d0c);
        

        circle_marker_4784b2c203e64c189d1b1b6e16886855.bindPopup(popup_cf33167a9d164867920d93c28dd7ca74)
        ;

        
    
    
            var circle_marker_d81d442934fb49ebabce2fef7d6feb65 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7b2cb6889fee46c0b8bde5e057f017d3 = L.popup({"maxWidth": "100%"});

        
            var html_58bea119565546638c630f529a1e4e28 = $(`<div id="html_58bea119565546638c630f529a1e4e28" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.researchgate.net/profile/Hossein_A_Rahmani/publication/337768473_A_Regression_Approach_to_Movie_Rating_Prediction_using_Multimedia_Content_and_Metadata/links/5df8b821a6fdcc283726d8c7/A-Regression-Approach-to-Movie-Rating-Prediction-using-Multimedia-Content-and-Metadata.pdf?_sg%5B0%5D=3b-miF1_LzXGwxQhQxRYO5FfklSPn0YfAlrK6IWfJilrpkC8-9h2ljnRbzLhZjp8z22CM_vzJpiQ-DaxhD7RLQ.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_sg%5B1%5D=0EyWPCbfTe4cM2OzZ6VGxzinAQj21QkakmKNQ9xBAXR2oXBv7SQahoq94YN7zTBLR7KuEuKMzCqZtGzBOqJvKV6rX2V1E4ISvHGuBwdOwd-p.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_iepl=">A Regression Approach to Movie Rating Prediction using Multimedia Content and Metadata</a><br></div>`)[0];
            popup_7b2cb6889fee46c0b8bde5e057f017d3.setContent(html_58bea119565546638c630f529a1e4e28);
        

        circle_marker_d81d442934fb49ebabce2fef7d6feb65.bindPopup(popup_7b2cb6889fee46c0b8bde5e057f017d3)
        ;

        
    
    
            var circle_marker_583f1e0af3e44561abcc6847a75c53ca = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3641ec400a1e4a4897c4f159af32d113 = L.popup({"maxWidth": "100%"});

        
            var html_78367f024eba4852b5c382abcfd40b47 = $(`<div id="html_78367f024eba4852b5c382abcfd40b47" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7966035/">A convolutional neural network approach for acoustic scene classification</a><br></div>`)[0];
            popup_3641ec400a1e4a4897c4f159af32d113.setContent(html_78367f024eba4852b5c382abcfd40b47);
        

        circle_marker_583f1e0af3e44561abcc6847a75c53ca.bindPopup(popup_3641ec400a1e4a4897c4f159af32d113)
        ;

        
    
    
            var circle_marker_e867865410324ff8ad6bdef0bd33d33a = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3118a6269af947bfb89b12258b8dac9f = L.popup({"maxWidth": "100%"});

        
            var html_587df451e6ee47f28c7a16f5eb53604b = $(`<div id="html_587df451e6ee47f28c7a16f5eb53604b" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/2005.10322">Adversarial Machine Learning in Recommender Systems: State of the art and Challenges</a><br></div>`)[0];
            popup_3118a6269af947bfb89b12258b8dac9f.setContent(html_587df451e6ee47f28c7a16f5eb53604b);
        

        circle_marker_e867865410324ff8ad6bdef0bd33d33a.bindPopup(popup_3118a6269af947bfb89b12258b8dac9f)
        ;

        
    
    
            var circle_marker_3da23b23fa93477db828cc28e844676a = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1551d2820afb4094805caa0a0dedecb5 = L.popup({"maxWidth": "100%"});

        
            var html_5806ac867f2a4503a9e43b4690ac308c = $(`<div id="html_5806ac867f2a4503a9e43b4690ac308c" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3336191.3371877">Adversarial machine learning in recommender systems (aml-recsys)</a><br></div>`)[0];
            popup_1551d2820afb4094805caa0a0dedecb5.setContent(html_5806ac867f2a4503a9e43b4690ac308c);
        

        circle_marker_3da23b23fa93477db828cc28e844676a.bindPopup(popup_1551d2820afb4094805caa0a0dedecb5)
        ;

        
    
    
            var circle_marker_a1bad6baaf254c7a8bb45b1e9623c73b = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_13f461b9f9344281bcc4fef39116e6db = L.popup({"maxWidth": "100%"});

        
            var html_ab57057f36014649b5cd6d76e6210e37 = $(`<div id="html_ab57057f36014649b5cd6d76e6210e37" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/1908.07968">Assessing the impact of a user-item collaborative attack on class of users</a><br></div>`)[0];
            popup_13f461b9f9344281bcc4fef39116e6db.setContent(html_ab57057f36014649b5cd6d76e6210e37);
        

        circle_marker_a1bad6baaf254c7a8bb45b1e9623c73b.bindPopup(popup_13f461b9f9344281bcc4fef39116e6db)
        ;

        
    
    
            var circle_marker_92dac7ddb59c43ed856bfd024f6c5191 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_af4ca5f0059f4b429d1aacef6349d4fa = L.popup({"maxWidth": "100%"});

        
            var html_bde964fe48d04402b9f34015992aebd1 = $(`<div id="html_bde964fe48d04402b9f34015992aebd1" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3240407">Audio-visual encoding of multimedia content for enhancing movie recommendations</a><br></div>`)[0];
            popup_af4ca5f0059f4b429d1aacef6349d4fa.setContent(html_bde964fe48d04402b9f34015992aebd1);
        

        circle_marker_92dac7ddb59c43ed856bfd024f6c5191.bindPopup(popup_af4ca5f0059f4b429d1aacef6349d4fa)
        ;

        
    
    
            var circle_marker_5ce41d8c6f7f4b99852a7d46e44e5a22 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_73bf5cac54de43c5ab392523098a9892 = L.popup({"maxWidth": "100%"});

        
            var html_7631d826a8e64d23b354bf0a12fb59bc = $(`<div id="html_7631d826a8e64d23b354bf0a12fb59bc" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8903002/">Automatic playlist generation using Convolutional Neural Networks and Recurrent Neural Networks</a><br></div>`)[0];
            popup_73bf5cac54de43c5ab392523098a9892.setContent(html_7631d826a8e64d23b354bf0a12fb59bc);
        

        circle_marker_5ce41d8c6f7f4b99852a7d46e44e5a22.bindPopup(popup_73bf5cac54de43c5ab392523098a9892)
        ;

        
    
    
            var circle_marker_8c586949dcc44b84a13fbffe2e09a0a1 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_eff438a9767b4cca8fc3b2c37477d520 = L.popup({"maxWidth": "100%"});

        
            var html_97129e45f9404988bff7ee1642427d27 = $(`<div id="html_97129e45f9404988bff7ee1642427d27" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s13735-018-0154-2">Current challenges and visions in music recommender systems research</a><br></div>`)[0];
            popup_eff438a9767b4cca8fc3b2c37477d520.setContent(html_97129e45f9404988bff7ee1642427d27);
        

        circle_marker_8c586949dcc44b84a13fbffe2e09a0a1.bindPopup(popup_eff438a9767b4cca8fc3b2c37477d520)
        ;

        
    
    
            var circle_marker_543ada13f717445783601f67e441f8e8 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3599a39b57f7465c818b64cb4fa78863 = L.popup({"maxWidth": "100%"});

        
            var html_bbf03328392e40d3a474c43aa32dea0c = $(`<div id="html_bbf03328392e40d3a474c43aa32dea0c" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3267471.3267486">Efficient similarity based methods for the playlist continuation task</a><br></div>`)[0];
            popup_3599a39b57f7465c818b64cb4fa78863.setContent(html_bbf03328392e40d3a474c43aa32dea0c);
        

        circle_marker_543ada13f717445783601f67e441f8e8.bindPopup(popup_3599a39b57f7465c818b64cb4fa78863)
        ;

        
    
    
            var circle_marker_bd7c8f32fc214d40932734744153f5e8 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f1fd28e1f7fb46a98776f8b44039b439 = L.popup({"maxWidth": "100%"});

        
            var html_2018ec923b5f4a3e8cef5d96912612c3 = $(`<div id="html_2018ec923b5f4a3e8cef5d96912612c3" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://library.oapen.org/bitstream/handle/20.500.12657/23079/1007079.pdf?sequence=1#page=84">Enhancing Video Recommendation Using Multimedia Content</a><br></div>`)[0];
            popup_f1fd28e1f7fb46a98776f8b44039b439.setContent(html_2018ec923b5f4a3e8cef5d96912612c3);
        

        circle_marker_bd7c8f32fc214d40932734744153f5e8.bindPopup(popup_f1fd28e1f7fb46a98776f8b44039b439)
        ;

        
    
    
            var circle_marker_716d5062d8c64e89bfc49b7d2aacc3dc = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_926619f25f044b9d83618e4b9d797c88 = L.popup({"maxWidth": "100%"});

        
            var html_ef99ef9e455749d4a218c465089e8291 = $(`<div id="html_ef99ef9e455749d4a218c465089e8291" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="http://sisinflab.poliba.it/publications/2020/ADDMAD20/SEBD2020___Knowledge_enhanced_Shilling_Attacks_for_recommendation.pdf">Knowledge-enhanced Shilling Attacks for Recommendation⋆</a><br></div>`)[0];
            popup_926619f25f044b9d83618e4b9d797c88.setContent(html_ef99ef9e455749d4a218c465089e8291);
        

        circle_marker_716d5062d8c64e89bfc49b7d2aacc3dc.bindPopup(popup_926619f25f044b9d83618e4b9d797c88)
        ;

        
    
    
            var circle_marker_f9abc06927f14ddfa78a433e75839145 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cd207c1f35744b3ea1829dcaf006e904 = L.popup({"maxWidth": "100%"});

        
            var html_3b15521df8e34e8fb79f1a9cb684cac9 = $(`<div id="html_3b15521df8e34e8fb79f1a9cb684cac9" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3359555.3359563">Leveraging laziness, browsing-pattern aware stacked models for sequential accommodation learning to rank</a><br></div>`)[0];
            popup_cd207c1f35744b3ea1829dcaf006e904.setContent(html_3b15521df8e34e8fb79f1a9cb684cac9);
        

        circle_marker_f9abc06927f14ddfa78a433e75839145.bindPopup(popup_cd207c1f35744b3ea1829dcaf006e904)
        ;

        
    
    
            var circle_marker_32b967c6ab2544a390a8989b64f96f22 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_657792c4f55241f6881c28964850f8ec = L.popup({"maxWidth": "100%"});

        
            var html_0dbfadd06c1f4d3a8c50251fa25ca1ed = $(`<div id="html_0dbfadd06c1f4d3a8c50251fa25ca1ed" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3204949.3208141">MMTF-14K: a multifaceted movie trailer feature dataset for recommendation and retrieval</a><br></div>`)[0];
            popup_657792c4f55241f6881c28964850f8ec.setContent(html_0dbfadd06c1f4d3a8c50251fa25ca1ed);
        

        circle_marker_32b967c6ab2544a390a8989b64f96f22.bindPopup(popup_657792c4f55241f6881c28964850f8ec)
        ;

        
    
    
            var circle_marker_0af43f6270d840d792179b30172cf2c7 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ab6d04e2e84e49aaa185591db31a9c5d = L.popup({"maxWidth": "100%"});

        
            var html_90a667a44b8c42ebb67aa081441717cb = $(`<div id="html_90a667a44b8c42ebb67aa081441717cb" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="">Movie Rating Prediction Using Multimedia Content and Modeling as a Classification Problem.</a><br></div>`)[0];
            popup_ab6d04e2e84e49aaa185591db31a9c5d.setContent(html_90a667a44b8c42ebb67aa081441717cb);
        

        circle_marker_0af43f6270d840d792179b30172cf2c7.bindPopup(popup_ab6d04e2e84e49aaa185591db31a9c5d)
        ;

        
    
    
            var circle_marker_7069024a37944ab28fce6e2d7d94cf50 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bcccf0bd694c4d8a8ef88ed7a09512fa = L.popup({"maxWidth": "100%"});

        
            var html_09958b155b2a4d29b588ecb372321863 = $(`<div id="html_09958b155b2a4d29b588ecb372321863" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877452/">Movie genome recommender: A novel recommender system based on multimedia content</a><br></div>`)[0];
            popup_bcccf0bd694c4d8a8ef88ed7a09512fa.setContent(html_09958b155b2a4d29b588ecb372321863);
        

        circle_marker_7069024a37944ab28fce6e2d7d94cf50.bindPopup(popup_bcccf0bd694c4d8a8ef88ed7a09512fa)
        ;

        
    
    
            var circle_marker_a8a8dfebdbe24610b8d19c969ec7031d = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_faf1d15cc2074784a1e1dfdabd125aab = L.popup({"maxWidth": "100%"});

        
            var html_90411a013d8f462692fbf25b1dbcc81b = $(`<div id="html_90411a013d8f462692fbf25b1dbcc81b" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09221-y">Movie genome: alleviating new item cold start in movie recommendation</a><br></div>`)[0];
            popup_faf1d15cc2074784a1e1dfdabd125aab.setContent(html_90411a013d8f462692fbf25b1dbcc81b);
        

        circle_marker_a8a8dfebdbe24610b8d19c969ec7031d.bindPopup(popup_faf1d15cc2074784a1e1dfdabd125aab)
        ;

        
    
    
            var circle_marker_45b2117cffb7422eb21261f4a2c3274d = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_616beb0aaa484238b9bf6b494d73d6d6 = L.popup({"maxWidth": "100%"});

        
            var html_f3c1ad90ca90489fa4bf305c66fc84ae = $(`<div id="html_f3c1ad90ca90489fa4bf305c66fc84ae" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3241620">Multimedia recommender systems</a><br></div>`)[0];
            popup_616beb0aaa484238b9bf6b494d73d6d6.setContent(html_f3c1ad90ca90489fa4bf305c66fc84ae);
        

        circle_marker_45b2117cffb7422eb21261f4a2c3274d.bindPopup(popup_616beb0aaa484238b9bf6b494d73d6d6)
        ;

        
    
    
            var circle_marker_b5647cd2ff7d42b9aad2294ec079c449 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_dd4e65b2541b43afa311f1b6e82b808c = L.popup({"maxWidth": "100%"});

        
            var html_351dce2647654374b39ec6401c9dba1b = $(`<div id="html_351dce2647654374b39ec6401c9dba1b" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3301275.3302266">Prediction of music pairwise preferences from facial expressions</a><br></div>`)[0];
            popup_dd4e65b2541b43afa311f1b6e82b808c.setContent(html_351dce2647654374b39ec6401c9dba1b);
        

        circle_marker_b5647cd2ff7d42b9aad2294ec079c449.bindPopup(popup_dd4e65b2541b43afa311f1b6e82b808c)
        ;

        
    
    
            var circle_marker_ccae78a8b1b9472b98b04316018d5416 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fcdbb656f539483cac033cdddd2d752d = L.popup({"maxWidth": "100%"});

        
            var html_57294a5d34ec441587e06c8bacbabc68 = $(`<div id="html_57294a5d34ec441587e06c8bacbabc68" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/1908.06708">Recommender Systems Fairness Evaluation via Generalized Cross Entropy</a><br></div>`)[0];
            popup_fcdbb656f539483cac033cdddd2d752d.setContent(html_57294a5d34ec441587e06c8bacbabc68);
        

        circle_marker_ccae78a8b1b9472b98b04316018d5416.bindPopup(popup_fcdbb656f539483cac033cdddd2d752d)
        ;

        
    
    
            var circle_marker_d4eb97f2789841fc8a960e52df9e8349 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_29049498736b4c40a87c74868f27c007 = L.popup({"maxWidth": "100%"});

        
            var html_10bf9b9f6e514150b6ddfa574f31f32f = $(`<div id="html_10bf9b9f6e514150b6ddfa574f31f32f" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/342211693_Recommender_Systems_Leveraging_Multimedia_Content/links/5f1a654d45851515ef44cb80/Recommender-Systems-Leveraging-Multimedia-Content.pdf">Recommender systems leveraging multimedia content</a><br></div>`)[0];
            popup_29049498736b4c40a87c74868f27c007.setContent(html_10bf9b9f6e514150b6ddfa574f31f32f);
        

        circle_marker_d4eb97f2789841fc8a960e52df9e8349.bindPopup(popup_29049498736b4c40a87c74868f27c007)
        ;

        
    
    
            var circle_marker_8524eb1e1f47457189644a0e7ad24481 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d505bfa198074bdd9df8b3b31ed7a2af = L.popup({"maxWidth": "100%"});

        
            var html_3f74a3a5239f400daed4d914c2d0b722 = $(`<div id="html_3f74a3a5239f400daed4d914c2d0b722" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877420/">Retrieving relevant and diverse movie clips using the mfvcd-7k multifaceted video clip dataset</a><br></div>`)[0];
            popup_d505bfa198074bdd9df8b3b31ed7a2af.setContent(html_3f74a3a5239f400daed4d914c2d0b722);
        

        circle_marker_8524eb1e1f47457189644a0e7ad24481.bindPopup(popup_d505bfa198074bdd9df8b3b31ed7a2af)
        ;

        
    
    
            var circle_marker_494259eacba54a46b02d740df4337e97 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3bcb1efcf87d41a89d010dd9742707f6 = L.popup({"maxWidth": "100%"});

        
            var html_48e586f09a464a3db669002789ac9db4 = $(`<div id="html_48e586f09a464a3db669002789ac9db4" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="http://www-ictserv.poliba.it/publications/2020/ADDDM20/2020_Anelli_ESWC2020.pdf">SAShA: Semantic-Aware Shilling Attacks on Recommender Systems Exploiting Knowledge Graphs</a><br></div>`)[0];
            popup_3bcb1efcf87d41a89d010dd9742707f6.setContent(html_48e586f09a464a3db669002789ac9db4);
        

        circle_marker_494259eacba54a46b02d740df4337e97.bindPopup(popup_3bcb1efcf87d41a89d010dd9742707f6)
        ;

        
    
    
            var circle_marker_38ac774d14e84e14a950a37172ca999a = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cd4877c8c39a4622859d72de01707e18 = L.popup({"maxWidth": "100%"});

        
            var html_5669f947bd7f446bb01c8a4b9e761cae = $(`<div id="html_5669f947bd7f446bb01c8a4b9e761cae" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-49461-2_18">Sasha: Semantic-aware shilling attacks on recommender systems exploiting knowledge graphs</a><br></div>`)[0];
            popup_cd4877c8c39a4622859d72de01707e18.setContent(html_5669f947bd7f446bb01c8a4b9e761cae);
        

        circle_marker_38ac774d14e84e14a950a37172ca999a.bindPopup(popup_cd4877c8c39a4622859d72de01707e18)
        ;

        
    
    
            var circle_marker_fa61ff10f1c94d72b6810543ea0f8de8 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_89cca3a24241418f83b0361ea9c0f7e5 = L.popup({"maxWidth": "100%"});

        
            var html_be9dddcb209c41338949850c75537fd3 = $(`<div id="html_be9dddcb209c41338949850c75537fd3" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/334548980_The_2019_Multimedia_for_Recommender_System_Task_MovieREC_and_NewsREEL_at_MediaEval/links/5d30c3ac299bf1547cc258c1/The-2019-Multimedia-for-Recommender-System-Task-MovieREC-and-NewsREEL-at-MediaEval.pdf">The 2019 Multimedia for Recommender System Task: MovieREC and NewsREEL at MediaEval</a><br></div>`)[0];
            popup_89cca3a24241418f83b0361ea9c0f7e5.setContent(html_be9dddcb209c41338949850c75537fd3);
        

        circle_marker_fa61ff10f1c94d72b6810543ea0f8de8.bindPopup(popup_89cca3a24241418f83b0361ea9c0f7e5)
        ;

        
    
    
            var circle_marker_1b79ace37d3d48399d7a5aab82a73d91 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_956d74f706b24de1b1872cf3bda0cedb = L.popup({"maxWidth": "100%"});

        
            var html_ac238e7bf69643379d29226af2b3347a = $(`<div id="html_ac238e7bf69643379d29226af2b3347a" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.jku.at/fileadmin/gruppen/173/Research/deldjoo_mediaeval_2018.pdf">The MediaEval 2018 Movie Recommendation Task: Recommending Movies Using Content.</a><br></div>`)[0];
            popup_956d74f706b24de1b1872cf3bda0cedb.setContent(html_ac238e7bf69643379d29226af2b3347a);
        

        circle_marker_1b79ace37d3d48399d7a5aab82a73d91.bindPopup(popup_956d74f706b24de1b1872cf3bda0cedb)
        ;

        
    
    
            var circle_marker_ae48620468d540f5a61022c10da9838a = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ee57965b2b744b98b0ee5186dd833b90 = L.popup({"maxWidth": "100%"});

        
            var html_4941cee8382f49ad8a665413f01b06a4 = $(`<div id="html_4941cee8382f49ad8a665413f01b06a4" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/1908.11055">Towards Evaluating User Profiling Methods Based on Explicit Ratings on Item Features</a><br></div>`)[0];
            popup_ee57965b2b744b98b0ee5186dd833b90.setContent(html_4941cee8382f49ad8a665413f01b06a4);
        

        circle_marker_ae48620468d540f5a61022c10da9838a.bindPopup(popup_ee57965b2b744b98b0ee5186dd833b90)
        ;

        
    
    
            var circle_marker_a9790fb477e04523a53063af92010db1 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b4698304760743e6a923b22ec88b0333 = L.popup({"maxWidth": "100%"});

        
            var html_f5295907993742f49557764db65742bf = $(`<div id="html_f5295907993742f49557764db65742bf" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09231-w">Trends in content-based recommendation</a><br></div>`)[0];
            popup_b4698304760743e6a923b22ec88b0333.setContent(html_f5295907993742f49557764db65742bf);
        

        circle_marker_a9790fb477e04523a53063af92010db1.bindPopup(popup_b4698304760743e6a923b22ec88b0333)
        ;

        
    
    
            var circle_marker_e689c573029a46b3afcaeeb22d677797 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5c7ba36639d74a20ac72b51658d9aa95 = L.popup({"maxWidth": "100%"});

        
            var html_b9726c8babb04c82b386fb4969a63a4d = $(`<div id="html_b9726c8babb04c82b386fb4969a63a4d" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://arxiv.org/abs/2005.00145">Unsupervised Domain Adaptation for Acoustic Scene Classification Using Band-Wise Statistics Matching</a><br></div>`)[0];
            popup_5c7ba36639d74a20ac72b51658d9aa95.setContent(html_b9726c8babb04c82b386fb4969a63a4d);
        

        circle_marker_e689c573029a46b3afcaeeb22d677797.bindPopup(popup_5c7ba36639d74a20ac72b51658d9aa95)
        ;

        
    
    
            var circle_marker_dbcc2b48641449e0b9ec0dcdf54f9019 = L.circleMarker(
                [42.6384261, 12.674297],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0d02d03e8fa84ff38895bf9a34cd6b11 = L.popup({"maxWidth": "100%"});

        
            var html_6a4719f762e141fd9590fe6cce515be2 = $(`<div id="html_6a4719f762e141fd9590fe6cce515be2" style="width: 100.0%; height: 100.0%;">Country : Italy<br>                         Paper : <a href="https://www.politesi.polimi.it/handle/10589/141256">Video recommendation by exploiting the multimedia content</a><br></div>`)[0];
            popup_0d02d03e8fa84ff38895bf9a34cd6b11.setContent(html_6a4719f762e141fd9590fe6cce515be2);
        

        circle_marker_dbcc2b48641449e0b9ec0dcdf54f9019.bindPopup(popup_0d02d03e8fa84ff38895bf9a34cd6b11)
        ;

        
    
    
            var circle_marker_6b87d82bd12548099155c7c734d4dac0 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d09c3356945c42d4b5416da0acba29ed = L.popup({"maxWidth": "100%"});

        
            var html_4a7d79b095c14edc9ff96486a1f57f88 = $(`<div id="html_4a7d79b095c14edc9ff96486a1f57f88" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Hossein_A_Rahmani/publication/337768473_A_Regression_Approach_to_Movie_Rating_Prediction_using_Multimedia_Content_and_Metadata/links/5df8b821a6fdcc283726d8c7/A-Regression-Approach-to-Movie-Rating-Prediction-using-Multimedia-Content-and-Metadata.pdf?_sg%5B0%5D=3b-miF1_LzXGwxQhQxRYO5FfklSPn0YfAlrK6IWfJilrpkC8-9h2ljnRbzLhZjp8z22CM_vzJpiQ-DaxhD7RLQ.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_sg%5B1%5D=0EyWPCbfTe4cM2OzZ6VGxzinAQj21QkakmKNQ9xBAXR2oXBv7SQahoq94YN7zTBLR7KuEuKMzCqZtGzBOqJvKV6rX2V1E4ISvHGuBwdOwd-p.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_iepl=">A Regression Approach to Movie Rating Prediction using Multimedia Content and Metadata</a><br></div>`)[0];
            popup_d09c3356945c42d4b5416da0acba29ed.setContent(html_4a7d79b095c14edc9ff96486a1f57f88);
        

        circle_marker_6b87d82bd12548099155c7c734d4dac0.bindPopup(popup_d09c3356945c42d4b5416da0acba29ed)
        ;

        
    
    
            var circle_marker_c84850f19c6142f6bf1f543357d3c773 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cff0b91359cc43ac95ad3362474a6af2 = L.popup({"maxWidth": "100%"});

        
            var html_81a12c298eff4644861417367e22a50a = $(`<div id="html_81a12c298eff4644861417367e22a50a" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dafx16.vutbr.cz/dafxpapers/09-DAFx-16_paper_37-PN.pdf">A cosine-distance based neural network for music artist recognition using raw i-vector features</a><br></div>`)[0];
            popup_cff0b91359cc43ac95ad3362474a6af2.setContent(html_81a12c298eff4644861417367e22a50a);
        

        circle_marker_c84850f19c6142f6bf1f543357d3c773.bindPopup(popup_cff0b91359cc43ac95ad3362474a6af2)
        ;

        
    
    
            var circle_marker_15e2149c62a7477698ad5b5cc763db84 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_263e0e286b484e68b5b51f44a65022a1 = L.popup({"maxWidth": "100%"});

        
            var html_25758a7418e6436fa159d97a7db7328f = $(`<div id="html_25758a7418e6436fa159d97a7db7328f" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3167132.3167280">A hybrid approach to music playlist continuation based on playlist-song membership</a><br></div>`)[0];
            popup_263e0e286b484e68b5b51f44a65022a1.setContent(html_25758a7418e6436fa159d97a7db7328f);
        

        circle_marker_15e2149c62a7477698ad5b5cc763db84.bindPopup(popup_263e0e286b484e68b5b51f44a65022a1)
        ;

        
    
    
            var circle_marker_ed7ca4e8b7084a5e86f034f568c19a92 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_26d35eb553e5466ba385c547b0362016 = L.popup({"maxWidth": "100%"});

        
            var html_47e48af4825043659c97db5a940a32ab = $(`<div id="html_47e48af4825043659c97db5a940a32ab" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8081711/">A hybrid approach with multi-channel i-vectors and convolutional neural networks for acoustic scene classification</a><br></div>`)[0];
            popup_26d35eb553e5466ba385c547b0362016.setContent(html_47e48af4825043659c97db5a940a32ab);
        

        circle_marker_ed7ca4e8b7084a5e86f034f568c19a92.bindPopup(popup_26d35eb553e5466ba385c547b0362016)
        ;

        
    
    
            var circle_marker_026b5a7aca874b2096cdf5fb395bf071 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4c231465f3e548c88ae8c10d83fa5ce0 = L.popup({"maxWidth": "100%"});

        
            var html_1395128be0c441edb4c69f3d30f91bc0 = $(`<div id="html_1395128be0c441edb4c69f3d30f91bc0" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8999242/">Acoustic Scene Classification Using Deep Mixtures of Pre-trained Convolutional Neural Networks</a><br></div>`)[0];
            popup_4c231465f3e548c88ae8c10d83fa5ce0.setContent(html_1395128be0c441edb4c69f3d30f91bc0);
        

        circle_marker_026b5a7aca874b2096cdf5fb395bf071.bindPopup(popup_4c231465f3e548c88ae8c10d83fa5ce0)
        ;

        
    
    
            var circle_marker_43f2a0119f454d2a9c8fd22af57cd42b = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2fb18e410184418b8b428e4154b949a1 = L.popup({"maxWidth": "100%"});

        
            var html_bd2741d7376049bfa1a94441982ef4cc = $(`<div id="html_bd2741d7376049bfa1a94441982ef4cc" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053582/">Acoustic Scene Classification for Mismatched Recording Devices Using Heated-Up Softmax and Spectrum Correction</a><br></div>`)[0];
            popup_2fb18e410184418b8b428e4154b949a1.setContent(html_bd2741d7376049bfa1a94441982ef4cc);
        

        circle_marker_43f2a0119f454d2a9c8fd22af57cd42b.bindPopup(popup_2fb18e410184418b8b428e4154b949a1)
        ;

        
    
    
            var circle_marker_41b1fcc114d74ce8b399cc3a68d66093 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d56f5bd91bfb4fc1b33ac988123a457f = L.popup({"maxWidth": "100%"});

        
            var html_a34340b42c74420cb9e300a200c0a83f = $(`<div id="html_a34340b42c74420cb9e300a200c0a83f" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/3002.pdf">Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation.</a><br></div>`)[0];
            popup_d56f5bd91bfb4fc1b33ac988123a457f.setContent(html_a34340b42c74420cb9e300a200c0a83f);
        

        circle_marker_41b1fcc114d74ce8b399cc3a68d66093.bindPopup(popup_d56f5bd91bfb4fc1b33ac988123a457f)
        ;

        
    
    
            var circle_marker_7069d1c753934ff989e77c037b377b5a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b6d8bec1aa774f1d8cfbfce5178aa159 = L.popup({"maxWidth": "100%"});

        
            var html_84a5222be8e74b55b6a41945bd1c5df9 = $(`<div id="html_84a5222be8e74b55b6a41945bd1c5df9" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://trepo.tuni.fi/bitstream/handle/10024/116599/DCASE_2018_proceedings.pdf?sequence=1&isAllowed=y#page=35">Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters</a><br></div>`)[0];
            popup_b6d8bec1aa774f1d8cfbfce5178aa159.setContent(html_84a5222be8e74b55b6a41945bd1c5df9);
        

        circle_marker_7069d1c753934ff989e77c037b377b5a.bindPopup(popup_b6d8bec1aa774f1d8cfbfce5178aa159)
        ;

        
    
    
            var circle_marker_0bd39da4f4354d8990636b7528ae3c74 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b48fbc83fe12415fafdbb72b4c1bb8c7 = L.popup({"maxWidth": "100%"});

        
            var html_8a08fc1921734039ba5626f820c10dc9 = $(`<div id="html_8a08fc1921734039ba5626f820c10dc9" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="">Acoustic scene classification using deep mixture of parallel convolutional neural networks</a><br></div>`)[0];
            popup_b48fbc83fe12415fafdbb72b4c1bb8c7.setContent(html_8a08fc1921734039ba5626f820c10dc9);
        

        circle_marker_0bd39da4f4354d8990636b7528ae3c74.bindPopup(popup_b48fbc83fe12415fafdbb72b4c1bb8c7)
        ;

        
    
    
            var circle_marker_b65edb7b921d4ab2a8af198716a48d97 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_76bfc933376643fdace80a290c5cb6ca = L.popup({"maxWidth": "100%"});

        
            var html_205bfc4d7eaf4da99376f7e74cd06f86 = $(`<div id="html_205bfc4d7eaf4da99376f7e74cd06f86" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Dorfer_97.pdf">Acoustic scene classification with fully convolutional neural networks and I-vectors</a><br></div>`)[0];
            popup_76bfc933376643fdace80a290c5cb6ca.setContent(html_205bfc4d7eaf4da99376f7e74cd06f86);
        

        circle_marker_b65edb7b921d4ab2a8af198716a48d97.bindPopup(popup_76bfc933376643fdace80a290c5cb6ca)
        ;

        
    
    
            var circle_marker_f7be4dedb16d494caa4d83be3a7b5e8f = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d49c0afb6e6341a9bbf5111dd8f46ece = L.popup({"maxWidth": "100%"});

        
            var html_3d7c1ac48cc144efba7d554915037439 = $(`<div id="html_3d7c1ac48cc144efba7d554915037439" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8784816/">Acoustic scene classification with mismatched recording devices using mixture of experts layer</a><br></div>`)[0];
            popup_d49c0afb6e6341a9bbf5111dd8f46ece.setContent(html_3d7c1ac48cc144efba7d554915037439);
        

        circle_marker_f7be4dedb16d494caa4d83be3a7b5e8f.bindPopup(popup_d49c0afb6e6341a9bbf5111dd8f46ece)
        ;

        
    
    
            var circle_marker_d6e9a4db62f74291ad9e29204b123d3a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ae8fdc7f838a4305ac1378638cba8431 = L.popup({"maxWidth": "100%"});

        
            var html_2b17cf0868174f7bb2447ffd17995c26 = $(`<div id="html_2b17cf0868174f7bb2447ffd17995c26" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Bernhard_Lehner/publication/337834114_ACOUSTIC_SCENE_CLASSIFICATION_WITH_REJECT_OPTION_BASED_ON_RESNETS/links/5dee3d10299bf10bc34ce38b/ACOUSTIC-SCENE-CLASSIFICATION-WITH-REJECT-OPTION-BASED-ON-RESNETS.pdf">Acoustic scene classification with reject option based on resnets</a><br></div>`)[0];
            popup_ae8fdc7f838a4305ac1378638cba8431.setContent(html_2b17cf0868174f7bb2447ffd17995c26);
        

        circle_marker_d6e9a4db62f74291ad9e29204b123d3a.bindPopup(popup_ae8fdc7f838a4305ac1378638cba8431)
        ;

        
    
    
            var circle_marker_1931c90d0e974240b5bf4442084de1b9 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8a89237f4e9a401183369c5025be9cf8 = L.popup({"maxWidth": "100%"});

        
            var html_9ae837efc1be479f8e160b7113888c6e = $(`<div id="html_9ae837efc1be479f8e160b7113888c6e" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-57321-8_25">Applying AI in Practice: Key Challenges and Lessons Learned</a><br></div>`)[0];
            popup_8a89237f4e9a401183369c5025be9cf8.setContent(html_9ae837efc1be479f8e160b7113888c6e);
        

        circle_marker_1931c90d0e974240b5bf4442084de1b9.bindPopup(popup_8a89237f4e9a401183369c5025be9cf8)
        ;

        
    
    
            var circle_marker_7859581781d54ca885042f33ba61da8d = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_73d34a69885d49329353084b6f8287b0 = L.popup({"maxWidth": "100%"});

        
            var html_af7554d51d76402bb8b732224cb7964e = $(`<div id="html_af7554d51d76402bb8b732224cb7964e" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Paischer_37_t2.pdf">Audio tagging with convolutional neural networks trained with noisy data</a><br></div>`)[0];
            popup_73d34a69885d49329353084b6f8287b0.setContent(html_af7554d51d76402bb8b732224cb7964e);
        

        circle_marker_7859581781d54ca885042f33ba61da8d.bindPopup(popup_73d34a69885d49329353084b6f8287b0)
        ;

        
    
    
            var circle_marker_7fe6093c12424abaa0b7e95b5a245d0b = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e486bb31630d4d09be9915dedcef7efc = L.popup({"maxWidth": "100%"});

        
            var html_d73eda69ead74270a86fc480c700a241 = $(`<div id="html_d73eda69ead74270a86fc480c700a241" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3240407">Audio-visual encoding of multimedia content for enhancing movie recommendations</a><br></div>`)[0];
            popup_e486bb31630d4d09be9915dedcef7efc.setContent(html_d73eda69ead74270a86fc480c700a241);
        

        circle_marker_7fe6093c12424abaa0b7e95b5a245d0b.bindPopup(popup_e486bb31630d4d09be9915dedcef7efc)
        ;

        
    
    
            var circle_marker_27a977cf84f540f2bad83d99d54d092d = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_581b9ea295514d18acd7d805bed76065 = L.popup({"maxWidth": "100%"});

        
            var html_6a901fac76864600a6639d213957652c = $(`<div id="html_6a901fac76864600a6639d213957652c" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dbis.uibk.ac.at/sites/default/files/2019-06/paper_2.pdf">Autoencoders for Next-Track-Recommendation.</a><br></div>`)[0];
            popup_581b9ea295514d18acd7d805bed76065.setContent(html_6a901fac76864600a6639d213957652c);
        

        circle_marker_27a977cf84f540f2bad83d99d54d092d.bindPopup(popup_581b9ea295514d18acd7d805bed76065)
        ;

        
    
    
            var circle_marker_0931a4ef421e4b1aa2887888efc577bf = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_518cb61f5d1548e29b06b1104139c37e = L.popup({"maxWidth": "100%"});

        
            var html_48debf26d52349dc84d5600b1711bc46 = $(`<div id="html_48debf26d52349dc84d5600b1711bc46" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Koutini_142.pdf">CP-JKU SUBMISSIONS TO DCASE'20: LOW-COMPLEXITY CROSS-DEVICE ACOUSTIC SCENE CLASSIFICATION WITH RF-REGULARIZED CNNS</a><br></div>`)[0];
            popup_518cb61f5d1548e29b06b1104139c37e.setContent(html_48debf26d52349dc84d5600b1711bc46);
        

        circle_marker_0931a4ef421e4b1aa2887888efc577bf.bindPopup(popup_518cb61f5d1548e29b06b1104139c37e)
        ;

        
    
    
            var circle_marker_2bcaf0fb9a4747f88c627e20cb08b2dc = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_14e4e6068f5148c78972a49f5a5a99d5 = L.popup({"maxWidth": "100%"});

        
            var html_4444e5167b774f82a78d25de80fd4e93 = $(`<div id="html_4444e5167b774f82a78d25de80fd4e93" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Koutini_99_t2.pdf">CP-JKU Submissions to DCASE'19: Acoustic Scene Classification and Audio Tagging with REceptive-Field-Regularized CNNs</a><br></div>`)[0];
            popup_14e4e6068f5148c78972a49f5a5a99d5.setContent(html_4444e5167b774f82a78d25de80fd4e93);
        

        circle_marker_2bcaf0fb9a4747f88c627e20cb08b2dc.bindPopup(popup_14e4e6068f5148c78972a49f5a5a99d5)
        ;

        
    
    
            var circle_marker_d4dbb6a031124589a4e21383cab25bf4 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_754300c76fa8485dbb30aac33531e32b = L.popup({"maxWidth": "100%"});

        
            var html_c577198203c24c33a86f417594331844 = $(`<div id="html_c577198203c24c33a86f417594331844" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2016/technical_reports/DCASE2016_Eghbal-Zadeh_1028.pdf">CP-JKU submissions for DCASE-2016: A hybrid approach using binaural i-vectors and deep convolutional neural networks</a><br></div>`)[0];
            popup_754300c76fa8485dbb30aac33531e32b.setContent(html_c577198203c24c33a86f417594331844);
        

        circle_marker_d4dbb6a031124589a4e21383cab25bf4.bindPopup(popup_754300c76fa8485dbb30aac33531e32b)
        ;

        
    
    
            var circle_marker_7582b87070bb49b38412749c28a1cfce = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_45b52ffd00834567ae750db996254f04 = L.popup({"maxWidth": "100%"});

        
            var html_2ebc5c035a904cd7a85679cfa3307257 = $(`<div id="html_2ebc5c035a904cd7a85679cfa3307257" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Lehner_142.pdf">Classifying short acoustic scenes with I-vectors and CNNs: Challenges and optimisations for the 2017 DCASE ASC task</a><br></div>`)[0];
            popup_45b52ffd00834567ae750db996254f04.setContent(html_2ebc5c035a904cd7a85679cfa3307257);
        

        circle_marker_7582b87070bb49b38412749c28a1cfce.bindPopup(popup_45b52ffd00834567ae750db996254f04)
        ;

        
    
    
            var circle_marker_71569a1234c64dda90521cd07fd82613 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_31dfebef284645cd9c971b7112ab9828 = L.popup({"maxWidth": "100%"});

        
            var html_59d45a9eb48d4fd7ac547d0a6a28b46a = $(`<div id="html_59d45a9eb48d4fd7ac547d0a6a28b46a" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s13735-018-0154-2">Current challenges and visions in music recommender systems research</a><br></div>`)[0];
            popup_31dfebef284645cd9c971b7112ab9828.setContent(html_59d45a9eb48d4fd7ac547d0a6a28b46a);
        

        circle_marker_71569a1234c64dda90521cd07fd82613.bindPopup(popup_31dfebef284645cd9c971b7112ab9828)
        ;

        
    
    
            var circle_marker_51e36a005d5e4ab8b2c757d85c5d52f4 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_17a8fe8362ea40e7bb1adcff138c2f4c = L.popup({"maxWidth": "100%"});

        
            var html_5304e7e2fd044f8b97d1c60d85652217 = $(`<div id="html_5304e7e2fd044f8b97d1c60d85652217" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="">DEEP WITHIN-CLASS COVARIANCE ANALYSIS FOR ACOUSTIC SCENE CLASSIFICATION</a><br></div>`)[0];
            popup_17a8fe8362ea40e7bb1adcff138c2f4c.setContent(html_5304e7e2fd044f8b97d1c60d85652217);
        

        circle_marker_51e36a005d5e4ab8b2c757d85c5d52f4.bindPopup(popup_17a8fe8362ea40e7bb1adcff138c2f4c)
        ;

        
    
    
            var circle_marker_38ca5efddad74e8db3980e2bfd2fdcdd = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1320db9e1d6443499075e5111faed4e0 = L.popup({"maxWidth": "100%"});

        
            var html_3de7036d1bdb4090bcc035216fdadbd9 = $(`<div id="html_3de7036d1bdb4090bcc035216fdadbd9" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.frontiersin.org/articles/10.3389/fams.2019.00044/abstract">Deep Learning in Music Recommendation Systems</a><br></div>`)[0];
            popup_1320db9e1d6443499075e5111faed4e0.setContent(html_3de7036d1bdb4090bcc035216fdadbd9);
        

        circle_marker_38ca5efddad74e8db3980e2bfd2fdcdd.bindPopup(popup_1320db9e1d6443499075e5111faed4e0)
        ;

        
    
    
            var circle_marker_2946667b249949eebcef605094eceddb = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fe8283b8b83047468aa9314c49b03fa8 = L.popup({"maxWidth": "100%"});

        
            var html_278c99206c0e46228e267eeef96d3b30 = $(`<div id="html_278c99206c0e46228e267eeef96d3b30" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1806.08840">Deep snp: An end-to-end deep neural network with attention-based localization for break-point detection in snp array genomic data</a><br></div>`)[0];
            popup_fe8283b8b83047468aa9314c49b03fa8.setContent(html_278c99206c0e46228e267eeef96d3b30);
        

        circle_marker_2946667b249949eebcef605094eceddb.bindPopup(popup_fe8283b8b83047468aa9314c49b03fa8)
        ;

        
    
    
            var circle_marker_e8ec5b4a7ce04d8eb8ce855c255cb851 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b8d3e4033bf44602b334b2ac06b5192d = L.popup({"maxWidth": "100%"});

        
            var html_8053b614d4514ad2ad23c5099a46fb96 = $(`<div id="html_8053b614d4514ad2ad23c5099a46fb96" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1711.04022">Deep within-class covariance analysis for robust audio representation learning</a><br></div>`)[0];
            popup_b8d3e4033bf44602b334b2ac06b5192d.setContent(html_8053b614d4514ad2ad23c5099a46fb96);
        

        circle_marker_e8ec5b4a7ce04d8eb8ce855c255cb851.bindPopup(popup_b8d3e4033bf44602b334b2ac06b5192d)
        ;

        
    
    
            var circle_marker_cfd03f9a8c7b4d90a5245813780a06dc = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_76399f206f5641a790ec6aa122ba2762 = L.popup({"maxWidth": "100%"});

        
            var html_9039852b24f74574ae70f64c51698a51 = $(`<div id="html_9039852b24f74574ae70f64c51698a51" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.liebertpub.com/doi/abs/10.1089/cmb.2018.0172">DeepSNP: An End-to-End Deep Neural Network with Attention-Based Localization for Breakpoint Detection in Single-Nucleotide Polymorphism Array Genomic Data</a><br></div>`)[0];
            popup_76399f206f5641a790ec6aa122ba2762.setContent(html_9039852b24f74574ae70f64c51698a51);
        

        circle_marker_cfd03f9a8c7b4d90a5245813780a06dc.bindPopup(popup_76399f206f5641a790ec6aa122ba2762)
        ;

        
    
    
            var circle_marker_a15aae6820a04ed8a5ccbf83de874c20 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b93ac789cdf6417f9689a1b6d2697287 = L.popup({"maxWidth": "100%"});

        
            var html_6f5f67550336452e95cb88157f3c0613 = $(`<div id="html_6f5f67550336452e95cb88157f3c0613" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Bernhard_Lehner/publication/331223906_Detecting_the_Presence_of_Singing_Voice_in_Mixed_Music_Signals/links/5c6d205892851c1c9deee284/Detecting-the-Presence-of-Singing-Voice-in-Mixed-Music-Signals.pdf">Detecting the Presence of Singing Voice in Mixed Music Signals</a><br></div>`)[0];
            popup_b93ac789cdf6417f9689a1b6d2697287.setContent(html_6f5f67550336452e95cb88157f3c0613);
        

        circle_marker_a15aae6820a04ed8a5ccbf83de874c20.bindPopup(popup_b93ac789cdf6417f9689a1b6d2697287)
        ;

        
    
    
            var circle_marker_1defca2fc70f4ccf909426d8a94eb880 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d2cbd8b800f8451bac0b1871a39b447a = L.popup({"maxWidth": "100%"});

        
            var html_d7822f1afb0a4836ab1953b12215f944 = $(`<div id="html_d7822f1afb0a4836ab1953b12215f944" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1911.05833">Emotion and Theme Recognition in Music with Frequency-Aware RF-Regularized CNNs</a><br></div>`)[0];
            popup_d2cbd8b800f8451bac0b1871a39b447a.setContent(html_d7822f1afb0a4836ab1953b12215f944);
        

        circle_marker_1defca2fc70f4ccf909426d8a94eb880.bindPopup(popup_d2cbd8b800f8451bac0b1871a39b447a)
        ;

        
    
    
            var circle_marker_70a70db5a59443ef8780a79ff6e7271a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e378cfafe7bd49878a1d21de2dc317cc = L.popup({"maxWidth": "100%"});

        
            var html_b1439520e5294e07a5936e322f061a8c = $(`<div id="html_b1439520e5294e07a5936e322f061a8c" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1909.02869">Exploiting parallel audio recordings to enforce device invariance in cnn-based acoustic scene classification</a><br></div>`)[0];
            popup_e378cfafe7bd49878a1d21de2dc317cc.setContent(html_b1439520e5294e07a5936e322f061a8c);
        

        circle_marker_70a70db5a59443ef8780a79ff6e7271a.bindPopup(popup_e378cfafe7bd49878a1d21de2dc317cc)
        ;

        
    
    
            var circle_marker_5555759779ac403ca6f5f0c85fee62d2 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_eaa279f33967417988ed7a34f6f87111 = L.popup({"maxWidth": "100%"});

        
            var html_29302df66db24acf9669160eeefba987 = $(`<div id="html_29302df66db24acf9669160eeefba987" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-018-9215-8">Feature-combination hybrid recommender systems for automated music playlist continuation</a><br></div>`)[0];
            popup_eaa279f33967417988ed7a34f6f87111.setContent(html_29302df66db24acf9669160eeefba987);
        

        circle_marker_5555759779ac403ca6f5f0c85fee62d2.bindPopup(popup_eaa279f33967417988ed7a34f6f87111)
        ;

        
    
    
            var circle_marker_2a46a7ffdadd4ee089a07b888f5c361f = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_95c6048d1dca4b1b8c359b15b9833d13 = L.popup({"maxWidth": "100%"});

        
            var html_b1bce13754ca4084ae4832af79eeaa2e = $(`<div id="html_b1bce13754ca4084ae4832af79eeaa2e" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1808.05340">Genre-agnostic key classification with convolutional neural networks</a><br></div>`)[0];
            popup_95c6048d1dca4b1b8c359b15b9833d13.setContent(html_b1bce13754ca4084ae4832af79eeaa2e);
        

        circle_marker_2a46a7ffdadd4ee089a07b888f5c361f.bindPopup(popup_95c6048d1dca4b1b8c359b15b9833d13)
        ;

        
    
    
            var circle_marker_4a2fcb2430b94f4a969d8cc63359ec34 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bb7c94af1cbb48229561ae0bc59143e4 = L.popup({"maxWidth": "100%"});

        
            var html_a387506f05d94fd3ba3b956e78b9c1c5 = $(`<div id="html_a387506f05d94fd3ba3b956e78b9c1c5" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Hamid_Eghbal-Zadeh/publication/280227823_I-VECTORS_FOR_TIMBRE-BASED_MUSIC_SIMILARITY_AND_MUSIC_ARTIST_CLASSIFICATION/links/55d4676608ae0b8f3ef9fa8c/I-VECTORS-FOR-TIMBRE-BASED-MUSIC-SIMILARITY-AND-MUSIC-ARTIST-CLASSIFICATION.pdf">I-Vectors for Timbre-Based Music Similarity and Music Artist Classification.</a><br></div>`)[0];
            popup_bb7c94af1cbb48229561ae0bc59143e4.setContent(html_a387506f05d94fd3ba3b956e78b9c1c5);
        

        circle_marker_4a2fcb2430b94f4a969d8cc63359ec34.bindPopup(popup_bb7c94af1cbb48229561ae0bc59143e4)
        ;

        
    
    
            var circle_marker_5e3a09e7f9614132a34765f61f6816e2 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d8c6ef9b8491499db41c75816003f4ec = L.popup({"maxWidth": "100%"});

        
            var html_d17b700cdafe4e9babfd4f359e8407bc = $(`<div id="html_d17b700cdafe4e9babfd4f359e8407bc" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://archives.ismir.net/ismir2019/paper/000003.pdf">Intelligent User Interfaces for Music Discovery: The Past 20 Years and What's to Come.</a><br></div>`)[0];
            popup_d8c6ef9b8491499db41c75816003f4ec.setContent(html_d17b700cdafe4e9babfd4f359e8407bc);
        

        circle_marker_5e3a09e7f9614132a34765f61f6816e2.bindPopup(popup_d8c6ef9b8491499db41c75816003f4ec)
        ;

        
    
    
            var circle_marker_77cf92d025a847e4bf1bb615f2a49f2c = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b7146932ff224b61ad248139d3a95153 = L.popup({"maxWidth": "100%"});

        
            var html_021473b17391482ea6150e4fdc3a7b22 = $(`<div id="html_021473b17391482ea6150e4fdc3a7b22" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.jku.at/fileadmin/gruppen/173/Research/Iterative_Knowledge_Distillation_In_R-CNNs.pdf">Iterative knowledge distillation in R-CNNs for weakly-labeled semisupervised sound event detection</a><br></div>`)[0];
            popup_b7146932ff224b61ad248139d3a95153.setContent(html_021473b17391482ea6150e4fdc3a7b22);
        

        circle_marker_77cf92d025a847e4bf1bb615f2a49f2c.bindPopup(popup_b7146932ff224b61ad248139d3a95153)
        ;

        
    
    
            var circle_marker_299ea0bb48394efb8a28930548e126a0 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0e84e8ad450e4245a2e344b3ebb672ea = L.popup({"maxWidth": "100%"});

        
            var html_d9c5eb7efd2546c68f867fe8631f6938 = $(`<div id="html_d9c5eb7efd2546c68f867fe8631f6938" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dbis.uibk.ac.at/sites/default/files/2019-06/paper_1.pdf">Language Models for Next-Track Music Recommendation.</a><br></div>`)[0];
            popup_0e84e8ad450e4245a2e344b3ebb672ea.setContent(html_d9c5eb7efd2546c68f867fe8631f6938);
        

        circle_marker_299ea0bb48394efb8a28930548e126a0.bindPopup(popup_0e84e8ad450e4245a2e344b3ebb672ea)
        ;

        
    
    
            var circle_marker_fa830e08439a4c4e96cfdfb15310174b = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8ea5accf5a03401e9995f6e12be65135 = L.popup({"maxWidth": "100%"});

        
            var html_6f085a23919e46268a6aecf8f4ce935b = $(`<div id="html_6f085a23919e46268a6aecf8f4ce935b" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-10997-4_42">Machine learning approaches to hybrid music recommender systems</a><br></div>`)[0];
            popup_8ea5accf5a03401e9995f6e12be65135.setContent(html_6f085a23919e46268a6aecf8f4ce935b);
        

        circle_marker_fa830e08439a4c4e96cfdfb15310174b.bindPopup(popup_8ea5accf5a03401e9995f6e12be65135)
        ;

        
    
    
            var circle_marker_98d6d6b37df4448691b3d32dd69b8d61 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7f05f61caea149f5b190b56034f9014d = L.popup({"maxWidth": "100%"});

        
            var html_0a1630c156604b35aea9e03c247aed8d = $(`<div id="html_0a1630c156604b35aea9e03c247aed8d" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://transactions.ismir.net/articles/10.5334/tismir.39/?utm_source=TrendMD&utm_medium=cpc&utm_campaign=Transactions_of_the_International_Society_for_Music_Information_Retrieval_TrendMD_0">Modeling Popularity and Temporal Drift of Music Genre Preferences</a><br></div>`)[0];
            popup_7f05f61caea149f5b190b56034f9014d.setContent(html_0a1630c156604b35aea9e03c247aed8d);
        

        circle_marker_98d6d6b37df4448691b3d32dd69b8d61.bindPopup(popup_7f05f61caea149f5b190b56034f9014d)
        ;

        
    
    
            var circle_marker_b368fd620ed441559201b401e2249dde = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_64a34ab905084379870d26525721a7c4 = L.popup({"maxWidth": "100%"});

        
            var html_a50eb10d4bdd49f2af00b5c9d943e202 = $(`<div id="html_a50eb10d4bdd49f2af00b5c9d943e202" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2004.10618">Moment-Based Domain Adaptation: Learning Bounds and Algorithms</a><br></div>`)[0];
            popup_64a34ab905084379870d26525721a7c4.setContent(html_a50eb10d4bdd49f2af00b5c9d943e202);
        

        circle_marker_b368fd620ed441559201b401e2249dde.bindPopup(popup_64a34ab905084379870d26525721a7c4)
        ;

        
    
    
            var circle_marker_507f5349f9aa4ff7b43f3fb7d2e04e67 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_613bd5369faf4303bd2ba7a639fa0715 = L.popup({"maxWidth": "100%"});

        
            var html_21074551fd1c4f6d825603d16a540dc3 = $(`<div id="html_21074551fd1c4f6d825603d16a540dc3" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877452/">Movie genome recommender: A novel recommender system based on multimedia content</a><br></div>`)[0];
            popup_613bd5369faf4303bd2ba7a639fa0715.setContent(html_21074551fd1c4f6d825603d16a540dc3);
        

        circle_marker_507f5349f9aa4ff7b43f3fb7d2e04e67.bindPopup(popup_613bd5369faf4303bd2ba7a639fa0715)
        ;

        
    
    
            var circle_marker_b2af63879b3545218a0881a1e824db1f = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8bece98c3942478f89041bdb4abaa5b5 = L.popup({"maxWidth": "100%"});

        
            var html_35338006fdef4972892d6bb6be667486 = $(`<div id="html_35338006fdef4972892d6bb6be667486" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2002.00251">Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with Visual Computing for Improved Music Video Analysis</a><br></div>`)[0];
            popup_8bece98c3942478f89041bdb4abaa5b5.setContent(html_35338006fdef4972892d6bb6be667486);
        

        circle_marker_b2af63879b3545218a0881a1e824db1f.bindPopup(popup_8bece98c3942478f89041bdb4abaa5b5)
        ;

        
    
    
            var circle_marker_06cf56d924e245ccbefaad39373d0876 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_393741a5ffc24238aad1507a628326dc = L.popup({"maxWidth": "100%"});

        
            var html_e556b61d6aad4f43ac6ae5231016299f = $(`<div id="html_e556b61d6aad4f43ac6ae5231016299f" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1811.04419">Multi-temporal resolution convolutional neural networks for acoustic scene classification</a><br></div>`)[0];
            popup_393741a5ffc24238aad1507a628326dc.setContent(html_e556b61d6aad4f43ac6ae5231016299f);
        

        circle_marker_06cf56d924e245ccbefaad39373d0876.bindPopup(popup_393741a5ffc24238aad1507a628326dc)
        ;

        
    
    
            var circle_marker_33d11e89006f413cbc154c1389c95f7e = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5b49ff7456e642fe89b2a0fb686ea590 = L.popup({"maxWidth": "100%"});

        
            var html_c671aa3b89d34d52b0f5360e37b5d5cd = $(`<div id="html_c671aa3b89d34d52b0f5360e37b5d5cd" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3241620">Multimedia recommender systems</a><br></div>`)[0];
            popup_5b49ff7456e642fe89b2a0fb686ea590.setContent(html_c671aa3b89d34d52b0f5360e37b5d5cd);
        

        circle_marker_33d11e89006f413cbc154c1389c95f7e.bindPopup(popup_5b49ff7456e642fe89b2a0fb686ea590)
        ;

        
    
    
            var circle_marker_485d540385aa4f6ea264fa99e940d18f = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7538550cec2f4f5c879645ebe3765b29 = L.popup({"maxWidth": "100%"});

        
            var html_efb51786a73e4f1f8003c4db8a817a10 = $(`<div id="html_efb51786a73e4f1f8003c4db8a817a10" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://www.cp.jku.at/people/vall/vall_etal_ecml-pkdd2017.pdf">Music Playlist Continuation by Learning from Hand-Curated Examples and Song Features</a><br></div>`)[0];
            popup_7538550cec2f4f5c879645ebe3765b29.setContent(html_efb51786a73e4f1f8003c4db8a817a10);
        

        circle_marker_485d540385aa4f6ea264fa99e940d18f.bindPopup(popup_7538550cec2f4f5c879645ebe3765b29)
        ;

        
    
    
            var circle_marker_316f99f6cf9242288b3fab5c42416bee = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_da27e44f7cd04789970cda7a8e9aa6c4 = L.popup({"maxWidth": "100%"});

        
            var html_0b9f485f29f44aaf8d868825a28929be = $(`<div id="html_0b9f485f29f44aaf8d868825a28929be" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3125486.3125494">Music playlist continuation by learning from hand-curated examples and song features: Alleviating the cold-start problem for rare and out-of-set songs</a><br></div>`)[0];
            popup_da27e44f7cd04789970cda7a8e9aa6c4.setContent(html_0b9f485f29f44aaf8d868825a28929be);
        

        circle_marker_316f99f6cf9242288b3fab5c42416bee.bindPopup(popup_da27e44f7cd04789970cda7a8e9aa6c4)
        ;

        
    
    
            var circle_marker_c7ebd674a05b441998318813f026873a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_515242e78c6b4ae2b9d7abed71eace8b = L.popup({"maxWidth": "100%"});

        
            var html_4c52c63b5e4f405a8a6d117a035dec53 = $(`<div id="html_4c52c63b5e4f405a8a6d117a035dec53" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/2484028.2484193">Music similarity and retrieval</a><br></div>`)[0];
            popup_515242e78c6b4ae2b9d7abed71eace8b.setContent(html_4c52c63b5e4f405a8a6d117a035dec53);
        

        circle_marker_c7ebd674a05b441998318813f026873a.bindPopup(popup_515242e78c6b4ae2b9d7abed71eace8b)
        ;

        
    
    
            var circle_marker_132d7357518e4e20b39aaa38fad4c55f = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bba781db1c17445896e618e26dbc97b5 = L.popup({"maxWidth": "100%"});

        
            var html_2ed4cb0a2695426a96f921261f2a37fe = $(`<div id="html_2ed4cb0a2695426a96f921261f2a37fe" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Hamid_Eghbal-Zadeh/publication/305687848_NOISE_ROBUST_MUSIC_ARTIST_RECOGNITION_USING_I-VECTOR_FEATURES/links/5799cded08aeb0ffcd0f818c/NOISE-ROBUST-MUSIC-ARTIST-RECOGNITION-USING-I-VECTOR-FEATURES.pdf">Noise robust music artist recognition using i-vector</a><br></div>`)[0];
            popup_bba781db1c17445896e618e26dbc97b5.setContent(html_2ed4cb0a2695426a96f921261f2a37fe);
        

        circle_marker_132d7357518e4e20b39aaa38fad4c55f.bindPopup(popup_bba781db1c17445896e618e26dbc97b5)
        ;

        
    
    
            var circle_marker_f1c29e93e2e743a08400677bd7652826 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8f35ebe39b514f9ab0f0ad2d81d709a9 = L.popup({"maxWidth": "100%"});

        
            var html_2f3a9e0011a54f96bce028e0e34665ff = $(`<div id="html_2f3a9e0011a54f96bce028e0e34665ff" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2008.02194">On the Characterization of Expressive Performance in Classical Music: First Results of the Con Espressione Game</a><br></div>`)[0];
            popup_8f35ebe39b514f9ab0f0ad2d81d709a9.setContent(html_2f3a9e0011a54f96bce028e0e34665ff);
        

        circle_marker_f1c29e93e2e743a08400677bd7652826.bindPopup(popup_8f35ebe39b514f9ab0f0ad2d81d709a9)
        ;

        
    
    
            var circle_marker_5c5316a3f7fa4cef9ef2e56129239aec = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_534368e6f1ad40ffbc1f0d94e8b490b0 = L.popup({"maxWidth": "100%"});

        
            var html_f1f40ca28c46469fb0c71d927f7d20ad = $(`<div id="html_f1f40ca28c46469fb0c71d927f7d20ad" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://www.ifs.tuwien.ac.at/~vogl/slides/ismir-18-exhibition.pdf">Pose Estimation</a><br></div>`)[0];
            popup_534368e6f1ad40ffbc1f0d94e8b490b0.setContent(html_f1f40ca28c46469fb0c71d927f7d20ad);
        

        circle_marker_5c5316a3f7fa4cef9ef2e56129239aec.bindPopup(popup_534368e6f1ad40ffbc1f0d94e8b490b0)
        ;

        
    
    
            var circle_marker_ecf1bf92c76d4f2797979737b511140a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_42a874e32e5845bf9178d296a38d088a = L.popup({"maxWidth": "100%"});

        
            var html_b20ac58562b8411db494c2bb0a30331b = $(`<div id="html_b20ac58562b8411db494c2bb0a30331b" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2007.13503">Receptive-Field Regularized CNNs for Music Classification and Tagging</a><br></div>`)[0];
            popup_42a874e32e5845bf9178d296a38d088a.setContent(html_b20ac58562b8411db494c2bb0a30331b);
        

        circle_marker_ecf1bf92c76d4f2797979737b511140a.bindPopup(popup_42a874e32e5845bf9178d296a38d088a)
        ;

        
    
    
            var circle_marker_3f165768e4b14216881b1cc6e185c2f0 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9e19eaf5e44b4ebebfb41f7b00587fc7 = L.popup({"maxWidth": "100%"});

        
            var html_c129b3121f2c4db89a650cb10f39797f = $(`<div id="html_c129b3121f2c4db89a650cb10f39797f" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1909.02859">Receptive-field-regularized CNN variants for acoustic scene classification</a><br></div>`)[0];
            popup_9e19eaf5e44b4ebebfb41f7b00587fc7.setContent(html_c129b3121f2c4db89a650cb10f39797f);
        

        circle_marker_3f165768e4b14216881b1cc6e185c2f0.bindPopup(popup_9e19eaf5e44b4ebebfb41f7b00587fc7)
        ;

        
    
    
            var circle_marker_e6bc8cd63688462ca12f90f2275db8e0 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4787f110d34f4c4f89fc1acaf8f9671e = L.popup({"maxWidth": "100%"});

        
            var html_d54fa61d86c9435e8e4030bf31d54602 = $(`<div id="html_d54fa61d86c9435e8e4030bf31d54602" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/342211693_Recommender_Systems_Leveraging_Multimedia_Content/links/5f1a654d45851515ef44cb80/Recommender-Systems-Leveraging-Multimedia-Content.pdf">Recommender systems leveraging multimedia content</a><br></div>`)[0];
            popup_4787f110d34f4c4f89fc1acaf8f9671e.setContent(html_d54fa61d86c9435e8e4030bf31d54602);
        

        circle_marker_e6bc8cd63688462ca12f90f2275db8e0.bindPopup(popup_4787f110d34f4c4f89fc1acaf8f9671e)
        ;

        
    
    
            var circle_marker_21865b13d8cb46a3a0c2f476eb8f93a8 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c1658f6bec0e45a7a1c25ac5e0c1ccb7 = L.popup({"maxWidth": "100%"});

        
            var html_d29693ebfedf49d9aa6ab539fe075e6c = $(`<div id="html_d29693ebfedf49d9aa6ab539fe075e6c" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877420/">Retrieving relevant and diverse movie clips using the mfvcd-7k multifaceted video clip dataset</a><br></div>`)[0];
            popup_c1658f6bec0e45a7a1c25ac5e0c1ccb7.setContent(html_d29693ebfedf49d9aa6ab539fe075e6c);
        

        circle_marker_21865b13d8cb46a3a0c2f476eb8f93a8.bindPopup(popup_c1658f6bec0e45a7a1c25ac5e0c1ccb7)
        ;

        
    
    
            var circle_marker_26d0704a83f14f20baa851b30eb81700 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_aca068d8ce4a4bb3a69c2f3aea376fa4 = L.popup({"maxWidth": "100%"});

        
            var html_1f8008fd01b24957b8b052d1e2d3d608 = $(`<div id="html_1f8008fd01b24957b8b052d1e2d3d608" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8587031/">Robust machine learning based acoustic classification of a material transport process</a><br></div>`)[0];
            popup_aca068d8ce4a4bb3a69c2f3aea376fa4.setContent(html_1f8008fd01b24957b8b052d1e2d3d608);
        

        circle_marker_26d0704a83f14f20baa851b30eb81700.bindPopup(popup_aca068d8ce4a4bb3a69c2f3aea376fa4)
        ;

        
    
    
            var circle_marker_03784c02fcc34e5aa90c89ce670e45fa = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d51f247392f24859bc4898b41e5a527d = L.popup({"maxWidth": "100%"});

        
            var html_b60020a4c54542908f798575b2a44c14 = $(`<div id="html_b60020a4c54542908f798575b2a44c14" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/1810.06897">Sound event detection using weakly-labeled semi-supervised data with GCRNNS, VAT and Self-Adaptive Label Refinement</a><br></div>`)[0];
            popup_d51f247392f24859bc4898b41e5a527d.setContent(html_b60020a4c54542908f798575b2a44c14);
        

        circle_marker_03784c02fcc34e5aa90c89ce670e45fa.bindPopup(popup_d51f247392f24859bc4898b41e5a527d)
        ;

        
    
    
            var circle_marker_dc6db2add4e442ecb1c507ded014926e = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6aeca531c71a46319216adfb90cb2000 = L.popup({"maxWidth": "100%"});

        
            var html_541187f02a2e422ab00be496b6aad953 = $(`<div id="html_541187f02a2e422ab00be496b6aad953" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.researchgate.net/profile/Yashar_Deldjoo3/publication/334548980_The_2019_Multimedia_for_Recommender_System_Task_MovieREC_and_NewsREEL_at_MediaEval/links/5d30c3ac299bf1547cc258c1/The-2019-Multimedia-for-Recommender-System-Task-MovieREC-and-NewsREEL-at-MediaEval.pdf">The 2019 Multimedia for Recommender System Task: MovieREC and NewsREEL at MediaEval</a><br></div>`)[0];
            popup_6aeca531c71a46319216adfb90cb2000.setContent(html_541187f02a2e422ab00be496b6aad953);
        

        circle_marker_dc6db2add4e442ecb1c507ded014926e.bindPopup(popup_6aeca531c71a46319216adfb90cb2000)
        ;

        
    
    
            var circle_marker_91bb31a1eeff40b8992e0bda511f1d47 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4bba3c2f642b4998968546d8d0c4f72d = L.popup({"maxWidth": "100%"});

        
            var html_570d196f211b48e1a98c3d3cd0799d3a = $(`<div id="html_570d196f211b48e1a98c3d3cd0799d3a" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8902732/">The receptive field as a regularizer in deep convolutional neural networks for acoustic scene classification</a><br></div>`)[0];
            popup_4bba3c2f642b4998968546d8d0c4f72d.setContent(html_570d196f211b48e1a98c3d3cd0799d3a);
        

        circle_marker_91bb31a1eeff40b8992e0bda511f1d47.bindPopup(popup_4bba3c2f642b4998968546d8d0c4f72d)
        ;

        
    
    
            var circle_marker_f28785ba646e40a8ada219dcdfcd02bd = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_67d2cd8045c346ec82a6fb98cbf61c1e = L.popup({"maxWidth": "100%"});

        
            var html_c8ff149b733a42878ed334ed8e8435bf = $(`<div id="html_c8ff149b733a42878ed334ed8e8435bf" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://www.jku.at/fileadmin/gruppen/173/Research/vall_mdw_2016.pdf">Timbral and semantic features for music playlists</a><br></div>`)[0];
            popup_67d2cd8045c346ec82a6fb98cbf61c1e.setContent(html_c8ff149b733a42878ed334ed8e8435bf);
        

        circle_marker_f28785ba646e40a8ada219dcdfcd02bd.bindPopup(popup_67d2cd8045c346ec82a6fb98cbf61c1e)
        ;

        
    
    
            var circle_marker_5dff8828295c464f9305ec05524c8e4b = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d56c7fe4e7a2427581de2ec8fe2c4072 = L.popup({"maxWidth": "100%"});

        
            var html_87e8429ebbd849f58afe33272d3e277e = $(`<div id="html_87e8429ebbd849f58afe33272d3e277e" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Dorfer_999.pdf">Training general-purpose audio tagging networks with noisy labels and iterative self-verification</a><br></div>`)[0];
            popup_d56c7fe4e7a2427581de2ec8fe2c4072.setContent(html_87e8429ebbd849f58afe33272d3e277e);
        

        circle_marker_5dff8828295c464f9305ec05524c8e4b.bindPopup(popup_d56c7fe4e7a2427581de2ec8fe2c4072)
        ;

        
    
    
            var circle_marker_6d751f4942d3436d89207e5a2acb5b8a = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c85a19088df946e587eee13d945fbbbc = L.popup({"maxWidth": "100%"});

        
            var html_14a2d4f712654ad496f6d36f66aaac6a = $(`<div id="html_14a2d4f712654ad496f6d36f66aaac6a" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09231-w">Trends in content-based recommendation</a><br></div>`)[0];
            popup_c85a19088df946e587eee13d945fbbbc.setContent(html_14a2d4f712654ad496f6d36f66aaac6a);
        

        circle_marker_6d751f4942d3436d89207e5a2acb5b8a.bindPopup(popup_c85a19088df946e587eee13d945fbbbc)
        ;

        
    
    
            var circle_marker_065fdda9479f40bdb129e9a3a59ea5f2 = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2e679c234dff427ea0aaf44cdb417892 = L.popup({"maxWidth": "100%"});

        
            var html_aa2618202fb647ee8e1836b206e9610b = $(`<div id="html_aa2618202fb647ee8e1836b206e9610b" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://books.google.com/books?hl=de&lr=&id=6jvEDwAAQBAJ&oi=fnd&pg=PA223&ots=FUoGquoOv0&sig=3RyQmnd8MD1YU2VZq8Kbbhl_ezU">User Awareness in Music Recommender Systems</a><br></div>`)[0];
            popup_2e679c234dff427ea0aaf44cdb417892.setContent(html_aa2618202fb647ee8e1836b206e9610b);
        

        circle_marker_065fdda9479f40bdb129e9a3a59ea5f2.bindPopup(popup_2e679c234dff427ea0aaf44cdb417892)
        ;

        
    
    
            var circle_marker_54e0e0185b504a7dbfc1128278bd7f0c = L.circleMarker(
                [47.2000338, 13.199959],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_96e83478ce7c4cd3a40706b28365bd9c = L.popup({"maxWidth": "100%"});

        
            var html_a2facbf650a94791a398f09d8db7c62a = $(`<div id="html_a2facbf650a94791a398f09d8db7c62a" style="width: 100.0%; height: 100.0%;">Country : Austria<br>                         Paper : <a href="https://arxiv.org/abs/2003.10699">Utilizing Human Memory Processes to Model Genre Preferences for Personalized Music Recommendations</a><br></div>`)[0];
            popup_96e83478ce7c4cd3a40706b28365bd9c.setContent(html_a2facbf650a94791a398f09d8db7c62a);
        

        circle_marker_54e0e0185b504a7dbfc1128278bd7f0c.bindPopup(popup_96e83478ce7c4cd3a40706b28365bd9c)
        ;

        
    
    
            var circle_marker_0087b4a758164bd4afaefbf601fd030f = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9f0e12724db94224ac1f4fcc2b86670f = L.popup({"maxWidth": "100%"});

        
            var html_51f7151f45af41f5872525b651bfb851 = $(`<div id="html_51f7151f45af41f5872525b651bfb851" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://www.researchgate.net/profile/Hossein_A_Rahmani/publication/337768473_A_Regression_Approach_to_Movie_Rating_Prediction_using_Multimedia_Content_and_Metadata/links/5df8b821a6fdcc283726d8c7/A-Regression-Approach-to-Movie-Rating-Prediction-using-Multimedia-Content-and-Metadata.pdf?_sg%5B0%5D=3b-miF1_LzXGwxQhQxRYO5FfklSPn0YfAlrK6IWfJilrpkC8-9h2ljnRbzLhZjp8z22CM_vzJpiQ-DaxhD7RLQ.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_sg%5B1%5D=0EyWPCbfTe4cM2OzZ6VGxzinAQj21QkakmKNQ9xBAXR2oXBv7SQahoq94YN7zTBLR7KuEuKMzCqZtGzBOqJvKV6rX2V1E4ISvHGuBwdOwd-p.1deeKt1JLIU4JOBZxBPLOKsnFfXnhj0DGEuGzqDcp0M4uCBRUFpxTTSbWGdpbYdmTc_GC81Xosz4slNg_bsWzQ&_iepl=">A Regression Approach to Movie Rating Prediction using Multimedia Content and Metadata</a><br></div>`)[0];
            popup_9f0e12724db94224ac1f4fcc2b86670f.setContent(html_51f7151f45af41f5872525b651bfb851);
        

        circle_marker_0087b4a758164bd4afaefbf601fd030f.bindPopup(popup_9f0e12724db94224ac1f4fcc2b86670f)
        ;

        
    
    
            var circle_marker_f0723fe99cc946ab887dc07dd4ed8b74 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_67a9e6b2822c46dfa0f7baac4b528b62 = L.popup({"maxWidth": "100%"});

        
            var html_e5029cf23f704f5794e43dc7f158a431 = $(`<div id="html_e5029cf23f704f5794e43dc7f158a431" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8964809/">Acoustic Scene Classification using Binaural Representation and Classifier Combination</a><br></div>`)[0];
            popup_67a9e6b2822c46dfa0f7baac4b528b62.setContent(html_e5029cf23f704f5794e43dc7f158a431);
        

        circle_marker_f0723fe99cc946ab887dc07dd4ed8b74.bindPopup(popup_67a9e6b2822c46dfa0f7baac4b528b62)
        ;

        
    
    
            var circle_marker_c50ad666c82d4977ae8ce29324f586bc = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_42489c494ed44dd5a24685ce722ca002 = L.popup({"maxWidth": "100%"});

        
            var html_07d324c50b3a4ba58bf75eca5bc9efff = $(`<div id="html_07d324c50b3a4ba58bf75eca5bc9efff" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://arxiv.org/abs/1907.07127">Acoustic scene classification using fusion of attentive convolutional neural networks for DCASE2019 challenge</a><br></div>`)[0];
            popup_42489c494ed44dd5a24685ce722ca002.setContent(html_07d324c50b3a4ba58bf75eca5bc9efff);
        

        circle_marker_c50ad666c82d4977ae8ce29324f586bc.bindPopup(popup_42489c494ed44dd5a24685ce722ca002)
        ;

        
    
    
            var circle_marker_0a90de9520104d1baa4265131a72e065 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3702b0ba8ffd42009aa10a0bdcf2d38f = L.popup({"maxWidth": "100%"});

        
            var html_9f99426c5a174260bd107ae34b8bb929 = $(`<div id="html_9f99426c5a174260bd107ae34b8bb929" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://arxiv.org/abs/1810.04273">Convolutional neural networks and x-vector embedding for DCASE2018 acoustic scene classification challenge</a><br></div>`)[0];
            popup_3702b0ba8ffd42009aa10a0bdcf2d38f.setContent(html_9f99426c5a174260bd107ae34b8bb929);
        

        circle_marker_0a90de9520104d1baa4265131a72e065.bindPopup(popup_3702b0ba8ffd42009aa10a0bdcf2d38f)
        ;

        
    
    
            var circle_marker_cb84d330659f437ab435bad3b25c60b5 = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_58f249120a684765b3d901aa3ddd1471 = L.popup({"maxWidth": "100%"});

        
            var html_06155d2d3e234dc197648d50f9c96683 = $(`<div id="html_06155d2d3e234dc197648d50f9c96683" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8270136/">On the usage of i-vector representation for online handwritten signature verification</a><br></div>`)[0];
            popup_58f249120a684765b3d901aa3ddd1471.setContent(html_06155d2d3e234dc197648d50f9c96683);
        

        circle_marker_cb84d330659f437ab435bad3b25c60b5.bindPopup(popup_58f249120a684765b3d901aa3ddd1471)
        ;

        
    
    
            var circle_marker_a266f825d3af41d0af62a71be506db5b = L.circleMarker(
                [32.6475314, 54.5643516],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8213b6d1868749808ca9824e17175090 = L.popup({"maxWidth": "100%"});

        
            var html_80c348f4bec7430ca329a447aab41c6a = $(`<div id="html_80c348f4bec7430ca329a447aab41c6a" style="width: 100.0%; height: 100.0%;">Country : Iran<br>                         Paper : <a href="https://digital-library.theiet.org/content/journals/10.1049/iet-bmt.2017.0059">Online signature verification using i-vector representation</a><br></div>`)[0];
            popup_8213b6d1868749808ca9824e17175090.setContent(html_80c348f4bec7430ca329a447aab41c6a);
        

        circle_marker_a266f825d3af41d0af62a71be506db5b.bindPopup(popup_8213b6d1868749808ca9824e17175090)
        ;

        
    
    
            var circle_marker_8bc2e630fde64428a078d2bd3d32d5d5 = L.circleMarker(
                [49.8167003, 15.4749544],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_eaf06706a3894c72af9f199bf69ae287 = L.popup({"maxWidth": "100%"});

        
            var html_1c6d0ec8532740d1bff292f6846e7887 = $(`<div id="html_1c6d0ec8532740d1bff292f6846e7887" style="width: 100.0%; height: 100.0%;">Country : Czech Republic<br>                         Paper : <a href="https://arxiv.org/abs/1907.07127">Acoustic scene classification using fusion of attentive convolutional neural networks for DCASE2019 challenge</a><br></div>`)[0];
            popup_eaf06706a3894c72af9f199bf69ae287.setContent(html_1c6d0ec8532740d1bff292f6846e7887);
        

        circle_marker_8bc2e630fde64428a078d2bd3d32d5d5.bindPopup(popup_eaf06706a3894c72af9f199bf69ae287)
        ;

        
    
    
            var circle_marker_e49190dee20349639f184885fa990a8f = L.circleMarker(
                [49.8167003, 15.4749544],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e79dbf1edc7546a2a2495b4ef2f6c0a9 = L.popup({"maxWidth": "100%"});

        
            var html_b9fd38dfb3874c9d8a9ab064e124784b = $(`<div id="html_b9fd38dfb3874c9d8a9ab064e124784b" style="width: 100.0%; height: 100.0%;">Country : Czech Republic<br>                         Paper : <a href="https://arxiv.org/abs/1810.04273">Convolutional neural networks and x-vector embedding for DCASE2018 acoustic scene classification challenge</a><br></div>`)[0];
            popup_e79dbf1edc7546a2a2495b4ef2f6c0a9.setContent(html_b9fd38dfb3874c9d8a9ab064e124784b);
        

        circle_marker_e49190dee20349639f184885fa990a8f.bindPopup(popup_e79dbf1edc7546a2a2495b4ef2f6c0a9)
        ;

        
    
    
            var circle_marker_cc22d4da6b784b05a95bfa2956dfb5be = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_60412d4de672495ea59ec6a89957a212 = L.popup({"maxWidth": "100%"});

        
            var html_322d408cc19c410eab893e1b751c4281 = $(`<div id="html_322d408cc19c410eab893e1b751c4281" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-981-13-8707-4_8">A Comparison of Attention Mechanisms of Convolutional Neural Network in Weakly Labeled Audio Tagging</a><br></div>`)[0];
            popup_60412d4de672495ea59ec6a89957a212.setContent(html_322d408cc19c410eab893e1b751c4281);
        

        circle_marker_cc22d4da6b784b05a95bfa2956dfb5be.bindPopup(popup_60412d4de672495ea59ec6a89957a212)
        ;

        
    
    
            var circle_marker_5ee342c2b480401e8ba7593153119087 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9c719f241aeb4993b6ccbf26709b7809 = L.popup({"maxWidth": "100%"});

        
            var html_37dd2889ca2f4f3ab9e2d7c6664177b9 = $(`<div id="html_37dd2889ca2f4f3ab9e2d7c6664177b9" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8794834/">A Novel Social Situation Analytics-Based Recommendation Algorithm for Multimedia Social Networks</a><br></div>`)[0];
            popup_9c719f241aeb4993b6ccbf26709b7809.setContent(html_37dd2889ca2f4f3ab9e2d7c6664177b9);
        

        circle_marker_5ee342c2b480401e8ba7593153119087.bindPopup(popup_9c719f241aeb4993b6ccbf26709b7809)
        ;

        
    
    
            var circle_marker_e8f8935860b7479c83f63d61136f0530 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2cc53065b20a46009383a17e7c5ad4f4 = L.popup({"maxWidth": "100%"});

        
            var html_5f81dee0ff6c4668a121dc9eba00da0d = $(`<div id="html_5f81dee0ff6c4668a121dc9eba00da0d" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/1904.05243">A compact and discriminative feature based on auditory summary statistics for acoustic scene classification</a><br></div>`)[0];
            popup_2cc53065b20a46009383a17e7c5ad4f4.setContent(html_5f81dee0ff6c4668a121dc9eba00da0d);
        

        circle_marker_e8f8935860b7479c83f63d61136f0530.bindPopup(popup_2cc53065b20a46009383a17e7c5ad4f4)
        ;

        
    
    
            var circle_marker_589a25a62a9f449cab5fa05c69efd008 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e99ddae513df4646a5c86f123d27c03e = L.popup({"maxWidth": "100%"});

        
            var html_3c70c9dd1fd147e58faccce035260dbe = $(`<div id="html_3c70c9dd1fd147e58faccce035260dbe" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Dinkel_38.pdf">A hybrid asr model approach on weakly labeled scene classification</a><br></div>`)[0];
            popup_e99ddae513df4646a5c86f123d27c03e.setContent(html_3c70c9dd1fd147e58faccce035260dbe);
        

        circle_marker_589a25a62a9f449cab5fa05c69efd008.bindPopup(popup_e99ddae513df4646a5c86f123d27c03e)
        ;

        
    
    
            var circle_marker_90b3abeba04749098bc9d85ecf7654b6 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ebd48e1cab5c42a5893ac14c955457e1 = L.popup({"maxWidth": "100%"});

        
            var html_5ad994d3602849899ad5b7107a797fb0 = $(`<div id="html_5ad994d3602849899ad5b7107a797fb0" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682376/">A region based attention method for weakly supervised sound event detection and classification</a><br></div>`)[0];
            popup_ebd48e1cab5c42a5893ac14c955457e1.setContent(html_5ad994d3602849899ad5b7107a797fb0);
        

        circle_marker_90b3abeba04749098bc9d85ecf7654b6.bindPopup(popup_ebd48e1cab5c42a5893ac14c955457e1)
        ;

        
    
    
            var circle_marker_e9359029c74047e190418c16257b852f = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_59ff1c256d4646bb98ec486ebacda5f1 = L.popup({"maxWidth": "100%"});

        
            var html_a853c3ef721344a59e0c479ffacd6976 = $(`<div id="html_a853c3ef721344a59e0c479ffacd6976" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s00034-019-01094-1">A survey: Neural network-based deep learning for acoustic event detection</a><br></div>`)[0];
            popup_59ff1c256d4646bb98ec486ebacda5f1.setContent(html_a853c3ef721344a59e0c479ffacd6976);
        

        circle_marker_e9359029c74047e190418c16257b852f.bindPopup(popup_59ff1c256d4646bb98ec486ebacda5f1)
        ;

        
    
    
            var circle_marker_0a0db495717f468aa055a348781041ae = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2802459052404f70a4bdd3af008d6f2a = L.popup({"maxWidth": "100%"});

        
            var html_562a98023ac84a60bb90258540d2eaaf = $(`<div id="html_562a98023ac84a60bb90258540d2eaaf" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2005.13146">ACGAN-based Data Augmentation Integrated with Long-term Scalogram for Acoustic Scene Classification</a><br></div>`)[0];
            popup_2802459052404f70a4bdd3af008d6f2a.setContent(html_562a98023ac84a60bb90258540d2eaaf);
        

        circle_marker_0a0db495717f468aa055a348781041ae.bindPopup(popup_2802459052404f70a4bdd3af008d6f2a)
        ;

        
    
    
            var circle_marker_3386a80d029c4205b57524163ab50aef = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9297b9096c4947138dc358d65e902d34 = L.popup({"maxWidth": "100%"});

        
            var html_f7156f8eaa9b43d6a16679d7be4836b2 = $(`<div id="html_f7156f8eaa9b43d6a16679d7be4836b2" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://api.research-repository.uwa.edu.au/portalfiles/portal/50929769/THESIS_DOCTOR_OF_PHILOSOPHY_XIA_Xianjun_2019.pdf">Acoustic Event Detection Utilizing Event Class and Localization Information</a><br></div>`)[0];
            popup_9297b9096c4947138dc358d65e902d34.setContent(html_f7156f8eaa9b43d6a16679d7be4836b2);
        

        circle_marker_3386a80d029c4205b57524163ab50aef.bindPopup(popup_9297b9096c4947138dc358d65e902d34)
        ;

        
    
    
            var circle_marker_741bddecef6f4fac9711079b9d740786 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ceb5dd1dfff1496b9b1ed8d9e11dd0b9 = L.popup({"maxWidth": "100%"});

        
            var html_79b6e34b67424931b1825b121c9a9a5a = $(`<div id="html_79b6e34b67424931b1825b121c9a9a5a" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/1904.05204">Acoustic scene classification by implicitly identifying distinct sound events</a><br></div>`)[0];
            popup_ceb5dd1dfff1496b9b1ed8d9e11dd0b9.setContent(html_79b6e34b67424931b1825b121c9a9a5a);
        

        circle_marker_741bddecef6f4fac9711079b9d740786.bindPopup(popup_ceb5dd1dfff1496b9b1ed8d9e11dd0b9)
        ;

        
    
    
            var circle_marker_f1203a619d644121a68c1722f1160a36 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5d32e8a1e8334675a16536b18f47672e = L.popup({"maxWidth": "100%"});

        
            var html_cdd7c23e7dd24a4b8287b51479d363f1 = $(`<div id="html_cdd7c23e7dd24a4b8287b51479d363f1" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Yang_22.pdf">Acoustic scene classification using CNN ensembles and primary ambient extraction</a><br></div>`)[0];
            popup_5d32e8a1e8334675a16536b18f47672e.setContent(html_cdd7c23e7dd24a4b8287b51479d363f1);
        

        circle_marker_f1203a619d644121a68c1722f1160a36.bindPopup(popup_5d32e8a1e8334675a16536b18f47672e)
        ;

        
    
    
            var circle_marker_33d296b9f8f84297b37179c96afe5dc5 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1d7d4720f8d7485a803c3fa0a09b4c99 = L.popup({"maxWidth": "100%"});

        
            var html_a9e2820ea085490e97a031a3ef7d3fe7 = $(`<div id="html_a9e2820ea085490e97a031a3ef7d3fe7" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8455765/">Acoustic scene classification using deep audio feature and BLSTM network</a><br></div>`)[0];
            popup_1d7d4720f8d7485a803c3fa0a09b4c99.setContent(html_a9e2820ea085490e97a031a3ef7d3fe7);
        

        circle_marker_33d296b9f8f84297b37179c96afe5dc5.bindPopup(popup_1d7d4720f8d7485a803c3fa0a09b4c99)
        ;

        
    
    
            var circle_marker_e925d2e8152c4290975505516983cb72 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_393d865f226142ccaffad6676cd3d524 = L.popup({"maxWidth": "100%"});

        
            var html_f679b662ef9c4600925a7d07434eff71 = $(`<div id="html_f679b662ef9c4600925a7d07434eff71" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8897625/">Adaptive multi-scale detection of acoustic events</a><br></div>`)[0];
            popup_393d865f226142ccaffad6676cd3d524.setContent(html_f679b662ef9c4600925a7d07434eff71);
        

        circle_marker_e925d2e8152c4290975505516983cb72.bindPopup(popup_393d865f226142ccaffad6676cd3d524)
        ;

        
    
    
            var circle_marker_a298ab5310d84b9b86d111bfaef6366e = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6f91137aab20475eb2bf127d52aa54e3 = L.popup({"maxWidth": "100%"});

        
            var html_745bd366caec4e2a83955a1fb7a1723f = $(`<div id="html_745bd366caec4e2a83955a1fb7a1723f" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2008.00107">An Acoustic Segment Model Based Segment Unit Selection Approach to Acoustic Scene Classification with Partial Utterances</a><br></div>`)[0];
            popup_6f91137aab20475eb2bf127d52aa54e3.setContent(html_745bd366caec4e2a83955a1fb7a1723f);
        

        circle_marker_a298ab5310d84b9b86d111bfaef6366e.bindPopup(popup_6f91137aab20475eb2bf127d52aa54e3)
        ;

        
    
    
            var circle_marker_4481788f2bf74cca8b8614a60d8d3d47 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c311d78d29ae4f7ebf314d06461e9578 = L.popup({"maxWidth": "100%"});

        
            var html_19077f26047740b8a7d2638d637e6099 = $(`<div id="html_19077f26047740b8a7d2638d637e6099" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://joics.org/gallery/ics-3652.pdf">An Exploratory Study of Collaborative Filtering Vs. Content Based Movie Recommendation</a><br></div>`)[0];
            popup_c311d78d29ae4f7ebf314d06461e9578.setContent(html_19077f26047740b8a7d2638d637e6099);
        

        circle_marker_4481788f2bf74cca8b8614a60d8d3d47.bindPopup(popup_c311d78d29ae4f7ebf314d06461e9578)
        ;

        
    
    
            var circle_marker_8bd3db9f592e4178adeecafd41652537 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_89a51399b2b24e148d9dd8cc27174dc2 = L.popup({"maxWidth": "100%"});

        
            var html_afce99bc0fa14955b3b9365fab5ddfe9 = $(`<div id="html_afce99bc0fa14955b3b9365fab5ddfe9" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8706712/">An investigation of transfer learning mechanism for acoustic scene classification</a><br></div>`)[0];
            popup_89a51399b2b24e148d9dd8cc27174dc2.setContent(html_afce99bc0fa14955b3b9365fab5ddfe9);
        

        circle_marker_8bd3db9f592e4178adeecafd41652537.bindPopup(popup_89a51399b2b24e148d9dd8cc27174dc2)
        ;

        
    
    
            var circle_marker_1748160bc9104624b60698d5916ac75c = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_76da9bfd2b934acb87cfe6680c8d9c7f = L.popup({"maxWidth": "100%"});

        
            var html_2389cfabe1844998a67e67d494a803fc = $(`<div id="html_2389cfabe1844998a67e67d494a803fc" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://ir.ia.ac.cn/bitstream/173211/21601/1/bare_conf.pdf">Anomaly Detection via Minimum Likelihood Generative Adversarial Networks</a><br></div>`)[0];
            popup_76da9bfd2b934acb87cfe6680c8d9c7f.setContent(html_2389cfabe1844998a67e67d494a803fc);
        

        circle_marker_1748160bc9104624b60698d5916ac75c.bindPopup(popup_76da9bfd2b934acb87cfe6680c8d9c7f)
        ;

        
    
    
            var circle_marker_83f8bb5a57944be3985b4f7639a66b56 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7b00cd3038454a489b0ee6a42dcc4e6d = L.popup({"maxWidth": "100%"});

        
            var html_f258bc26168a44eba41d09dd0e8604e3 = $(`<div id="html_f258bc26168a44eba41d09dd0e8604e3" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8545381/">Anomaly detection via minimum likelihood generative adversarial networks</a><br></div>`)[0];
            popup_7b00cd3038454a489b0ee6a42dcc4e6d.setContent(html_f258bc26168a44eba41d09dd0e8604e3);
        

        circle_marker_83f8bb5a57944be3985b4f7639a66b56.bindPopup(popup_7b00cd3038454a489b0ee6a42dcc4e6d)
        ;

        
    
    
            var circle_marker_936468aaa9f04c518a3c32b380626619 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_eaa847691fc84d7faab3311da1fa735f = L.popup({"maxWidth": "100%"});

        
            var html_a7de7af5fbbe421da5f2c5ebe5da11f2 = $(`<div id="html_a7de7af5fbbe421da5f2c5ebe5da11f2" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_Zhang_78.pdf">BUPT SUBMISSIONS TO DCASE 2020: LOW-COMPLEXITY ACOUSTIC SCENE CLASSIFICATION WITH POST TRAINING STATIC QUANTIZATION AND …</a><br></div>`)[0];
            popup_eaa847691fc84d7faab3311da1fa735f.setContent(html_a7de7af5fbbe421da5f2c5ebe5da11f2);
        

        circle_marker_936468aaa9f04c518a3c32b380626619.bindPopup(popup_eaa847691fc84d7faab3311da1fa735f)
        ;

        
    
    
            var circle_marker_f9da4c00d23742f9bb127a1ee9822261 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_08aa89e399804323bb411a3f699aaf74 = L.popup({"maxWidth": "100%"});

        
            var html_0bf5efba60864ca588b5e9bb710d37e3 = $(`<div id="html_0bf5efba60864ca588b5e9bb710d37e3" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8689028/">CMBPR: Category-Aided Multi-Channel Bayesian Personalized Ranking for Short Video Recommendation</a><br></div>`)[0];
            popup_08aa89e399804323bb411a3f699aaf74.setContent(html_0bf5efba60864ca588b5e9bb710d37e3);
        

        circle_marker_f9da4c00d23742f9bb127a1ee9822261.bindPopup(popup_08aa89e399804323bb411a3f699aaf74)
        ;

        
    
    
            var circle_marker_0b627bc03938491fa7ac141a045df0ae = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_700d78d3416846779118cf67065e0fed = L.popup({"maxWidth": "100%"});

        
            var html_4476d642ed10423d9fbafe16aeb65b7a = $(`<div id="html_4476d642ed10423d9fbafe16aeb65b7a" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Zhu_52.pdf">DCASE 2019 challenge task1 technical report</a><br></div>`)[0];
            popup_700d78d3416846779118cf67065e0fed.setContent(html_4476d642ed10423d9fbafe16aeb65b7a);
        

        circle_marker_0b627bc03938491fa7ac141a045df0ae.bindPopup(popup_700d78d3416846779118cf67065e0fed)
        ;

        
    
    
            var circle_marker_c30efdcfcbd5475fb5eb287800d96d89 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c686c01bc8d84c458b4fe3d938f06e56 = L.popup({"maxWidth": "100%"});

        
            var html_332029d3c53e4bf58250cfc4c0498925 = $(`<div id="html_332029d3c53e4bf58250cfc4c0498925" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2007.12864">DD-CNN: Depthwise Disout Convolutional Neural Network for Low-complexity Acoustic Scene Classification</a><br></div>`)[0];
            popup_c686c01bc8d84c458b4fe3d938f06e56.setContent(html_332029d3c53e4bf58250cfc4c0498925);
        

        circle_marker_c30efdcfcbd5475fb5eb287800d96d89.bindPopup(popup_c686c01bc8d84c458b4fe3d938f06e56)
        ;

        
    
    
            var circle_marker_db7b53b828fe41ce8e286ef7711928ae = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1dac2608635c4e77836f3478eefcd6da = L.popup({"maxWidth": "100%"});

        
            var html_668601a28ab3481cb69a25e8f8226e79 = $(`<div id="html_668601a28ab3481cb69a25e8f8226e79" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053459/">Duration robust weakly supervised sound event detection</a><br></div>`)[0];
            popup_1dac2608635c4e77836f3478eefcd6da.setContent(html_668601a28ab3481cb69a25e8f8226e79);
        

        circle_marker_db7b53b828fe41ce8e286ef7711928ae.bindPopup(popup_1dac2608635c4e77836f3478eefcd6da)
        ;

        
    
    
            var circle_marker_233e4bfc9c34456ba63fc0967b6035df = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0f089518b10249b2bcfac102364cb92f = L.popup({"maxWidth": "100%"});

        
            var html_ba2f85674838462da236007dd1ae6aa2 = $(`<div id="html_ba2f85674838462da236007dd1ae6aa2" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://web.pkusz.edu.cn/adsp/files/2020/08/Interspeech2020_%E7%8E%8B%E8%B5%AB%E9%BA%9F_Environmental-Sound-Classification-with-Parallel-Temporal-spectral-Attention.pdf">Environmental sound classification with parallel temporal-spectral attention</a><br></div>`)[0];
            popup_0f089518b10249b2bcfac102364cb92f.setContent(html_ba2f85674838462da236007dd1ae6aa2);
        

        circle_marker_233e4bfc9c34456ba63fc0967b6035df.bindPopup(popup_0f089518b10249b2bcfac102364cb92f)
        ;

        
    
    
            var circle_marker_8ee909f7895c4b568021d96d89cb8981 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f73107fb168a46e9965e19f66c504eb0 = L.popup({"maxWidth": "100%"});

        
            var html_2073f3125e6a4bb8b7e0998d9c9f96d2 = $(`<div id="html_2073f3125e6a4bb8b7e0998d9c9f96d2" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8869872/">Feature Re-Learning with Data Augmentation for Video Relevance Prediction</a><br></div>`)[0];
            popup_f73107fb168a46e9965e19f66c504eb0.setContent(html_2073f3125e6a4bb8b7e0998d9c9f96d2);
        

        circle_marker_8ee909f7895c4b568021d96d89cb8981.bindPopup(popup_f73107fb168a46e9965e19f66c504eb0)
        ;

        
    
    
            var circle_marker_1faa9ddc4d1d44668f18fefd835c417d = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_abc91fe17ff4448690f6f96cfee84963 = L.popup({"maxWidth": "100%"});

        
            var html_cd00c5f88601464ca2a20bb24773efed = $(`<div id="html_cd00c5f88601464ca2a20bb24773efed" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Xu_131.pdf">Fusion model based on convolutional neural networks with two features for acoustic scene classification</a><br></div>`)[0];
            popup_abc91fe17ff4448690f6f96cfee84963.setContent(html_cd00c5f88601464ca2a20bb24773efed);
        

        circle_marker_1faa9ddc4d1d44668f18fefd835c417d.bindPopup(popup_abc91fe17ff4448690f6f96cfee84963)
        ;

        
    
    
            var circle_marker_3db42977313445a08415e719d4edeeb6 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_edf1f8069578495aa7f053c084eca0a2 = L.popup({"maxWidth": "100%"});

        
            var html_ab9ae9099bef4c399fa86d0d149c4138 = $(`<div id="html_ab9ae9099bef4c399fa86d0d149c4138" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2003.12222">GPVAD: Towards noise robust voice activity detection via weakly supervised sound event detection</a><br></div>`)[0];
            popup_edf1f8069578495aa7f053c084eca0a2.setContent(html_ab9ae9099bef4c399fa86d0d149c4138);
        

        circle_marker_3db42977313445a08415e719d4edeeb6.bindPopup(popup_edf1f8069578495aa7f053c084eca0a2)
        ;

        
    
    
            var circle_marker_bfc5b6ff829743949be0df63a548c99a = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_df506d07e0414cd691465a2728c52da6 = L.popup({"maxWidth": "100%"});

        
            var html_31ad0ed3f1ad42d182a9d120728964b5 = $(`<div id="html_31ad0ed3f1ad42d182a9d120728964b5" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/1909.06178">Guided learning convolution system for dcase 2019 task 4</a><br></div>`)[0];
            popup_df506d07e0414cd691465a2728c52da6.setContent(html_31ad0ed3f1ad42d182a9d120728964b5);
        

        circle_marker_bfc5b6ff829743949be0df63a548c99a.bindPopup(popup_df506d07e0414cd691465a2728c52da6)
        ;

        
    
    
            var circle_marker_9e15813d21744b8f94aa434d085b0263 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_01b8516585d24233ad7b34a3eb141acf = L.popup({"maxWidth": "100%"});

        
            var html_e5ad04944df941b2811026968c20aa1a = $(`<div id="html_e5ad04944df941b2811026968c20aa1a" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053584/">Guided learning for weakly-labeled semi-supervised sound event detection</a><br></div>`)[0];
            popup_01b8516585d24233ad7b34a3eb141acf.setContent(html_e5ad04944df941b2811026968c20aa1a);
        

        circle_marker_9e15813d21744b8f94aa434d085b0263.bindPopup(popup_01b8516585d24233ad7b34a3eb141acf)
        ;

        
    
    
            var circle_marker_9ca5c9d9a8ee4cf98bc78f4964a9c1f3 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_74c5b4ed154347389772a56b920f1b58 = L.popup({"maxWidth": "100%"});

        
            var html_1790b0098a1e425ba72cecf6cd287423 = $(`<div id="html_1790b0098a1e425ba72cecf6cd287423" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053519/">High-Resolution Attention Network with Acoustic Segment Model for Acoustic Scene Classification</a><br></div>`)[0];
            popup_74c5b4ed154347389772a56b920f1b58.setContent(html_1790b0098a1e425ba72cecf6cd287423);
        

        circle_marker_9ca5c9d9a8ee4cf98bc78f4964a9c1f3.bindPopup(popup_74c5b4ed154347389772a56b920f1b58)
        ;

        
    
    
            var circle_marker_c24a4a16f98440f59a571a452891eeea = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0790832859774edeafed75120814591f = L.popup({"maxWidth": "100%"});

        
            var html_81705a734fdd44e69135b2531933bab4 = $(`<div id="html_81705a734fdd44e69135b2531933bab4" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9023236/">Hybrid Constant-Q Transform Based CNN Ensemble for Acoustic Scene Classification</a><br></div>`)[0];
            popup_0790832859774edeafed75120814591f.setContent(html_81705a734fdd44e69135b2531933bab4);
        

        circle_marker_c24a4a16f98440f59a571a452891eeea.bindPopup(popup_0790832859774edeafed75120814591f)
        ;

        
    
    
            var circle_marker_0f711567fbae467eb6b19330064598d7 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_640170dabda74bce9ec0baf38b2d7b6e = L.popup({"maxWidth": "100%"});

        
            var html_9744e03119884fd1a6e26623c26eb115 = $(`<div id="html_9744e03119884fd1a6e26623c26eb115" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0957417419301794">Hybrid feature-based analysis of video's affective content using protagonist detection</a><br></div>`)[0];
            popup_640170dabda74bce9ec0baf38b2d7b6e.setContent(html_9744e03119884fd1a6e26623c26eb115);
        

        circle_marker_0f711567fbae467eb6b19330064598d7.bindPopup(popup_640170dabda74bce9ec0baf38b2d7b6e)
        ;

        
    
    
            var circle_marker_d94552feeb0f499d99b522a8d03b56c1 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f15f9c178c814faabe320fc78514aecb = L.popup({"maxWidth": "100%"});

        
            var html_687dc611b49a46389629a35f8eb6722a = $(`<div id="html_687dc611b49a46389629a35f8eb6722a" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2004.02182">Imbalanced Data Learning by Minority Class Augmentation using Capsule Adversarial Networks</a><br></div>`)[0];
            popup_f15f9c178c814faabe320fc78514aecb.setContent(html_687dc611b49a46389629a35f8eb6722a);
        

        circle_marker_d94552feeb0f499d99b522a8d03b56c1.bindPopup(popup_f15f9c178c814faabe320fc78514aecb)
        ;

        
    
    
            var circle_marker_5d0285cff85c46ddbb0e9e5b0f83bc0e = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ffb617b1b6e2471d95bd8a315be454e5 = L.popup({"maxWidth": "100%"});

        
            var html_052692d45b2447a4b0d90ecf236adde7 = $(`<div id="html_052692d45b2447a4b0d90ecf236adde7" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.mdpi.com/1099-4300/22/9/1055/htm">Improving Multi-Agent Generative Adversarial Nets with Variational Latent Representation</a><br></div>`)[0];
            popup_ffb617b1b6e2471d95bd8a315be454e5.setContent(html_052692d45b2447a4b0d90ecf236adde7);
        

        circle_marker_5d0285cff85c46ddbb0e9e5b0f83bc0e.bindPopup(popup_ffb617b1b6e2471d95bd8a315be454e5)
        ;

        
    
    
            var circle_marker_ddaa362179234a028d2a20a77e6bb96a = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c2dc624510e74bfab1a4cb5f8753a297 = L.popup({"maxWidth": "100%"});

        
            var html_2cc364a1a51e4476b5597f5db5129229 = $(`<div id="html_2cc364a1a51e4476b5597f5db5129229" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/1907.06639">Integrating the data augmentation scheme with various classifiers for acoustic scene modeling</a><br></div>`)[0];
            popup_c2dc624510e74bfab1a4cb5f8753a297.setContent(html_2cc364a1a51e4476b5597f5db5129229);
        

        circle_marker_ddaa362179234a028d2a20a77e6bb96a.bindPopup(popup_c2dc624510e74bfab1a4cb5f8753a297)
        ;

        
    
    
            var circle_marker_1c0ddcf16a5a4541a74bc81560cf755d = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c611e35a99f94383b7910b74e8e4e10a = L.popup({"maxWidth": "100%"});

        
            var html_08fad6805a4843a0b4d3d4d3844855c6 = $(`<div id="html_08fad6805a4843a0b4d3d4d3844855c6" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.ijcai.org/Proceedings/2020/0379.pdf">Internal and Contextual Attention Network for Cold-start Multi-channel Matching in Recommendation</a><br></div>`)[0];
            popup_c611e35a99f94383b7910b74e8e4e10a.setContent(html_08fad6805a4843a0b4d3d4d3844855c6);
        

        circle_marker_1c0ddcf16a5a4541a74bc81560cf755d.bindPopup(popup_c611e35a99f94383b7910b74e8e4e10a)
        ;

        
    
    
            var circle_marker_548f9b1f6f084b28aa458507eaf8bc39 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5a3a60295c9045d1bce2dcc483ad99bd = L.popup({"maxWidth": "100%"});

        
            var html_fedff6ebd4c7459eae5494c3ee5bf2dd = $(`<div id="html_fedff6ebd4c7459eae5494c3ee5bf2dd" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8922774/">Investigation of Different CNN-Based Models for Improved Bird Sound Classification</a><br></div>`)[0];
            popup_5a3a60295c9045d1bce2dcc483ad99bd.setContent(html_fedff6ebd4c7459eae5494c3ee5bf2dd);
        

        circle_marker_548f9b1f6f084b28aa458507eaf8bc39.bindPopup(popup_5a3a60295c9045d1bce2dcc483ad99bd)
        ;

        
    
    
            var circle_marker_5eda4e011e424bb2a915ba9353b37a8d = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f7436999527e45ee9c89a88af78b2955 = L.popup({"maxWidth": "100%"});

        
            var html_314b54ffda6f422998cd6765cb095454 = $(`<div id="html_314b54ffda6f422998cd6765cb095454" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0957417419300661">Investigation of acoustic and visual features for acoustic scene classification</a><br></div>`)[0];
            popup_f7436999527e45ee9c89a88af78b2955.setContent(html_314b54ffda6f422998cd6765cb095454);
        

        circle_marker_5eda4e011e424bb2a915ba9353b37a8d.bindPopup(popup_f7436999527e45ee9c89a88af78b2955)
        ;

        
    
    
            var circle_marker_dca98949022c4137a4107bc60c08e70f = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7f2b23416b254c61b4e4cf15f569efc2 = L.popup({"maxWidth": "100%"});

        
            var html_789c5b4761354672b3ae98b066dd5808 = $(`<div id="html_789c5b4761354672b3ae98b066dd5808" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8823934/">Learning attentive representations for environmental sound classification</a><br></div>`)[0];
            popup_7f2b23416b254c61b4e4cf15f569efc2.setContent(html_789c5b4761354672b3ae98b066dd5808);
        

        circle_marker_dca98949022c4137a4107bc60c08e70f.bindPopup(popup_7f2b23416b254c61b4e4cf15f569efc2)
        ;

        
    
    
            var circle_marker_c2d22a7082e0492296e46c576cd9a595 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9146bda5ef2b43a7b37a027917328761 = L.popup({"maxWidth": "100%"});

        
            var html_2ba724e35b0d4aaabbb352a6c71ac512 = $(`<div id="html_2ba724e35b0d4aaabbb352a6c71ac512" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-00764-5_2">Mixup-based acoustic scene classification using multi-channel convolutional neural network</a><br></div>`)[0];
            popup_9146bda5ef2b43a7b37a027917328761.setContent(html_2ba724e35b0d4aaabbb352a6c71ac512);
        

        circle_marker_c2d22a7082e0492296e46c576cd9a595.bindPopup(popup_9146bda5ef2b43a7b37a027917328761)
        ;

        
    
    
            var circle_marker_3216a42746c943f3b9bd8889a450e26e = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_911146b6041543cf9967109cb0022acc = L.popup({"maxWidth": "100%"});

        
            var html_03921c6f1d5f43059eb8cce8ef2cc076 = $(`<div id="html_03921c6f1d5f43059eb8cce8ef2cc076" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053023/">Multi-Branch Learning for Weakly-Labeled Sound Event Detection</a><br></div>`)[0];
            popup_911146b6041543cf9967109cb0022acc.setContent(html_03921c6f1d5f43059eb8cce8ef2cc076);
        

        circle_marker_3216a42746c943f3b9bd8889a450e26e.bindPopup(popup_911146b6041543cf9967109cb0022acc)
        ;

        
    
    
            var circle_marker_5369de10b2c04936b45c036b62cefc1e = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d97254727d5a413e92c78c36ecbf715f = L.popup({"maxWidth": "100%"});

        
            var html_b2e69a6d7d1949449dc59f3aedcf3a0e = $(`<div id="html_b2e69a6d7d1949449dc59f3aedcf3a0e" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8576017/">Multi-LCNN: A Hybrid Neural Network Based on Integrated Time-Frequency Characteristics for Acoustic Scene Classification</a><br></div>`)[0];
            popup_d97254727d5a413e92c78c36ecbf715f.setContent(html_b2e69a6d7d1949449dc59f3aedcf3a0e);
        

        circle_marker_5369de10b2c04936b45c036b62cefc1e.bindPopup(popup_d97254727d5a413e92c78c36ecbf715f)
        ;

        
    
    
            var circle_marker_e93f2a8e061f4d689c47f7eb985f2f91 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9129b06746d24af38ab97a2656d9b5f4 = L.popup({"maxWidth": "100%"});

        
            var html_b879203c20b34c2ea16b584848f9bd80 = $(`<div id="html_b879203c20b34c2ea16b584848f9bd80" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8925176/">Multi-scale convolutional recurrent neural network with ensemble method for weakly labeled sound event detection</a><br></div>`)[0];
            popup_9129b06746d24af38ab97a2656d9b5f4.setContent(html_b879203c20b34c2ea16b584848f9bd80);
        

        circle_marker_e93f2a8e061f4d689c47f7eb985f2f91.bindPopup(popup_9129b06746d24af38ab97a2656d9b5f4)
        ;

        
    
    
            var circle_marker_e1de0fedff1f4fadb8ba089566d3fbb1 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e95325dca76d4aaab063013a06777294 = L.popup({"maxWidth": "100%"});

        
            var html_fc2e9b717bd7420c933db6c97e3d16f1 = $(`<div id="html_fc2e9b717bd7420c933db6c97e3d16f1" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0003682X19312897">Multi-scale semantic feature fusion and data augmentation for acoustic scene classification</a><br></div>`)[0];
            popup_e95325dca76d4aaab063013a06777294.setContent(html_fc2e9b717bd7420c933db6c97e3d16f1);
        

        circle_marker_e1de0fedff1f4fadb8ba089566d3fbb1.bindPopup(popup_e95325dca76d4aaab063013a06777294)
        ;

        
    
    
            var circle_marker_27a6fe4d54ed40dd95826fc78cc86ce0 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a0b7890b0f7747b0a67d15f40746e637 = L.popup({"maxWidth": "100%"});

        
            var html_303f2163804d48ea8fe5303b7318d576 = $(`<div id="html_303f2163804d48ea8fe5303b7318d576" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2002.00522">Novelty Detection via Non-Adversarial Generative Network</a><br></div>`)[0];
            popup_a0b7890b0f7747b0a67d15f40746e637.setContent(html_303f2163804d48ea8fe5303b7318d576);
        

        circle_marker_27a6fe4d54ed40dd95826fc78cc86ce0.bindPopup(popup_a0b7890b0f7747b0a67d15f40746e637)
        ;

        
    
    
            var circle_marker_ed77399965b34b3f8b679b1080680432 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c125ff2c78a74ea99a0308022f32b763 = L.popup({"maxWidth": "100%"});

        
            var html_6ab543edaf064b9c9fc42ca33cbd92c8 = $(`<div id="html_6ab543edaf064b9c9fc42ca33cbd92c8" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0888327020305616">Oversampling adversarial network for class-imbalanced fault diagnosis</a><br></div>`)[0];
            popup_c125ff2c78a74ea99a0308022f32b763.setContent(html_6ab543edaf064b9c9fc42ca33cbd92c8);
        

        circle_marker_ed77399965b34b3f8b679b1080680432.bindPopup(popup_c125ff2c78a74ea99a0308022f32b763)
        ;

        
    
    
            var circle_marker_165f00cf89be41ec916cd5e3fe84ee06 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_32a16d55c1d84fc2b52a65abde2dd5d8 = L.popup({"maxWidth": "100%"});

        
            var html_fe4ad37a0879422bb31a9a32e1884e0e = $(`<div id="html_fe4ad37a0879422bb31a9a32e1884e0e" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2005.11459">Power Pooling Operators and Confidence Learning for Semi-Supervised Sound Event Detection</a><br></div>`)[0];
            popup_32a16d55c1d84fc2b52a65abde2dd5d8.setContent(html_fe4ad37a0879422bb31a9a32e1884e0e);
        

        circle_marker_165f00cf89be41ec916cd5e3fe84ee06.bindPopup(popup_32a16d55c1d84fc2b52a65abde2dd5d8)
        ;

        
    
    
            var circle_marker_6edd17eba7074ba68d80b925ddadb402 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b38509b02b814b9480297d7fc8c8a890 = L.popup({"maxWidth": "100%"});

        
            var html_0f22d7c056b3403f89be5f5c02d50255 = $(`<div id="html_0f22d7c056b3403f89be5f5c02d50255" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://arxiv.org/abs/2008.00110">Relational Teacher Student Learning with Neural Label Embedding for Device Adaptation in Acoustic Scene Classification</a><br></div>`)[0];
            popup_b38509b02b814b9480297d7fc8c8a890.setContent(html_0f22d7c056b3403f89be5f5c02d50255);
        

        circle_marker_6edd17eba7074ba68d80b925ddadb402.bindPopup(popup_b38509b02b814b9480297d7fc8c8a890)
        ;

        
    
    
            var circle_marker_480555115e884ca4827e0f9fe4012677 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ea4ed376097145e2a496b2333e8e8e26 = L.popup({"maxWidth": "100%"});

        
            var html_f5103bf4428541e3b6ce540492c14ba6 = $(`<div id="html_f5103bf4428541e3b6ce540492c14ba6" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11859-018-1308-z">Shallow convolutional neural networks for acoustic scene classification</a><br></div>`)[0];
            popup_ea4ed376097145e2a496b2333e8e8e26.setContent(html_f5103bf4428541e3b6ce540492c14ba6);
        

        circle_marker_480555115e884ca4827e0f9fe4012677.bindPopup(popup_ea4ed376097145e2a496b2333e8e8e26)
        ;

        
    
    
            var circle_marker_c83cd478f57b4418bb4a58be33e70ea4 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2465e21152d940fe8950f332451cddba = L.popup({"maxWidth": "100%"});

        
            var html_af324cc62fd14744b2d1e4c331e2c1da = $(`<div id="html_af324cc62fd14744b2d1e4c331e2c1da" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9103031/">Sound Event Detection Using Multiple Optimized Kernels</a><br></div>`)[0];
            popup_2465e21152d940fe8950f332451cddba.setContent(html_af324cc62fd14744b2d1e4c331e2c1da);
        

        circle_marker_c83cd478f57b4418bb4a58be33e70ea4.bindPopup(popup_2465e21152d940fe8950f332451cddba)
        ;

        
    
    
            var circle_marker_34484f9ec43e42dd9eec61b6a91a5362 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_71a97d65693340b788e656658affbed7 = L.popup({"maxWidth": "100%"});

        
            var html_66d94534ddd844b4856417956a46132e = $(`<div id="html_66d94534ddd844b4856417956a46132e" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9076321/">Specialized decision surface and disentangled feature for weakly-supervised polyphonic sound event detection</a><br></div>`)[0];
            popup_71a97d65693340b788e656658affbed7.setContent(html_66d94534ddd844b4856417956a46132e);
        

        circle_marker_34484f9ec43e42dd9eec61b6a91a5362.bindPopup(popup_71a97d65693340b788e656658affbed7)
        ;

        
    
    
            var circle_marker_4e226b64faa24f40b2cef533aad2d13e = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fc5cd401ae5449fc93fb1010b63d3d59 = L.popup({"maxWidth": "100%"});

        
            var html_0a6fd3e5afc94919acc58561f2512e1e = $(`<div id="html_0a6fd3e5afc94919acc58561f2512e1e" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8905033/">Speech Emotion Recognition with Hybrid Neural Network</a><br></div>`)[0];
            popup_fc5cd401ae5449fc93fb1010b63d3d59.setContent(html_0a6fd3e5afc94919acc58561f2512e1e);
        

        circle_marker_4e226b64faa24f40b2cef533aad2d13e.bindPopup(popup_fc5cd401ae5449fc93fb1010b63d3d59)
        ;

        
    
    
            var circle_marker_da00b8e0180545ac93064d4efc6d0c98 = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fa46d52ce61948898b5dcc45865ccc55 = L.popup({"maxWidth": "100%"});

        
            var html_36fb22b14fd647ae95deec2a9e228fdb = $(`<div id="html_36fb22b14fd647ae95deec2a9e228fdb" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0925231219316996">TNAM: A tag-aware neural attention model for Top-N recommendation</a><br></div>`)[0];
            popup_fa46d52ce61948898b5dcc45865ccc55.setContent(html_36fb22b14fd647ae95deec2a9e228fdb);
        

        circle_marker_da00b8e0180545ac93064d4efc6d0c98.bindPopup(popup_fa46d52ce61948898b5dcc45865ccc55)
        ;

        
    
    
            var circle_marker_7d4b3c0b577b43d98cf49e87c57c734e = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1bd0529bcfaa4c568ea6dc329d61f4f0 = L.popup({"maxWidth": "100%"});

        
            var html_a6aded7a8f134416a8a1107e55a22ced = $(`<div id="html_a6aded7a8f134416a8a1107e55a22ced" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8492416/">Transfer learning for wearable long-term social speech evaluations</a><br></div>`)[0];
            popup_1bd0529bcfaa4c568ea6dc329d61f4f0.setContent(html_a6aded7a8f134416a8a1107e55a22ced);
        

        circle_marker_7d4b3c0b577b43d98cf49e87c57c734e.bindPopup(popup_1bd0529bcfaa4c568ea6dc329d61f4f0)
        ;

        
    
    
            var circle_marker_2fc30f2da0b24f9bb316bc5d0d61a10a = L.circleMarker(
                [35.000074, 104.999927],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_63e41ed7a4c84468ba98b9e278f854b4 = L.popup({"maxWidth": "100%"});

        
            var html_df333e02534f497faa9e17b33dbaec0a = $(`<div id="html_df333e02534f497faa9e17b33dbaec0a" style="width: 100.0%; height: 100.0%;">Country : China<br>                         Paper : <a href="http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Liu_69.pdf">Ustc-nelslip system for dcase 2018 challenge task 4</a><br></div>`)[0];
            popup_63e41ed7a4c84468ba98b9e278f854b4.setContent(html_df333e02534f497faa9e17b33dbaec0a);
        

        circle_marker_2fc30f2da0b24f9bb316bc5d0d61a10a.bindPopup(popup_63e41ed7a4c84468ba98b9e278f854b4)
        ;

        
    
    
            var circle_marker_78d3faab426a4e4b883dec85416e4959 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1cb29b3611ea4f0ca9d1cffd01bd8826 = L.popup({"maxWidth": "100%"});

        
            var html_3519f2e29ccb42a186aee1aaf707a532 = $(`<div id="html_3519f2e29ccb42a186aee1aaf707a532" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2007.00222">A Transformer-based Audio Captioning Model with Keyword Estimation</a><br></div>`)[0];
            popup_1cb29b3611ea4f0ca9d1cffd01bd8826.setContent(html_3519f2e29ccb42a186aee1aaf707a532);
        

        circle_marker_78d3faab426a4e4b883dec85416e4959.bindPopup(popup_1cb29b3611ea4f0ca9d1cffd01bd8826)
        ;

        
    
    
            var circle_marker_bde713be736a4ac38b3d7d57930a0641 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_09b40994338a4f279086d6cd7a4aabb5 = L.popup({"maxWidth": "100%"});

        
            var html_4c8478a5cd564614a8443fa005b9fb79 = $(`<div id="html_4c8478a5cd564614a8443fa005b9fb79" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553423/">Anomalous sound event detection based on wavenet</a><br></div>`)[0];
            popup_09b40994338a4f279086d6cd7a4aabb5.setContent(html_4c8478a5cd564614a8443fa005b9fb79);
        

        circle_marker_bde713be736a4ac38b3d7d57930a0641.bindPopup(popup_09b40994338a4f279086d6cd7a4aabb5)
        ;

        
    
    
            var circle_marker_529eb3d01edd4e75a166d441d4dc5a59 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b7d6baf0b8ee469381a4c808442e5566 = L.popup({"maxWidth": "100%"});

        
            var html_a223091fd53b431c87924582271a1eb2 = $(`<div id="html_a223091fd53b431c87924582271a1eb2" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2002.06021">Hodge and Podge: Hybrid Supervised Sound Event Detection with Multi-Hot MixMatch and Composition Consistence Training</a><br></div>`)[0];
            popup_b7d6baf0b8ee469381a4c808442e5566.setContent(html_a223091fd53b431c87924582271a1eb2);
        

        circle_marker_529eb3d01edd4e75a166d441d4dc5a59.bindPopup(popup_b7d6baf0b8ee469381a4c808442e5566)
        ;

        
    
    
            var circle_marker_abde2a008fda4f4d9ba9adf35b5d06d9 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b311613fe7a24637939d8feb1b7aeef4 = L.popup({"maxWidth": "100%"});

        
            var html_b6581168388f431aa57175cddd8a2e6a = $(`<div id="html_b6581168388f431aa57175cddd8a2e6a" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/1907.07398">Hodgepodge: Sound event detection based on ensemble of semi-supervised learning methods</a><br></div>`)[0];
            popup_b311613fe7a24637939d8feb1b7aeef4.setContent(html_b6581168388f431aa57175cddd8a2e6a);
        

        circle_marker_abde2a008fda4f4d9ba9adf35b5d06d9.bindPopup(popup_b311613fe7a24637939d8feb1b7aeef4)
        ;

        
    
    
            var circle_marker_f31149999f76491da87ebce3c5a164af = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_24aba679194b47e6b240881715360f1c = L.popup({"maxWidth": "100%"});

        
            var html_aa71dad687eb4d20bc12a91076afa450 = $(`<div id="html_aa71dad687eb4d20bc12a91076afa450" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2004.02182">Imbalanced Data Learning by Minority Class Augmentation using Capsule Adversarial Networks</a><br></div>`)[0];
            popup_24aba679194b47e6b240881715360f1c.setContent(html_aa71dad687eb4d20bc12a91076afa450);
        

        circle_marker_f31149999f76491da87ebce3c5a164af.bindPopup(popup_24aba679194b47e6b240881715360f1c)
        ;

        
    
    
            var circle_marker_7fe1d34e9e294b7bb529a2d11d13c215 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d8c1caad55d04d958c860a822083a4a3 = L.popup({"maxWidth": "100%"});

        
            var html_db09474b11bb47289b04fe690c2fe432 = $(`<div id="html_db09474b11bb47289b04fe690c2fe432" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://archives.ismir.net/ismir2019/paper/000003.pdf">Intelligent User Interfaces for Music Discovery: The Past 20 Years and What's to Come.</a><br></div>`)[0];
            popup_d8c1caad55d04d958c860a822083a4a3.setContent(html_db09474b11bb47289b04fe690c2fe432);
        

        circle_marker_7fe1d34e9e294b7bb529a2d11d13c215.bindPopup(popup_d8c1caad55d04d958c860a822083a4a3)
        ;

        
    
    
            var circle_marker_8465220f7902425ab87ceed77411b1bb = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5de3f66a7bdf4955b5e17e14ff5dadb8 = L.popup({"maxWidth": "100%"});

        
            var html_8158f32b6ac244c78d6f1193b80163a9 = $(`<div id="html_8158f32b6ac244c78d6f1193b80163a9" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8682772/">Joint acoustic and class inference for weakly supervised sound event detection</a><br></div>`)[0];
            popup_5de3f66a7bdf4955b5e17e14ff5dadb8.setContent(html_8158f32b6ac244c78d6f1193b80163a9);
        

        circle_marker_8465220f7902425ab87ceed77411b1bb.bindPopup(popup_5de3f66a7bdf4955b5e17e14ff5dadb8)
        ;

        
    
    
            var circle_marker_e0257501f2e84a02b26350d40053a3f8 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_aa375b69b67c4eea85460dc966dfe231 = L.popup({"maxWidth": "100%"});

        
            var html_f19921a1aaa241cdb7ac368ffdae90b3 = $(`<div id="html_f19921a1aaa241cdb7ac368ffdae90b3" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3241620">Multimedia recommender systems</a><br></div>`)[0];
            popup_aa375b69b67c4eea85460dc966dfe231.setContent(html_f19921a1aaa241cdb7ac368ffdae90b3);
        

        circle_marker_e0257501f2e84a02b26350d40053a3f8.bindPopup(popup_aa375b69b67c4eea85460dc966dfe231)
        ;

        
    
    
            var circle_marker_e67511fa69694b37b66689df002d43e8 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4b7c9536f5b34dd4b8f1fa6f194b0b74 = L.popup({"maxWidth": "100%"});

        
            var html_55b5e1c6651c462e93272729f5d58fc9 = $(`<div id="html_55b5e1c6651c462e93272729f5d58fc9" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2004.04371">Music Artist Classification with WaveNet Classifier for Raw Waveform Audio Data</a><br></div>`)[0];
            popup_4b7c9536f5b34dd4b8f1fa6f194b0b74.setContent(html_55b5e1c6651c462e93272729f5d58fc9);
        

        circle_marker_e67511fa69694b37b66689df002d43e8.bindPopup(popup_4b7c9536f5b34dd4b8f1fa6f194b0b74)
        ;

        
    
    
            var circle_marker_ca52162cd6374b2697f2240194ac155b = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_853f4cdf95ee466b81332aa4a8d118c9 = L.popup({"maxWidth": "100%"});

        
            var html_670b0efe674949548b966356b370f680 = $(`<div id="html_670b0efe674949548b966356b370f680" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0888327020305616">Oversampling adversarial network for class-imbalanced fault diagnosis</a><br></div>`)[0];
            popup_853f4cdf95ee466b81332aa4a8d118c9.setContent(html_670b0efe674949548b966356b370f680);
        

        circle_marker_ca52162cd6374b2697f2240194ac155b.bindPopup(popup_853f4cdf95ee466b81332aa4a8d118c9)
        ;

        
    
    
            var circle_marker_d2abfba790df49b9bd6b02ce5330b8ec = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3485b75624ab47ebbae58d503def8d10 = L.popup({"maxWidth": "100%"});

        
            var html_d92afe12ed094d1abe841bad5dbe6ccc = $(`<div id="html_d92afe12ed094d1abe841bad5dbe6ccc" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683068/">Scene-dependent Anomalous Acoustic-event Detection Based on Conditional Wavenet and I-vector</a><br></div>`)[0];
            popup_3485b75624ab47ebbae58d503def8d10.setContent(html_d92afe12ed094d1abe841bad5dbe6ccc);
        

        circle_marker_d2abfba790df49b9bd6b02ce5330b8ec.bindPopup(popup_3485b75624ab47ebbae58d503def8d10)
        ;

        
    
    
            var circle_marker_a18bd8527ded46b1825e1ced4eb9cde9 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f67f90a2e20a4ed394df01958a8a5172 = L.popup({"maxWidth": "100%"});

        
            var html_0ce676b98c4a4cd2a39c89edf3faeff1 = $(`<div id="html_0ce676b98c4a4cd2a39c89edf3faeff1" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://arxiv.org/abs/2006.15253">Sound Event Detection Using Duration Robust Loss Function</a><br></div>`)[0];
            popup_f67f90a2e20a4ed394df01958a8a5172.setContent(html_0ce676b98c4a4cd2a39c89edf3faeff1);
        

        circle_marker_a18bd8527ded46b1825e1ced4eb9cde9.bindPopup(popup_f67f90a2e20a4ed394df01958a8a5172)
        ;

        
    
    
            var circle_marker_d3c9bcf66ff14dfd9e649af5c4cd11a8 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1a13bb65c07e44d6aad7aeca6b304454 = L.popup({"maxWidth": "100%"});

        
            var html_c68e47b8a67b4e7897f106f3e496a4e1 = $(`<div id="html_c68e47b8a67b4e7897f106f3e496a4e1" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8905033/">Speech Emotion Recognition with Hybrid Neural Network</a><br></div>`)[0];
            popup_1a13bb65c07e44d6aad7aeca6b304454.setContent(html_c68e47b8a67b4e7897f106f3e496a4e1);
        

        circle_marker_d3c9bcf66ff14dfd9e649af5c4cd11a8.bindPopup(popup_1a13bb65c07e44d6aad7aeca6b304454)
        ;

        
    
    
            var circle_marker_9609278dd11649df8ffd34df3b1a84a0 = L.circleMarker(
                [36.5748441, 139.2394179],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f372703a97164b6694db9ef23c63159a = L.popup({"maxWidth": "100%"});

        
            var html_e6b91283d879458188844eb3cc0d911b = $(`<div id="html_e6b91283d879458188844eb3cc0d911b" style="width: 100.0%; height: 100.0%;">Country : Japan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8501554/">Unsupervised detection of anomalous sound based on deep learning and the Neyman–Pearson lemma</a><br></div>`)[0];
            popup_f372703a97164b6694db9ef23c63159a.setContent(html_e6b91283d879458188844eb3cc0d911b);
        

        circle_marker_9609278dd11649df8ffd34df3b1a84a0.bindPopup(popup_f372703a97164b6694db9ef23c63159a)
        ;

        
    
    
            var circle_marker_ee6acc135cbe4e2286b1d396e1102dbc = L.circleMarker(
                [61.0666922, -107.9917071],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ff613a8cad5a402880d4a4b28b9cec08 = L.popup({"maxWidth": "100%"});

        
            var html_6df0d6bec2f043eb9bb62e6c6a525d0a = $(`<div id="html_6df0d6bec2f043eb9bb62e6c6a525d0a" style="width: 100.0%; height: 100.0%;">Country : Canada<br>                         Paper : <a href="https://arxiv.org/abs/1903.07714">A RAD approach to deep mixture models</a><br></div>`)[0];
            popup_ff613a8cad5a402880d4a4b28b9cec08.setContent(html_6df0d6bec2f043eb9bb62e6c6a525d0a);
        

        circle_marker_ee6acc135cbe4e2286b1d396e1102dbc.bindPopup(popup_ff613a8cad5a402880d4a4b28b9cec08)
        ;

        
    
    
            var circle_marker_d6784eeb8bcb42278eb8ce8e46c08fdb = L.circleMarker(
                [61.0666922, -107.9917071],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2625a498b9154eeaaf16243eb2333b2b = L.popup({"maxWidth": "100%"});

        
            var html_bd3b932f74774dbe911565fadfc5faf8 = $(`<div id="html_bd3b932f74774dbe911565fadfc5faf8" style="width: 100.0%; height: 100.0%;">Country : Canada<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7933046/">Combining temporal features by local binary pattern for acoustic scene classification</a><br></div>`)[0];
            popup_2625a498b9154eeaaf16243eb2333b2b.setContent(html_bd3b932f74774dbe911565fadfc5faf8);
        

        circle_marker_d6784eeb8bcb42278eb8ce8e46c08fdb.bindPopup(popup_2625a498b9154eeaaf16243eb2333b2b)
        ;

        
    
    
            var circle_marker_2ec92767b1c14c34a683dd3bc6c34b22 = L.circleMarker(
                [61.0666922, -107.9917071],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1035b4a53f124f0f99eb1025f7759bd3 = L.popup({"maxWidth": "100%"});

        
            var html_5332af55c1a14013a91cf06e7f084fff = $(`<div id="html_5332af55c1a14013a91cf06e7f084fff" style="width: 100.0%; height: 100.0%;">Country : Canada<br>                         Paper : <a href="https://arxiv.org/abs/1808.05777">Unsupervised adversarial domain adaptation for acoustic scene classification</a><br></div>`)[0];
            popup_1035b4a53f124f0f99eb1025f7759bd3.setContent(html_5332af55c1a14013a91cf06e7f084fff);
        

        circle_marker_2ec92767b1c14c34a683dd3bc6c34b22.bindPopup(popup_1035b4a53f124f0f99eb1025f7759bd3)
        ;

        
    
    
            var circle_marker_9b286e66ce374484926616a87b33d1c7 = L.circleMarker(
                [61.0666922, -107.9917071],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cddf566f18ac4d8e99347f15ff4e1b23 = L.popup({"maxWidth": "100%"});

        
            var html_674ba417eb9a4ec693aa53a808421b96 = $(`<div id="html_674ba417eb9a4ec693aa53a808421b96" style="width: 100.0%; height: 100.0%;">Country : Canada<br>                         Paper : <a href="https://arxiv.org/abs/2008.07702">VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations</a><br></div>`)[0];
            popup_cddf566f18ac4d8e99347f15ff4e1b23.setContent(html_674ba417eb9a4ec693aa53a808421b96);
        

        circle_marker_9b286e66ce374484926616a87b33d1c7.bindPopup(popup_cddf566f18ac4d8e99347f15ff4e1b23)
        ;

        
    
    
            var circle_marker_a3e5d6472ad84c05a1fae21ea07624bf = L.circleMarker(
                [45.9852129, 24.6859225],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7ad633397ca344ce9945ef0bbca5ec93 = L.popup({"maxWidth": "100%"});

        
            var html_b357d47cdf8041b7863a2a4af26c82fd = $(`<div id="html_b357d47cdf8041b7863a2a4af26c82fd" style="width: 100.0%; height: 100.0%;">Country : Romania<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240323.3240407">Audio-visual encoding of multimedia content for enhancing movie recommendations</a><br></div>`)[0];
            popup_7ad633397ca344ce9945ef0bbca5ec93.setContent(html_b357d47cdf8041b7863a2a4af26c82fd);
        

        circle_marker_a3e5d6472ad84c05a1fae21ea07624bf.bindPopup(popup_7ad633397ca344ce9945ef0bbca5ec93)
        ;

        
    
    
            var circle_marker_3b5de1f907334005a0914e1f83f29de5 = L.circleMarker(
                [45.9852129, 24.6859225],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b714ec5a5c9b4219a1ca60dd66250f5c = L.popup({"maxWidth": "100%"});

        
            var html_3a82df96c40e410595316875f3b9013e = $(`<div id="html_3a82df96c40e410595316875f3b9013e" style="width: 100.0%; height: 100.0%;">Country : Romania<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3204949.3208141">MMTF-14K: a multifaceted movie trailer feature dataset for recommendation and retrieval</a><br></div>`)[0];
            popup_b714ec5a5c9b4219a1ca60dd66250f5c.setContent(html_3a82df96c40e410595316875f3b9013e);
        

        circle_marker_3b5de1f907334005a0914e1f83f29de5.bindPopup(popup_b714ec5a5c9b4219a1ca60dd66250f5c)
        ;

        
    
    
            var circle_marker_90bfb902bc5249ae832e2414a4b5728f = L.circleMarker(
                [45.9852129, 24.6859225],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_bbb8af52101749ed8d8a13df638fd7a3 = L.popup({"maxWidth": "100%"});

        
            var html_421c166894d64d339fe54f00cc56c928 = $(`<div id="html_421c166894d64d339fe54f00cc56c928" style="width: 100.0%; height: 100.0%;">Country : Romania<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09221-y">Movie genome: alleviating new item cold start in movie recommendation</a><br></div>`)[0];
            popup_bbb8af52101749ed8d8a13df638fd7a3.setContent(html_421c166894d64d339fe54f00cc56c928);
        

        circle_marker_90bfb902bc5249ae832e2414a4b5728f.bindPopup(popup_bbb8af52101749ed8d8a13df638fd7a3)
        ;

        
    
    
            var circle_marker_4c8b0e37547847fdbd538619d6f8900c = L.circleMarker(
                [45.9852129, 24.6859225],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8fa3fc871436427c8bdf0753cf978f35 = L.popup({"maxWidth": "100%"});

        
            var html_69ea5e09c2d2408a8a6aa5afb17efc8f = $(`<div id="html_69ea5e09c2d2408a8a6aa5afb17efc8f" style="width: 100.0%; height: 100.0%;">Country : Romania<br>                         Paper : <a href="https://www.jku.at/fileadmin/gruppen/173/Research/deldjoo_mediaeval_2018.pdf">The MediaEval 2018 Movie Recommendation Task: Recommending Movies Using Content.</a><br></div>`)[0];
            popup_8fa3fc871436427c8bdf0753cf978f35.setContent(html_69ea5e09c2d2408a8a6aa5afb17efc8f);
        

        circle_marker_4c8b0e37547847fdbd538619d6f8900c.bindPopup(popup_8fa3fc871436427c8bdf0753cf978f35)
        ;

        
    
    
            var circle_marker_7046f83189a64f818b54b17ec3352bd6 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ea54346989fd4566ab0b3d51238ca83b = L.popup({"maxWidth": "100%"});

        
            var html_4c2b89d7b52f4df9bf8689d6bf65eca2 = $(`<div id="html_4c2b89d7b52f4df9bf8689d6bf65eca2" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9140774/">A Re-trained Model Based On Multi-kernel Convolutional Neural Network for Acoustic Scene Classification</a><br></div>`)[0];
            popup_ea54346989fd4566ab0b3d51238ca83b.setContent(html_4c2b89d7b52f4df9bf8689d6bf65eca2);
        

        circle_marker_7046f83189a64f818b54b17ec3352bd6.bindPopup(popup_ea54346989fd4566ab0b3d51238ca83b)
        ;

        
    
    
            var circle_marker_cab74e2b06ad40e99829ad422374c0de = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0d877b8538884ca7ab06c03add120b35 = L.popup({"maxWidth": "100%"});

        
            var html_ac25f2df100746e083d71bff5cf598b8 = $(`<div id="html_ac25f2df100746e083d71bff5cf598b8" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8999242/">Acoustic Scene Classification Using Deep Mixtures of Pre-trained Convolutional Neural Networks</a><br></div>`)[0];
            popup_0d877b8538884ca7ab06c03add120b35.setContent(html_ac25f2df100746e083d71bff5cf598b8);
        

        circle_marker_cab74e2b06ad40e99829ad422374c0de.bindPopup(popup_0d877b8538884ca7ab06c03add120b35)
        ;

        
    
    
            var circle_marker_17eec675330b4f598c4c4324e4b523aa = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_924eb46989584e0f98e4bbf57a122835 = L.popup({"maxWidth": "100%"});

        
            var html_31321e0140a24f96a0de62e1d3337570 = $(`<div id="html_31321e0140a24f96a0de62e1d3337570" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053582/">Acoustic Scene Classification for Mismatched Recording Devices Using Heated-Up Softmax and Spectrum Correction</a><br></div>`)[0];
            popup_924eb46989584e0f98e4bbf57a122835.setContent(html_31321e0140a24f96a0de62e1d3337570);
        

        circle_marker_17eec675330b4f598c4c4324e4b523aa.bindPopup(popup_924eb46989584e0f98e4bbf57a122835)
        ;

        
    
    
            var circle_marker_be5fadcbd70b49469a47c5249fdd01a1 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_eb31532d41ac423888e2b88ade6849b6 = L.popup({"maxWidth": "100%"});

        
            var html_7d9fab90362a4132a344e05ef61ff61d = $(`<div id="html_7d9fab90362a4132a344e05ef61ff61d" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/3002.pdf">Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation.</a><br></div>`)[0];
            popup_eb31532d41ac423888e2b88ade6849b6.setContent(html_7d9fab90362a4132a344e05ef61ff61d);
        

        circle_marker_be5fadcbd70b49469a47c5249fdd01a1.bindPopup(popup_eb31532d41ac423888e2b88ade6849b6)
        ;

        
    
    
            var circle_marker_793f11f8e28849d3be7f0bb34c046b61 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8ac024e29bf9431ba840b32b7c6a7d7e = L.popup({"maxWidth": "100%"});

        
            var html_cf9e90ed01a74355afd4c2403d03493e = $(`<div id="html_cf9e90ed01a74355afd4c2403d03493e" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://trepo.tuni.fi/bitstream/handle/10024/116599/DCASE_2018_proceedings.pdf?sequence=1&isAllowed=y#page=35">Acoustic scene classification using a convolutional neural network ensemble and nearest neighbor filters</a><br></div>`)[0];
            popup_8ac024e29bf9431ba840b32b7c6a7d7e.setContent(html_cf9e90ed01a74355afd4c2403d03493e);
        

        circle_marker_793f11f8e28849d3be7f0bb34c046b61.bindPopup(popup_8ac024e29bf9431ba840b32b7c6a7d7e)
        ;

        
    
    
            var circle_marker_f743e8d0dd6e4913918afd6991e36219 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0e9cf40999fe4f099db9035ce9aae286 = L.popup({"maxWidth": "100%"});

        
            var html_54804973bfac4857a6de712c251a650a = $(`<div id="html_54804973bfac4857a6de712c251a650a" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8784816/">Acoustic scene classification with mismatched recording devices using mixture of experts layer</a><br></div>`)[0];
            popup_0e9cf40999fe4f099db9035ce9aae286.setContent(html_54804973bfac4857a6de712c251a650a);
        

        circle_marker_f743e8d0dd6e4913918afd6991e36219.bindPopup(popup_0e9cf40999fe4f099db9035ce9aae286)
        ;

        
    
    
            var circle_marker_3df2866bc63d4b6cbe7ac07afb3ab424 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_87f7815568894a56a372f988c294cc76 = L.popup({"maxWidth": "100%"});

        
            var html_89292ec5f690451094f4fff3a79c18a9 = $(`<div id="html_89292ec5f690451094f4fff3a79c18a9" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_HCM_6.pdf">CDNN-CRNN JOINED MODEL FOR ACOUSTIC SCENE CLASSIFICATION</a><br></div>`)[0];
            popup_87f7815568894a56a372f988c294cc76.setContent(html_89292ec5f690451094f4fff3a79c18a9);
        

        circle_marker_3df2866bc63d4b6cbe7ac07afb3ab424.bindPopup(popup_87f7815568894a56a372f988c294cc76)
        ;

        
    
    
            var circle_marker_8c91ba606cbc4bff9c12b8ae3cdeb520 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7bc56c88ef924b45b6093c438b44f365 = L.popup({"maxWidth": "100%"});

        
            var html_f95c51ef878d4a8e9e51913828add350 = $(`<div id="html_f95c51ef878d4a8e9e51913828add350" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://www.researchgate.net/profile/Nghia_Duong_Trung/publication/339593299_Genres_and_ActorsActresses_as_Interpolated_Tags_for_Improving_Movie_Recommender_Systems/links/5e68a2ad299bf1744f72db23/Genres-and-Actors-Actresses-as-Interpolated-Tags-for-Improving-Movie-Recommender-Systems.pdf">Genres and Actors/Actresses as Interpolated Tags for Improving Movie Recommender Systems</a><br></div>`)[0];
            popup_7bc56c88ef924b45b6093c438b44f365.setContent(html_f95c51ef878d4a8e9e51913828add350);
        

        circle_marker_8c91ba606cbc4bff9c12b8ae3cdeb520.bindPopup(popup_7bc56c88ef924b45b6093c438b44f365)
        ;

        
    
    
            var circle_marker_fb67f83f13934e9daae3da05fc3c6a3a = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e702adf9ce6b4e82971705812d7397e0 = L.popup({"maxWidth": "100%"});

        
            var html_0ff30414e2b74492b6c87835d5b9645c = $(`<div id="html_0ff30414e2b74492b6c87835d5b9645c" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://www.researchgate.net/profile/Nghia_Duong_Trung/publication/339778055_Movie_Recommender_Systems_Made_Through_Tag_Interpolation/links/5e678f4492851c7ce0579a68/Movie-Recommender-Systems-Made-Through-Tag-Interpolation.pdf">Movie Recommender Systems Made Through Tag Interpolation</a><br></div>`)[0];
            popup_e702adf9ce6b4e82971705812d7397e0.setContent(html_0ff30414e2b74492b6c87835d5b9645c);
        

        circle_marker_fb67f83f13934e9daae3da05fc3c6a3a.bindPopup(popup_e702adf9ce6b4e82971705812d7397e0)
        ;

        
    
    
            var circle_marker_bf6a4c1790bc45f68304d6f9ba081498 = L.circleMarker(
                [13.2904027, 108.4265113],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_81d2bb053d504715b40fcd64e6852767 = L.popup({"maxWidth": "100%"});

        
            var html_4eb304ee0d4649ddb582fab97394bcde = $(`<div id="html_4eb304ee0d4649ddb582fab97394bcde" style="width: 100.0%; height: 100.0%;">Country : Vietnam<br>                         Paper : <a href="https://arxiv.org/abs/2002.04502">Robust Acoustic Scene Classification using a Multi-Spectrogram Encoder-Decoder Framework</a><br></div>`)[0];
            popup_81d2bb053d504715b40fcd64e6852767.setContent(html_4eb304ee0d4649ddb582fab97394bcde);
        

        circle_marker_bf6a4c1790bc45f68304d6f9ba081498.bindPopup(popup_81d2bb053d504715b40fcd64e6852767)
        ;

        
    
    
            var circle_marker_8df3c5a916ae410aa6e40ece06806814 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3e48e1ae61a2447b8c52889d2ee331f2 = L.popup({"maxWidth": "100%"});

        
            var html_30d6867ef5de442e831820454d6cc134 = $(`<div id="html_30d6867ef5de442e831820454d6cc134" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9049622/">A Hybrid Approach for Mobile Phone Clustering with Speech Recordings</a><br></div>`)[0];
            popup_3e48e1ae61a2447b8c52889d2ee331f2.setContent(html_30d6867ef5de442e831820454d6cc134);
        

        circle_marker_8df3c5a916ae410aa6e40ece06806814.bindPopup(popup_3e48e1ae61a2447b8c52889d2ee331f2)
        ;

        
    
    
            var circle_marker_90a0532f912345ea84431ae1a2745375 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6f63ecddd73e46f2ae4df5e133793d32 = L.popup({"maxWidth": "100%"});

        
            var html_562de17abe8444ed92f5f840400d396e = $(`<div id="html_562de17abe8444ed92f5f840400d396e" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://www.researchgate.net/profile/Lam_Pham4/publication/335829391_A_Robust_Framework_for_Acoustic_Scene_Classification/links/5dc03975a6fdcc2128011ee7/A-Robust-Framework-for-Acoustic-Scene-Classification.pdf">A Robust Framework for Acoustic Scene Classification.</a><br></div>`)[0];
            popup_6f63ecddd73e46f2ae4df5e133793d32.setContent(html_562de17abe8444ed92f5f840400d396e);
        

        circle_marker_90a0532f912345ea84431ae1a2745375.bindPopup(popup_6f63ecddd73e46f2ae4df5e133793d32)
        ;

        
    
    
            var circle_marker_f92b840b90ff47558629370682248e2b = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fe1f1ed3735f4e77b06a9558deeeb065 = L.popup({"maxWidth": "100%"});

        
            var html_3545ebf42ae141f088525d42a6adf2d3 = $(`<div id="html_3545ebf42ae141f088525d42a6adf2d3" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://core.ac.uk/download/pdf/323227034.pdf#page=156">ACOUSTIC SCENE CLASSIFICATION FROM BINAURAL SIGNALS USING CONVOLUTIONAL NEURAL NETWORKS</a><br></div>`)[0];
            popup_fe1f1ed3735f4e77b06a9558deeeb065.setContent(html_3545ebf42ae141f088525d42a6adf2d3);
        

        circle_marker_f92b840b90ff47558629370682248e2b.bindPopup(popup_fe1f1ed3735f4e77b06a9558deeeb065)
        ;

        
    
    
            var circle_marker_062638b8600a4bc4a0a973a4a0ca7df4 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_43a044f01ec44ce09ce9a455bb7dd822 = L.popup({"maxWidth": "100%"});

        
            var html_90836cc68b674cdb8f1f1a6a09b7a829 = $(`<div id="html_90836cc68b674cdb8f1f1a6a09b7a829" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Zhao_161.pdf">ADSC submission for DCASE 2017: Acoustic scene classification using deep residual convolutional neural networks</a><br></div>`)[0];
            popup_43a044f01ec44ce09ce9a455bb7dd822.setContent(html_90836cc68b674cdb8f1f1a6a09b7a829);
        

        circle_marker_062638b8600a4bc4a0a973a4a0ca7df4.bindPopup(popup_43a044f01ec44ce09ce9a455bb7dd822)
        ;

        
    
    
            var circle_marker_bcfb6231ae4c4d2da1e0b5843625d886 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a956b1c37030459e95d73a9466a68905 = L.popup({"maxWidth": "100%"});

        
            var html_4555f8fda8ce4f11a6127ff9d17e0dd5 = $(`<div id="html_4555f8fda8ce4f11a6127ff9d17e0dd5" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60748">Acoustic Scene Classification from Binaural Signals using Convolutional Neural Networks</a><br></div>`)[0];
            popup_a956b1c37030459e95d73a9466a68905.setContent(html_4555f8fda8ce4f11a6127ff9d17e0dd5);
        

        circle_marker_bcfb6231ae4c4d2da1e0b5843625d886.bindPopup(popup_a956b1c37030459e95d73a9466a68905)
        ;

        
    
    
            var circle_marker_6dbb7851fb53489d99530ac424177eb3 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6176c3c453394049a40ab50e0da62b40 = L.popup({"maxWidth": "100%"});

        
            var html_3d1623a3907c4bc38d7a3581629443db = $(`<div id="html_3d1623a3907c4bc38d7a3581629443db" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240631">Learning and fusing multimodal deep features for acoustic scene categorization</a><br></div>`)[0];
            popup_6176c3c453394049a40ab50e0da62b40.setContent(html_3d1623a3907c4bc38d7a3581629443db);
        

        circle_marker_6dbb7851fb53489d99530ac424177eb3.bindPopup(popup_6176c3c453394049a40ab50e0da62b40)
        ;

        
    
    
            var circle_marker_6333a90b23f341839f66c871936b18f0 = L.circleMarker(
                [1.3408630000000001, 103.83039182212079],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_dd62ba5a1a214872849df888dd955e59 = L.popup({"maxWidth": "100%"});

        
            var html_3c8c267da3e247fe9ed176815c597cd1 = $(`<div id="html_3c8c267da3e247fe9ed176815c597cd1" style="width: 100.0%; height: 100.0%;">Country : Singapore<br>                         Paper : <a href="https://pdfs.semanticscholar.org/5e34/9da3165367f4fb2e8ad3c07f2ed95411457a.pdf">On the Importance of Audio-Source Separation for Singer Identification in Polyphonic Music.</a><br></div>`)[0];
            popup_dd62ba5a1a214872849df888dd955e59.setContent(html_3c8c267da3e247fe9ed176815c597cd1);
        

        circle_marker_6333a90b23f341839f66c871936b18f0.bindPopup(popup_dd62ba5a1a214872849df888dd955e59)
        ;

        
    
    
            var circle_marker_d8c2573be52b459f8db5ef9038ff67ed = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5c34ff60e53146289c483ce24a3fd318 = L.popup({"maxWidth": "100%"});

        
            var html_d481889e620c427192fddeef3fa28f31 = $(`<div id="html_d481889e620c427192fddeef3fa28f31" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553052/">A layer-wise score level ensemble framework for acoustic scene classification</a><br></div>`)[0];
            popup_5c34ff60e53146289c483ce24a3fd318.setContent(html_d481889e620c427192fddeef3fa28f31);
        

        circle_marker_d8c2573be52b459f8db5ef9038ff67ed.bindPopup(popup_5c34ff60e53146289c483ce24a3fd318)
        ;

        
    
    
            var circle_marker_1c99c8d07fe941bdb2fa5a880917f664 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_fbed9b8bb462449c899de97ed3ba8427 = L.popup({"maxWidth": "100%"});

        
            var html_3d4247e37feb4bd2b5dc9ef0e1f45056 = $(`<div id="html_3d4247e37feb4bd2b5dc9ef0e1f45056" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://arxiv.org/abs/1811.00936">Acoustic features fusion using attentive multi-channel deep architecture</a><br></div>`)[0];
            popup_fbed9b8bb462449c899de97ed3ba8427.setContent(html_3d4247e37feb4bd2b5dc9ef0e1f45056);
        

        circle_marker_1c99c8d07fe941bdb2fa5a880917f664.bindPopup(popup_fbed9b8bb462449c899de97ed3ba8427)
        ;

        
    
    
            var circle_marker_c3c90202cc51434b9289f9a033946595 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_537414bb55984acaa948c1dff1fa6426 = L.popup({"maxWidth": "100%"});

        
            var html_3671a4b0858c4d46a7682dc74275bd15 = $(`<div id="html_3671a4b0858c4d46a7682dc74275bd15" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="">An Empirical Study of Iterative Knowledge Distillation for Neural Network Compression</a><br></div>`)[0];
            popup_537414bb55984acaa948c1dff1fa6426.setContent(html_3671a4b0858c4d46a7682dc74275bd15);
        

        circle_marker_c3c90202cc51434b9289f9a033946595.bindPopup(popup_537414bb55984acaa948c1dff1fa6426)
        ;

        
    
    
            var circle_marker_9989e4c11df44d008816cd38b6d87722 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7e36aab26f3440679605d30a95ddaec7 = L.popup({"maxWidth": "100%"});

        
            var html_3a4390ab1ec24afd8ef78756821dffb8 = $(`<div id="html_3a4390ab1ec24afd8ef78756821dffb8" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s11042-019-08279-5.pdf">Analysis and classification of acoustic scenes with wavelet transform-based mel-scaled features</a><br></div>`)[0];
            popup_7e36aab26f3440679605d30a95ddaec7.setContent(html_3a4390ab1ec24afd8ef78756821dffb8);
        

        circle_marker_9989e4c11df44d008816cd38b6d87722.bindPopup(popup_7e36aab26f3440679605d30a95ddaec7)
        ;

        
    
    
            var circle_marker_12add67b7cbb4f5396142924a2b25c2d = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9ae83288e6914972b8adadd47606acf0 = L.popup({"maxWidth": "100%"});

        
            var html_282830d9dfc44780894a678c46fe6742 = $(`<div id="html_282830d9dfc44780894a678c46fe6742" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S1051200418300046">Classification of audio scenes with novel features in a fused system framework</a><br></div>`)[0];
            popup_9ae83288e6914972b8adadd47606acf0.setContent(html_282830d9dfc44780894a678c46fe6742);
        

        circle_marker_12add67b7cbb4f5396142924a2b25c2d.bindPopup(popup_9ae83288e6914972b8adadd47606acf0)
        ;

        
    
    
            var circle_marker_6d524d244bb04c9e95d61e561d305453 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_31b579d1ce6b4ffc8e0ccc3ef38dc330 = L.popup({"maxWidth": "100%"});

        
            var html_0f7c59a1ea364b17b06f2c96432dd208 = $(`<div id="html_0f7c59a1ea364b17b06f2c96432dd208" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S095741741830229X">Classification of vocal and non-vocal segments in audio clips using genetic algorithm based feature selection (GAFS)</a><br></div>`)[0];
            popup_31b579d1ce6b4ffc8e0ccc3ef38dc330.setContent(html_0f7c59a1ea364b17b06f2c96432dd208);
        

        circle_marker_6d524d244bb04c9e95d61e561d305453.bindPopup(popup_31b579d1ce6b4ffc8e0ccc3ef38dc330)
        ;

        
    
    
            var circle_marker_6e35bb0881e542339ed17e67bf24bafb = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e0e301ecc7cd43dd8d3c4cae632ed326 = L.popup({"maxWidth": "100%"});

        
            var html_98a5f349a1ab43d6a8457f11ea7d78ed = $(`<div id="html_98a5f349a1ab43d6a8457f11ea7d78ed" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s12065-019-00283-w.pdf">Data augmentation for cancer classification in oncogenomics: an improved KNN based approach</a><br></div>`)[0];
            popup_e0e301ecc7cd43dd8d3c4cae632ed326.setContent(html_98a5f349a1ab43d6a8457f11ea7d78ed);
        

        circle_marker_6e35bb0881e542339ed17e67bf24bafb.bindPopup(popup_e0e301ecc7cd43dd8d3c4cae632ed326)
        ;

        
    
    
            var circle_marker_b3e84fc4fb8b4e8aa890f949570a73ee = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5bddf8e7f2fc4a65b1d2615fae89c22f = L.popup({"maxWidth": "100%"});

        
            var html_d2b3cf17ef7f453ba665f1aebe18609d = $(`<div id="html_d2b3cf17ef7f453ba665f1aebe18609d" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s00500-019-04602-2.pdf">Data augmentation using MG-GAN for improved cancer classification on gene expression data</a><br></div>`)[0];
            popup_5bddf8e7f2fc4a65b1d2615fae89c22f.setContent(html_d2b3cf17ef7f453ba665f1aebe18609d);
        

        circle_marker_b3e84fc4fb8b4e8aa890f949570a73ee.bindPopup(popup_5bddf8e7f2fc4a65b1d2615fae89c22f)
        ;

        
    
    
            var circle_marker_ae88cdbf1b54451f819627271f8dad35 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2fa95d6002374388b075f53ada01da4f = L.popup({"maxWidth": "100%"});

        
            var html_e0db5965ab3c4ef2882917f8912145da = $(`<div id="html_e0db5965ab3c4ef2882917f8912145da" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://archivefda.dlib.nyu.edu/handle/2451/60765">Deep Multi-view Features from Raw Audio for Acoustic Scene Classification</a><br></div>`)[0];
            popup_2fa95d6002374388b075f53ada01da4f.setContent(html_e0db5965ab3c4ef2882917f8912145da);
        

        circle_marker_ae88cdbf1b54451f819627271f8dad35.bindPopup(popup_2fa95d6002374388b075f53ada01da4f)
        ;

        
    
    
            var circle_marker_4c7be8ddd36b4223b024246eca4c2e8e = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_2cf87c81f95e44b48576123943b31935 = L.popup({"maxWidth": "100%"});

        
            var html_1fcfb2910cdd468280ee593f6c711dc4 = $(`<div id="html_1fcfb2910cdd468280ee593f6c711dc4" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://arxiv.org/abs/1708.05826">Ensemble Of Deep Neural Networks For Acoustic Scene Classification</a><br></div>`)[0];
            popup_2cf87c81f95e44b48576123943b31935.setContent(html_1fcfb2910cdd468280ee593f6c711dc4);
        

        circle_marker_4c7be8ddd36b4223b024246eca4c2e8e.bindPopup(popup_2cf87c81f95e44b48576123943b31935)
        ;

        
    
    
            var circle_marker_b20d39783b444fa1a320f9b3f263959d = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_23e6eb415c534d6e9e7d51b23f1abf6e = L.popup({"maxWidth": "100%"});

        
            var html_be9725de1ca44ce79db514dc96bbe4be = $(`<div id="html_be9725de1ca44ce79db514dc96bbe4be" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3322240">Environmental audio scene and sound event recognition for autonomous surveillance: A survey and comparative studies</a><br></div>`)[0];
            popup_23e6eb415c534d6e9e7d51b23f1abf6e.setContent(html_be9725de1ca44ce79db514dc96bbe4be);
        

        circle_marker_b20d39783b444fa1a320f9b3f263959d.bindPopup(popup_23e6eb415c534d6e9e7d51b23f1abf6e)
        ;

        
    
    
            var circle_marker_4c4b478a5c2e43b5bba89128fc3daf7b = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1ffe410b604048b9b695312a35f21e8b = L.popup({"maxWidth": "100%"});

        
            var html_91e3d017414846879c27fce88cc38d03 = $(`<div id="html_91e3d017414846879c27fce88cc38d03" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8752027/">Generative Model Driven Representation Learning in a Hybrid Framework for Environmental Audio Scene and Sound Event Recognition</a><br></div>`)[0];
            popup_1ffe410b604048b9b695312a35f21e8b.setContent(html_91e3d017414846879c27fce88cc38d03);
        

        circle_marker_4c4b478a5c2e43b5bba89128fc3daf7b.bindPopup(popup_1ffe410b604048b9b695312a35f21e8b)
        ;

        
    
    
            var circle_marker_bcc2207382d24898abfb4d86d12b4797 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_afaeed68efa84b83bf87e449b9eea334 = L.popup({"maxWidth": "100%"});

        
            var html_784ae70365ab4fb5b8751ccb65ea7b05 = $(`<div id="html_784ae70365ab4fb5b8751ccb65ea7b05" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8721416/">Input Fusion of MFCC and SCMC Features for Acoustic Scene Classification using DNN</a><br></div>`)[0];
            popup_afaeed68efa84b83bf87e449b9eea334.setContent(html_784ae70365ab4fb5b8751ccb65ea7b05);
        

        circle_marker_bcc2207382d24898abfb4d86d12b4797.bindPopup(popup_afaeed68efa84b83bf87e449b9eea334)
        ;

        
    
    
            var circle_marker_2f99d6027dbc4dafa0a4a69afe6f5214 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_10f25d9277e34205a558081ce88b1bcb = L.popup({"maxWidth": "100%"});

        
            var html_323705d708294be3a8558d45ef598028 = $(`<div id="html_323705d708294be3a8558d45ef598028" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240631">Learning and fusing multimodal deep features for acoustic scene categorization</a><br></div>`)[0];
            popup_10f25d9277e34205a558081ce88b1bcb.setContent(html_323705d708294be3a8558d45ef598028);
        

        circle_marker_2f99d6027dbc4dafa0a4a69afe6f5214.bindPopup(popup_10f25d9277e34205a558081ce88b1bcb)
        ;

        
    
    
            var circle_marker_10e9f55eac624a02aa5afa9b62d6be99 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_215b6c3305be4aa5b9442eff2dd56416 = L.popup({"maxWidth": "100%"});

        
            var html_37f72a509e3f4081b1dbd9f3858dc11e = $(`<div id="html_37f72a509e3f4081b1dbd9f3858dc11e" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s10772-020-09681-3.pdf">Pattern analysis based acoustic signal processing: a survey of the state-of-art</a><br></div>`)[0];
            popup_215b6c3305be4aa5b9442eff2dd56416.setContent(html_37f72a509e3f4081b1dbd9f3858dc11e);
        

        circle_marker_10e9f55eac624a02aa5afa9b62d6be99.bindPopup(popup_215b6c3305be4aa5b9442eff2dd56416)
        ;

        
    
    
            var circle_marker_565427fee8bd4b7480061e9632b6d77f = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1b0d29f60a6f487e88bbe9ba365d85e5 = L.popup({"maxWidth": "100%"});

        
            var html_13a4102b1b474d2dbaedc2a62ca02332 = $(`<div id="html_13a4102b1b474d2dbaedc2a62ca02332" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://www.researchgate.net/profile/Himanshu_Agrawal8/publication/336025224_Data_augmentation_for_cancer_classification_in_oncogenomics_an_improved_KNN_based_approach/links/5d92c634458515202b7778ff/Data-augmentation-for-cancer-classification-in-oncogenomics-an-improved-KNN-based-approach.pdf">Poonam Chaudhari, Himanshu Agarwal</a><br></div>`)[0];
            popup_1b0d29f60a6f487e88bbe9ba365d85e5.setContent(html_13a4102b1b474d2dbaedc2a62ca02332);
        

        circle_marker_565427fee8bd4b7480061e9632b6d77f.bindPopup(popup_1b0d29f60a6f487e88bbe9ba365d85e5)
        ;

        
    
    
            var circle_marker_b7004ceac77744de8b68a6f475353473 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f756dc26351d435990ae335d0a8e4d5b = L.popup({"maxWidth": "100%"});

        
            var html_5e02a79ca804482280a19c8b7162001f = $(`<div id="html_5e02a79ca804482280a19c8b7162001f" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://www.aclweb.org/anthology/2020.ecnlp-1.7/">Semi-Supervised Iterative Approach for Domain-Specific Complaint Detection in Social Media</a><br></div>`)[0];
            popup_f756dc26351d435990ae335d0a8e4d5b.setContent(html_5e02a79ca804482280a19c8b7162001f);
        

        circle_marker_b7004ceac77744de8b68a6f475353473.bindPopup(popup_f756dc26351d435990ae335d0a8e4d5b)
        ;

        
    
    
            var circle_marker_dc398c387ba64cf896cbc6878d9a6053 = L.circleMarker(
                [22.3511148, 78.6677428],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ad3c58e7997245de9ebfa07e09ceaf75 = L.popup({"maxWidth": "100%"});

        
            var html_da824fe9305d44d896e02b63aae917c9 = $(`<div id="html_da824fe9305d44d896e02b63aae917c9" style="width: 100.0%; height: 100.0%;">Country : India<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8593164/">Vocal and Non-vocal Segmentation based on the Analysis of Formant Structure</a><br></div>`)[0];
            popup_ad3c58e7997245de9ebfa07e09ceaf75.setContent(html_da824fe9305d44d896e02b63aae917c9);
        

        circle_marker_dc398c387ba64cf896cbc6878d9a6053.bindPopup(popup_ad3c58e7997245de9ebfa07e09ceaf75)
        ;

        
    
    
            var circle_marker_236c6903fc5b40959bf6891a7067cab6 = L.circleMarker(
                [23.59829785, 120.83536313817521],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0172e84aa6dd4ee4aa97f7f9e2d714b0 = L.popup({"maxWidth": "100%"});

        
            var html_5d7adc3fb545416cb929941b58bc01ab = $(`<div id="html_5d7adc3fb545416cb929941b58bc01ab" style="width: 100.0%; height: 100.0%;">Country : Taiwan<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8326315/">Acoustic scene classification using convolutional neural networks and multi-scale multi-feature extraction</a><br></div>`)[0];
            popup_0172e84aa6dd4ee4aa97f7f9e2d714b0.setContent(html_5d7adc3fb545416cb929941b58bc01ab);
        

        circle_marker_236c6903fc5b40959bf6891a7067cab6.bindPopup(popup_0172e84aa6dd4ee4aa97f7f9e2d714b0)
        ;

        
    
    
            var circle_marker_a92414dc9db54449a3e6903f3ff0d2cb = L.circleMarker(
                [23.59829785, 120.83536313817521],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_eb8a1163bebf4b07a24d5c82b176ec55 = L.popup({"maxWidth": "100%"});

        
            var html_4f8e2f9feb2a4ccfa28638547ee341d3 = $(`<div id="html_4f8e2f9feb2a4ccfa28638547ee341d3" style="width: 100.0%; height: 100.0%;">Country : Taiwan<br>                         Paper : <a href="http://dcase.community/documents/challenge2017/technical_reports/DCASE2017_Dang_209.pdf">Deep learning for DCASE2017 challenge</a><br></div>`)[0];
            popup_eb8a1163bebf4b07a24d5c82b176ec55.setContent(html_4f8e2f9feb2a4ccfa28638547ee341d3);
        

        circle_marker_a92414dc9db54449a3e6903f3ff0d2cb.bindPopup(popup_eb8a1163bebf4b07a24d5c82b176ec55)
        ;

        
    
    
            var circle_marker_10bf18a2414f4ad8bade8def2a0d340b = L.circleMarker(
                [23.59829785, 120.83536313817521],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_611f3818d8624ebfa790a7ad54dc91c7 = L.popup({"maxWidth": "100%"});

        
            var html_a5b5c030b54949bb8068fe1b30ced48f = $(`<div id="html_a5b5c030b54949bb8068fe1b30ced48f" style="width: 100.0%; height: 100.0%;">Country : Taiwan<br>                         Paper : <a href="http://archives.ismir.net/ismir2019/paper/000104.pdf">Generating Structured Drum Pattern Using Variational Autoencoder and Self-similarity Matrix.</a><br></div>`)[0];
            popup_611f3818d8624ebfa790a7ad54dc91c7.setContent(html_a5b5c030b54949bb8068fe1b30ced48f);
        

        circle_marker_10bf18a2414f4ad8bade8def2a0d340b.bindPopup(popup_611f3818d8624ebfa790a7ad54dc91c7)
        ;

        
    
    
            var circle_marker_351666d29a06461589d2cecb595a8da1 = L.circleMarker(
                [23.59829785, 120.83536313817521],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_03c8d9ce379e4ffab23cb6cab3604edd = L.popup({"maxWidth": "100%"});

        
            var html_2485ee9259444cc5b88323db8cf86e92 = $(`<div id="html_2485ee9259444cc5b88323db8cf86e92" style="width: 100.0%; height: 100.0%;">Country : Taiwan<br>                         Paper : <a href="https://www.ijcai.org/Proceedings/2018/0463.pdf">Learning to Recognize Transient Sound Events using Attentional Supervision.</a><br></div>`)[0];
            popup_03c8d9ce379e4ffab23cb6cab3604edd.setContent(html_2485ee9259444cc5b88323db8cf86e92);
        

        circle_marker_351666d29a06461589d2cecb595a8da1.bindPopup(popup_03c8d9ce379e4ffab23cb6cab3604edd)
        ;

        
    
    
            var circle_marker_161317abfd0849f4878964cbd6fde5fd = L.circleMarker(
                [52.215933, 19.134422],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6c831472da4e4bd6aeedb92c70c9f80f = L.popup({"maxWidth": "100%"});

        
            var html_235dbee45d70420b8dce0af5fa9f0609 = $(`<div id="html_235dbee45d70420b8dce0af5fa9f0609" style="width: 100.0%; height: 100.0%;">Country : Poland<br>                         Paper : <a href="https://www.karolpiczak.com/papers/Piczak2017-DCASE.pdf">The details that matter: Frequency resolution of spectrograms in acoustic scene classification</a><br></div>`)[0];
            popup_6c831472da4e4bd6aeedb92c70c9f80f.setContent(html_235dbee45d70420b8dce0af5fa9f0609);
        

        circle_marker_161317abfd0849f4878964cbd6fde5fd.bindPopup(popup_6c831472da4e4bd6aeedb92c70c9f80f)
        ;

        
    
    
            var circle_marker_8017aef1f3734f449213ed6645f27c1e = L.circleMarker(
                [22.2793278, 114.1628131],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7db0956a9fa54570a2c2e6960905d8b5 = L.popup({"maxWidth": "100%"});

        
            var html_bcbdd19764e6431f8d6e147956f818b7 = $(`<div id="html_bcbdd19764e6431f8d6e147956f818b7" style="width: 100.0%; height: 100.0%;">Country : Hong Kong<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8683490/">Enhancing sound texture in CNN-based acoustic scene classification</a><br></div>`)[0];
            popup_7db0956a9fa54570a2c2e6960905d8b5.setContent(html_bcbdd19764e6431f8d6e147956f818b7);
        

        circle_marker_8017aef1f3734f449213ed6645f27c1e.bindPopup(popup_7db0956a9fa54570a2c2e6960905d8b5)
        ;

        
    
    
            var circle_marker_96e21ee67854493ab6e1181825066cdb = L.circleMarker(
                [22.2793278, 114.1628131],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_185e31ca46bd4e44b919ff7ca76224ab = L.popup({"maxWidth": "100%"});

        
            var html_93bc9a02e3724a90a92c2113250bb2e5 = $(`<div id="html_93bc9a02e3724a90a92c2113250bb2e5" style="width: 100.0%; height: 100.0%;">Country : Hong Kong<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8462168/">Reducing model complexity for DNN based large-scale audio classification</a><br></div>`)[0];
            popup_185e31ca46bd4e44b919ff7ca76224ab.setContent(html_93bc9a02e3724a90a92c2113250bb2e5);
        

        circle_marker_96e21ee67854493ab6e1181825066cdb.bindPopup(popup_185e31ca46bd4e44b919ff7ca76224ab)
        ;

        
    
    
            var circle_marker_8e472c94517c445f80dc6917ada9755e = L.circleMarker(
                [38.9953683, 21.9877132],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e2086918146d4283b2893883d041dfb1 = L.popup({"maxWidth": "100%"});

        
            var html_49c0257db2f245c7ae8c795d5cc2d714 = $(`<div id="html_49c0257db2f245c7ae8c795d5cc2d714" style="width: 100.0%; height: 100.0%;">Country : Greece<br>                         Paper : <a href="https://dora.dmu.ac.uk/handle/2086/15000">Acoustic scene classification: From a hybrid classifier to deep learning</a><br></div>`)[0];
            popup_e2086918146d4283b2893883d041dfb1.setContent(html_49c0257db2f245c7ae8c795d5cc2d714);
        

        circle_marker_8e472c94517c445f80dc6917ada9755e.bindPopup(popup_e2086918146d4283b2893883d041dfb1)
        ;

        
    
    
            var circle_marker_27d086654ba64e398d48f4dd184950f9 = L.circleMarker(
                [38.9953683, 21.9877132],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_a9e00cadc03a474593bb8290df4267e0 = L.popup({"maxWidth": "100%"});

        
            var html_d12a8dd6e752473a87bb87f771c6e1d1 = $(`<div id="html_d12a8dd6e752473a87bb87f771c6e1d1" style="width: 100.0%; height: 100.0%;">Country : Greece<br>                         Paper : <a href="https://www.sciencedirect.com/science/article/pii/S0952197619302052">Audio content analysis for unobtrusive event detection in smart homes</a><br></div>`)[0];
            popup_a9e00cadc03a474593bb8290df4267e0.setContent(html_d12a8dd6e752473a87bb87f771c6e1d1);
        

        circle_marker_27d086654ba64e398d48f4dd184950f9.bindPopup(popup_a9e00cadc03a474593bb8290df4267e0)
        ;

        
    
    
            var circle_marker_c68fbeae093d4590b21342744b259442 = L.circleMarker(
                [38.9953683, 21.9877132],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1bf43e7c23ab439ebb6237720f944442 = L.popup({"maxWidth": "100%"});

        
            var html_6c0d067aa76746d1aa457c5697199eaa = $(`<div id="html_6c0d067aa76746d1aa457c5697199eaa" style="width: 100.0%; height: 100.0%;">Country : Greece<br>                         Paper : <a href="https://www.mdpi.com/1424-8220/19/22/4837">Deep learning on multi sensor data for counter UAV applications—a systematic review</a><br></div>`)[0];
            popup_1bf43e7c23ab439ebb6237720f944442.setContent(html_6c0d067aa76746d1aa457c5697199eaa);
        

        circle_marker_c68fbeae093d4590b21342744b259442.bindPopup(popup_1bf43e7c23ab439ebb6237720f944442)
        ;

        
    
    
            var circle_marker_9cdc69cd9b90408593f238e6dc9c6a38 = L.circleMarker(
                [38.9953683, 21.9877132],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b11ea28c413947bd8cd3e431e308beec = L.popup({"maxWidth": "100%"});

        
            var html_c7da1eb76db643499fe8176460a136bc = $(`<div id="html_c7da1eb76db643499fe8176460a136bc" style="width: 100.0%; height: 100.0%;">Country : Greece<br>                         Paper : <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Tsiami_STAViS_Spatio-Temporal_AudioVisual_Saliency_Network_CVPR_2020_paper.html">STAViS: Spatio-Temporal AudioVisual Saliency Network</a><br></div>`)[0];
            popup_b11ea28c413947bd8cd3e431e308beec.setContent(html_c7da1eb76db643499fe8176460a136bc);
        

        circle_marker_9cdc69cd9b90408593f238e6dc9c6a38.bindPopup(popup_b11ea28c413947bd8cd3e431e308beec)
        ;

        
    
    
            var circle_marker_5595e49c897d4822b236fbc13954b521 = L.circleMarker(
                [55.670249, 10.3333283],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_6ab0e97c6f7e4380b82a404a000c055b = L.popup({"maxWidth": "100%"});

        
            var html_9dff00812eab4271956bd3b89ffdb3a9 = $(`<div id="html_9dff00812eab4271956bd3b89ffdb3a9" style="width: 100.0%; height: 100.0%;">Country : Denmark<br>                         Paper : <a href="https://dora.dmu.ac.uk/handle/2086/15000">Acoustic scene classification: From a hybrid classifier to deep learning</a><br></div>`)[0];
            popup_6ab0e97c6f7e4380b82a404a000c055b.setContent(html_9dff00812eab4271956bd3b89ffdb3a9);
        

        circle_marker_5595e49c897d4822b236fbc13954b521.bindPopup(popup_6ab0e97c6f7e4380b82a404a000c055b)
        ;

        
    
    
            var circle_marker_5c6af030927247da8e780929d908e3e7 = L.circleMarker(
                [55.670249, 10.3333283],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1af2e8aa763a472b8b76d4a27f994cca = L.popup({"maxWidth": "100%"});

        
            var html_e3b73024fb0e473eb8016455c87a794c = $(`<div id="html_e3b73024fb0e473eb8016455c87a794c" style="width: 100.0%; height: 100.0%;">Country : Denmark<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11257-019-09231-w">Trends in content-based recommendation</a><br></div>`)[0];
            popup_1af2e8aa763a472b8b76d4a27f994cca.setContent(html_e3b73024fb0e473eb8016455c87a794c);
        

        circle_marker_5c6af030927247da8e780929d908e3e7.bindPopup(popup_1af2e8aa763a472b8b76d4a27f994cca)
        ;

        
    
    
            var circle_marker_f885c18a4a28458ebe1c6ac4979c19ab = L.circleMarker(
                [52.5001698, 5.7480821],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_028d35fa7192496b9a31d487a60d95e0 = L.popup({"maxWidth": "100%"});

        
            var html_924bea3fb9d54de395544fb2c97373d4 = $(`<div id="html_924bea3fb9d54de395544fb2c97373d4" style="width: 100.0%; height: 100.0%;">Country : Netherlands<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8553184/">Acoustic scene classification from few examples</a><br></div>`)[0];
            popup_028d35fa7192496b9a31d487a60d95e0.setContent(html_924bea3fb9d54de395544fb2c97373d4);
        

        circle_marker_f885c18a4a28458ebe1c6ac4979c19ab.bindPopup(popup_028d35fa7192496b9a31d487a60d95e0)
        ;

        
    
    
            var circle_marker_0531c9b4135e49d2936ed2704fa3274e = L.circleMarker(
                [52.5001698, 5.7480821],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_e6b47d2df3ec4ce4b2cbf70d1b6e6a09 = L.popup({"maxWidth": "100%"});

        
            var html_af077a66ba0c4a53aa8eae39896da1f2 = $(`<div id="html_af077a66ba0c4a53aa8eae39896da1f2" style="width: 100.0%; height: 100.0%;">Country : Netherlands<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/7927482/">DCAR: A discriminative and compact audio representation for audio processing</a><br></div>`)[0];
            popup_e6b47d2df3ec4ce4b2cbf70d1b6e6a09.setContent(html_af077a66ba0c4a53aa8eae39896da1f2);
        

        circle_marker_0531c9b4135e49d2936ed2704fa3274e.bindPopup(popup_e6b47d2df3ec4ce4b2cbf70d1b6e6a09)
        ;

        
    
    
            var circle_marker_f65e7515dcc04494b9b310ebf28e85e7 = L.circleMarker(
                [52.5001698, 5.7480821],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_dbc8c562cf1849acb644b85c7a7e1d55 = L.popup({"maxWidth": "100%"});

        
            var html_44faeb27b4a44a61964aac20c47049f4 = $(`<div id="html_44faeb27b4a44a61964aac20c47049f4" style="width: 100.0%; height: 100.0%;">Country : Netherlands<br>                         Paper : <a href="">K-shot learning of acoustic context</a><br></div>`)[0];
            popup_dbc8c562cf1849acb644b85c7a7e1d55.setContent(html_44faeb27b4a44a61964aac20c47049f4);
        

        circle_marker_f65e7515dcc04494b9b310ebf28e85e7.bindPopup(popup_dbc8c562cf1849acb644b85c7a7e1d55)
        ;

        
    
    
            var circle_marker_b07df2f2b4db4f7884d93273ad5b822b = L.circleMarker(
                [52.5001698, 5.7480821],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cf9f4264e93242c580192aabd9bf6161 = L.popup({"maxWidth": "100%"});

        
            var html_ed980da2d24648fba23148a36af29103 = $(`<div id="html_ed980da2d24648fba23148a36af29103" style="width: 100.0%; height: 100.0%;">Country : Netherlands<br>                         Paper : <a href="">Linking open public datasets for multifaceted music discovery</a><br></div>`)[0];
            popup_cf9f4264e93242c580192aabd9bf6161.setContent(html_ed980da2d24648fba23148a36af29103);
        

        circle_marker_b07df2f2b4db4f7884d93273ad5b822b.bindPopup(popup_cf9f4264e93242c580192aabd9bf6161)
        ;

        
    
    
            var circle_marker_4a49eaa10c614c2eb85183108761759a = L.circleMarker(
                [38.9597594, 34.9249653],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0d005a3905b54a72bc18f7e9b33dc09e = L.popup({"maxWidth": "100%"});

        
            var html_90ed5415f42845658173f3856b24653c = $(`<div id="html_90ed5415f42845658173f3856b24653c" style="width: 100.0%; height: 100.0%;">Country : Turkey<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9052658/">A New Deep CNN Model for Environmental Sound Classification</a><br></div>`)[0];
            popup_0d005a3905b54a72bc18f7e9b33dc09e.setContent(html_90ed5415f42845658173f3856b24653c);
        

        circle_marker_4a49eaa10c614c2eb85183108761759a.bindPopup(popup_0d005a3905b54a72bc18f7e9b33dc09e)
        ;

        
    
    
            var circle_marker_481236aec89a4fe49992b75c030fe830 = L.circleMarker(
                [38.9597594, 34.9249653],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_1bbac1d132794c2ea4c98409b2fb6362 = L.popup({"maxWidth": "100%"});

        
            var html_dc60abbe0f0d4cd9b2e38f69e6b686dd = $(`<div id="html_dc60abbe0f0d4cd9b2e38f69e6b686dd" style="width: 100.0%; height: 100.0%;">Country : Turkey<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s10462-018-9654-y">A review on deep learning for recommender systems: challenges and remedies</a><br></div>`)[0];
            popup_1bbac1d132794c2ea4c98409b2fb6362.setContent(html_dc60abbe0f0d4cd9b2e38f69e6b686dd);
        

        circle_marker_481236aec89a4fe49992b75c030fe830.bindPopup(popup_1bbac1d132794c2ea4c98409b2fb6362)
        ;

        
    
    
            var circle_marker_10d6b06797ce44b499a6eff12a12a22c = L.circleMarker(
                [38.9597594, 34.9249653],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_de8df516f6cb42c785ade61f60dfc62a = L.popup({"maxWidth": "100%"});

        
            var html_ad922601090249e18d6017380e283b32 = $(`<div id="html_ad922601090249e18d6017380e283b32" style="width: 100.0%; height: 100.0%;">Country : Turkey<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8665547/">Acoustic Scene Classification Using Spatial Pyramid Pooling with Convolutional Neural Networks</a><br></div>`)[0];
            popup_de8df516f6cb42c785ade61f60dfc62a.setContent(html_ad922601090249e18d6017380e283b32);
        

        circle_marker_10d6b06797ce44b499a6eff12a12a22c.bindPopup(popup_de8df516f6cb42c785ade61f60dfc62a)
        ;

        
    
    
            var circle_marker_5f1d880d00464c5da83fbdfc07a7fd1b = L.circleMarker(
                [38.9597594, 34.9249653],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_cf123eb9e8c14864872060a103f205b9 = L.popup({"maxWidth": "100%"});

        
            var html_9cad2e449f544502a354fc646bd100ee = $(`<div id="html_9cad2e449f544502a354fc646bd100ee" style="width: 100.0%; height: 100.0%;">Country : Turkey<br>                         Paper : <a href="http://etd.lib.metu.edu.tr/upload/12619947/index.pdf">An Integrated use of vectorial approach with analytical and synthetic approaches: a teaching experiment with eleventh grade students on quadrilaterals</a><br></div>`)[0];
            popup_cf123eb9e8c14864872060a103f205b9.setContent(html_9cad2e449f544502a354fc646bd100ee);
        

        circle_marker_5f1d880d00464c5da83fbdfc07a7fd1b.bindPopup(popup_cf123eb9e8c14864872060a103f205b9)
        ;

        
    
    
            var circle_marker_36539e74d6354929b3d4bab741b7a720 = L.circleMarker(
                [30.3308401, 71.247499],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_feb88f3c146a4761a843785714b4acc9 = L.popup({"maxWidth": "100%"});

        
            var html_0f2318b47b02444fb80a1a4ffa1b6214 = $(`<div id="html_0f2318b47b02444fb80a1a4ffa1b6214" style="width: 100.0%; height: 100.0%;">Country : Pakistan<br>                         Paper : <a href="http://asrjetsjournal.org/index.php/American_Scientific_Journal/article/view/4169">An improved acoustic scene classification method using convolutional neural networks (CNNs)</a><br></div>`)[0];
            popup_feb88f3c146a4761a843785714b4acc9.setContent(html_0f2318b47b02444fb80a1a4ffa1b6214);
        

        circle_marker_36539e74d6354929b3d4bab741b7a720.bindPopup(popup_feb88f3c146a4761a843785714b4acc9)
        ;

        
    
    
            var circle_marker_5187771e93f0470eb805c05d27b23033 = L.circleMarker(
                [30.3308401, 71.247499],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_0338e996af6c43d4a33f377a283267b0 = L.popup({"maxWidth": "100%"});

        
            var html_ab54870ff82945c19a5f21a77273d2f2 = $(`<div id="html_ab54870ff82945c19a5f21a77273d2f2" style="width: 100.0%; height: 100.0%;">Country : Pakistan<br>                         Paper : <a href="http://www.cs.tut.fi/sgn/arg/dcase2017/documents/challenge_technical_reports/DCASE2017_Hussain_166.pdf">Improved acoustic scene classification with DNN and CNN</a><br></div>`)[0];
            popup_0338e996af6c43d4a33f377a283267b0.setContent(html_ab54870ff82945c19a5f21a77273d2f2);
        

        circle_marker_5187771e93f0470eb805c05d27b23033.bindPopup(popup_0338e996af6c43d4a33f377a283267b0)
        ;

        
    
    
            var circle_marker_409cab2fcd164903a0c8fddb79cdbccd = L.circleMarker(
                [49.4871968, 31.2718321],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_32cbbeb0979f4909bf2e7a5265ff2ee1 = L.popup({"maxWidth": "100%"});

        
            var html_31d0798aad174614a6cedacc58f2dcd2 = $(`<div id="html_31d0798aad174614a6cedacc58f2dcd2" style="width: 100.0%; height: 100.0%;">Country : UAE<br>                         Paper : <a href="https://arxiv.org/abs/1811.00936">Acoustic features fusion using attentive multi-channel deep architecture</a><br></div>`)[0];
            popup_32cbbeb0979f4909bf2e7a5265ff2ee1.setContent(html_31d0798aad174614a6cedacc58f2dcd2);
        

        circle_marker_409cab2fcd164903a0c8fddb79cdbccd.bindPopup(popup_32cbbeb0979f4909bf2e7a5265ff2ee1)
        ;

        
    
    
            var circle_marker_7e364f7cbf1b4f1f9b3796d2b1943341 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ba45f3948d214ea5835d066d402469a1 = L.popup({"maxWidth": "100%"});

        
            var html_1de4704ca50a40a7b8f6433aa87d1638 = $(`<div id="html_1de4704ca50a40a7b8f6433aa87d1638" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s00034-019-01094-1">A survey: Neural network-based deep learning for acoustic event detection</a><br></div>`)[0];
            popup_ba45f3948d214ea5835d066d402469a1.setContent(html_1de4704ca50a40a7b8f6433aa87d1638);
        

        circle_marker_7e364f7cbf1b4f1f9b3796d2b1943341.bindPopup(popup_ba45f3948d214ea5835d066d402469a1)
        ;

        
    
    
            var circle_marker_32809a11d07a4691ab73623b8cb40529 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_9b7eb266ea444853aa687cdd9d8bf850 = L.popup({"maxWidth": "100%"});

        
            var html_1e52a70e0b0f4b18870e4a18f9b423b4 = $(`<div id="html_1e52a70e0b0f4b18870e4a18f9b423b4" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9053274/">Acoustic scene classification using deep residual networks with late fusion of separated high and low frequency paths</a><br></div>`)[0];
            popup_9b7eb266ea444853aa687cdd9d8bf850.setContent(html_1e52a70e0b0f4b18870e4a18f9b423b4);
        

        circle_marker_32809a11d07a4691ab73623b8cb40529.bindPopup(popup_9b7eb266ea444853aa687cdd9d8bf850)
        ;

        
    
    
            var circle_marker_85e80943f9c54699908429dbdb5897f8 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_da6ecf8f649a453796de723ea1a3d80c = L.popup({"maxWidth": "100%"});

        
            var html_4e5d93fcee824a468552e4be57ae9581 = $(`<div id="html_4e5d93fcee824a468552e4be57ae9581" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://arxiv.org/abs/1901.06125">Cold-start playlist recommendation with multitask learning</a><br></div>`)[0];
            popup_da6ecf8f649a453796de723ea1a3d80c.setContent(html_4e5d93fcee824a468552e4be57ae9581);
        

        circle_marker_85e80943f9c54699908429dbdb5897f8.bindPopup(popup_da6ecf8f649a453796de723ea1a3d80c)
        ;

        
    
    
            var circle_marker_2fdab7bdebba4a05aaae8053d042f65d = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_42c636f3f72c47cd887c1779d90808ad = L.popup({"maxWidth": "100%"});

        
            var html_b645f84cb7ef4bc18e0b5c50a099f051 = $(`<div id="html_b645f84cb7ef4bc18e0b5c50a099f051" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://link.springer.com/content/pdf/10.1007/s00500-019-04602-2.pdf">Data augmentation using MG-GAN for improved cancer classification on gene expression data</a><br></div>`)[0];
            popup_42c636f3f72c47cd887c1779d90808ad.setContent(html_b645f84cb7ef4bc18e0b5c50a099f051);
        

        circle_marker_2fdab7bdebba4a05aaae8053d042f65d.bindPopup(popup_42c636f3f72c47cd887c1779d90808ad)
        ;

        
    
    
            var circle_marker_b3e928f038a14af8a0fb1e9726df4579 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f7cc4525676c469e9b5b8af1bf2edac9 = L.popup({"maxWidth": "100%"});

        
            var html_1f9938ff63e14684acc1eb92bcb5eeba = $(`<div id="html_1f9938ff63e14684acc1eb92bcb5eeba" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://journals.sagepub.com/doi/abs/10.1177/2059204319893179">Encouraging Attention and Exploration in a Hybrid Recommender System for Libraries of Unfamiliar Music</a><br></div>`)[0];
            popup_f7cc4525676c469e9b5b8af1bf2edac9.setContent(html_1f9938ff63e14684acc1eb92bcb5eeba);
        

        circle_marker_b3e928f038a14af8a0fb1e9726df4579.bindPopup(popup_f7cc4525676c469e9b5b8af1bf2edac9)
        ;

        
    
    
            var circle_marker_5b94a04a8599455c9351cde0d3f2d85f = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b56fa312dd9b4d30af56b059066f0408 = L.popup({"maxWidth": "100%"});

        
            var html_a3cf0a1f0c4a4ffdb5b93bdb7b7c32a6 = $(`<div id="html_a3cf0a1f0c4a4ffdb5b93bdb7b7c32a6" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://openreview.net/forum?id=HJgEe1SKPr">GAN-based Gaussian Mixture Model Responsibility Learning</a><br></div>`)[0];
            popup_b56fa312dd9b4d30af56b059066f0408.setContent(html_a3cf0a1f0c4a4ffdb5b93bdb7b7c32a6);
        

        circle_marker_5b94a04a8599455c9351cde0d3f2d85f.bindPopup(popup_b56fa312dd9b4d30af56b059066f0408)
        ;

        
    
    
            var circle_marker_9dabb5d987ca47658da741eb59592142 = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_50d1dc9669424edeabf49096180f7a38 = L.popup({"maxWidth": "100%"});

        
            var html_baa7c00818804ab29743c8db00dfcb7f = $(`<div id="html_baa7c00818804ab29743c8db00dfcb7f" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9103031/">Sound Event Detection Using Multiple Optimized Kernels</a><br></div>`)[0];
            popup_50d1dc9669424edeabf49096180f7a38.setContent(html_baa7c00818804ab29743c8db00dfcb7f);
        

        circle_marker_9dabb5d987ca47658da741eb59592142.bindPopup(popup_50d1dc9669424edeabf49096180f7a38)
        ;

        
    
    
            var circle_marker_2a537a264af44f708a30b85b53d3f51c = L.circleMarker(
                [-24.7761086, 134.755],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_214a161639ac49a6b51b493478ab9a13 = L.popup({"maxWidth": "100%"});

        
            var html_61683dab8e6148e083562ff0c450f381 = $(`<div id="html_61683dab8e6148e083562ff0c450f381" style="width: 100.0%; height: 100.0%;">Country : Australia<br>                         Paper : <a href="https://www.ingentaconnect.com/contentone/ince/ncej/2020/00000068/00000004/art00004">Squeak and rattle noise classification using radial basis function neural networks</a><br></div>`)[0];
            popup_214a161639ac49a6b51b493478ab9a13.setContent(html_61683dab8e6148e083562ff0c450f381);
        

        circle_marker_2a537a264af44f708a30b85b53d3f51c.bindPopup(popup_214a161639ac49a6b51b493478ab9a13)
        ;

        
    
    
            var circle_marker_a2ac04e6755a48f4b06a94bf9f5a2619 = L.circleMarker(
                [24.4768783, 90.2932426],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_3f5d309c84c148018c3e62a3aa761b08 = L.popup({"maxWidth": "100%"});

        
            var html_d1349abe686b4c1a8f540c8e707c2a6f = $(`<div id="html_d1349abe686b4c1a8f540c8e707c2a6f" style="width: 100.0%; height: 100.0%;">Country : Bangladesh<br>                         Paper : <a href="https://arxiv.org/abs/1811.05540">Native Language Identification using i-vector</a><br></div>`)[0];
            popup_3f5d309c84c148018c3e62a3aa761b08.setContent(html_d1349abe686b4c1a8f540c8e707c2a6f);
        

        circle_marker_a2ac04e6755a48f4b06a94bf9f5a2619.bindPopup(popup_3f5d309c84c148018c3e62a3aa761b08)
        ;

        
    
    
            var circle_marker_18df24c9ce954addb67ee119ce1ffead = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f4101855f630489b91f080efb8761456 = L.popup({"maxWidth": "100%"});

        
            var html_6ae3fa478be44bd6a1a56d293fdbbb5b = $(`<div id="html_6ae3fa478be44bd6a1a56d293fdbbb5b" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://arxiv.org/abs/1904.11914">I-vector Based Features Embedding for Heart Sound Classification</a><br></div>`)[0];
            popup_f4101855f630489b91f080efb8761456.setContent(html_6ae3fa478be44bd6a1a56d293fdbbb5b);
        

        circle_marker_18df24c9ce954addb67ee119ce1ffead.bindPopup(popup_f4101855f630489b91f080efb8761456)
        ;

        
    
    
            var circle_marker_84aabb8bd3f44db6a9fa240a65df3720 = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f50ebc39e78c4b408d38e038a8d0079f = L.popup({"maxWidth": "100%"});

        
            var html_a3dfef183330427bb483d3fa79622e68 = $(`<div id="html_a3dfef183330427bb483d3fa79622e68" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8877452/">Movie genome recommender: A novel recommender system based on multimedia content</a><br></div>`)[0];
            popup_f50ebc39e78c4b408d38e038a8d0079f.setContent(html_a3dfef183330427bb483d3fa79622e68);
        

        circle_marker_84aabb8bd3f44db6a9fa240a65df3720.bindPopup(popup_f50ebc39e78c4b408d38e038a8d0079f)
        ;

        
    
    
            var circle_marker_b63a2d33a0334540b0586bd3f770f1f3 = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ab26f88637d547e787bc5218e9201237 = L.popup({"maxWidth": "100%"});

        
            var html_15f02a4d58014626be859c54437aad25 = $(`<div id="html_15f02a4d58014626be859c54437aad25" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3301275.3302266">Prediction of music pairwise preferences from facial expressions</a><br></div>`)[0];
            popup_ab26f88637d547e787bc5218e9201237.setContent(html_15f02a4d58014626be859c54437aad25);
        

        circle_marker_b63a2d33a0334540b0586bd3f770f1f3.bindPopup(popup_ab26f88637d547e787bc5218e9201237)
        ;

        
    
    
            var circle_marker_689f6c6f0c014e66ab12fc8d1ca0bd42 = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f1ec3cd78fba49c8a7745a173a5faacb = L.popup({"maxWidth": "100%"});

        
            var html_85c867f61f5b4b7fbd8dcf61e3f4141d = $(`<div id="html_85c867f61f5b4b7fbd8dcf61e3f4141d" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://www.researchgate.net/profile/Mohammad_Adiban3/publication/336312145_Statistical_Feature_Embedding_for_Heart_Sound_Classification/links/5d9b32da92851c2f70f2b9ca/Statistical-Feature-Embedding-for-Heart-Sound-Classification.pdf">Statistical based features embedding for heart sound classification</a><br></div>`)[0];
            popup_f1ec3cd78fba49c8a7745a173a5faacb.setContent(html_85c867f61f5b4b7fbd8dcf61e3f4141d);
        

        circle_marker_689f6c6f0c014e66ab12fc8d1ca0bd42.bindPopup(popup_f1ec3cd78fba49c8a7745a173a5faacb)
        ;

        
    
    
            var circle_marker_4f3d0e4168f7477b8d30649dff4516ab = L.circleMarker(
                [64.5731537, 11.52803643954819],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_4347159ff66240f38a4cfc02a9f83d7a = L.popup({"maxWidth": "100%"});

        
            var html_60498aa4d3c14184a989bf7233716b95 = $(`<div id="html_60498aa4d3c14184a989bf7233716b95" style="width: 100.0%; height: 100.0%;">Country : Norway<br>                         Paper : <a href="https://content.sciendo.com/view/journals/jee/70/4/article-p259.xml">Statistical feature embedding for heart sound classification</a><br></div>`)[0];
            popup_4347159ff66240f38a4cfc02a9f83d7a.setContent(html_60498aa4d3c14184a989bf7233716b95);
        

        circle_marker_4f3d0e4168f7477b8d30649dff4516ab.bindPopup(popup_4347159ff66240f38a4cfc02a9f83d7a)
        ;

        
    
    
            var circle_marker_b8f0b1d08df04d4787fab92020eed64c = L.circleMarker(
                [40.0332629, -7.8896263],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_d5291ff15fa4416389a763d3c872f36f = L.popup({"maxWidth": "100%"});

        
            var html_6c17b51a413c46b2afce1efe6ffd3afe = $(`<div id="html_6c17b51a413c46b2afce1efe6ffd3afe" style="width: 100.0%; height: 100.0%;">Country : Portugal<br>                         Paper : <a href="http://www.journals.isel.pt/index.php/i-ETC/article/view/35">Automatic Acoustic Scene Classification</a><br></div>`)[0];
            popup_d5291ff15fa4416389a763d3c872f36f.setContent(html_6c17b51a413c46b2afce1efe6ffd3afe);
        

        circle_marker_b8f0b1d08df04d4787fab92020eed64c.bindPopup(popup_d5291ff15fa4416389a763d3c872f36f)
        ;

        
    
    
            var circle_marker_08b33f97c66d4e92b71a9fe1d3e3b83b = L.circleMarker(
                [-10.3333333, -53.2],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_b82d6cd0109a46f4a9cdcd233028029d = L.popup({"maxWidth": "100%"});

        
            var html_58c3a50e0a4a4677926ca1790d63e7f5 = $(`<div id="html_58c3a50e0a4a4677926ca1790d63e7f5" style="width: 100.0%; height: 100.0%;">Country : Brazil<br>                         Paper : <a href="https://www.soundeffects.dk/article/view/115027">An acoustemology of streaming media and information and communication technologies</a><br></div>`)[0];
            popup_b82d6cd0109a46f4a9cdcd233028029d.setContent(html_58c3a50e0a4a4677926ca1790d63e7f5);
        

        circle_marker_08b33f97c66d4e92b71a9fe1d3e3b83b.bindPopup(popup_b82d6cd0109a46f4a9cdcd233028029d)
        ;

        
    
    
            var circle_marker_38988b3fae3840d8beff7e7d4774689f = L.circleMarker(
                [-10.3333333, -53.2],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_7f44915977e8480a897c1830b6527714 = L.popup({"maxWidth": "100%"});

        
            var html_ac3760641d4347e0b3af30f76e70526e = $(`<div id="html_ac3760641d4347e0b3af30f76e70526e" style="width: 100.0%; height: 100.0%;">Country : Brazil<br>                         Paper : <a href="https://sol.sbc.org.br/index.php/sbcup/article/view/11227">Arquitetura embarcável para detecção de eventos sonoros utilizando inteligência artificial</a><br></div>`)[0];
            popup_7f44915977e8480a897c1830b6527714.setContent(html_ac3760641d4347e0b3af30f76e70526e);
        

        circle_marker_38988b3fae3840d8beff7e7d4774689f.bindPopup(popup_7f44915977e8480a897c1830b6527714)
        ;

        
    
    
            var circle_marker_5dabba65798543369db3feff5d37283d = L.circleMarker(
                [26.2540493, 29.2675469],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_ffe2b4f817cb4811a7775588eb392fa5 = L.popup({"maxWidth": "100%"});

        
            var html_047634612d864f3da447657a670b1611 = $(`<div id="html_047634612d864f3da447657a670b1611" style="width: 100.0%; height: 100.0%;">Country : Egypt<br>                         Paper : <a href="https://link.springer.com/chapter/10.1007/978-3-030-44289-7_51">Improving the Data Quality of the MovieLens Dataset Using Dimensionality Reduction Techniques</a><br></div>`)[0];
            popup_ffe2b4f817cb4811a7775588eb392fa5.setContent(html_047634612d864f3da447657a670b1611);
        

        circle_marker_5dabba65798543369db3feff5d37283d.bindPopup(popup_ffe2b4f817cb4811a7775588eb392fa5)
        ;

        
    
    
            var circle_marker_da666494610d4aea9e01b55f5d3ff6ae = L.circleMarker(
                [52.865196, -7.9794599],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_5fbf52b1f5654d33962d27fd768a0617 = L.popup({"maxWidth": "100%"});

        
            var html_ad746d516daf41cea4573bcc11feab8b = $(`<div id="html_ad746d516daf41cea4573bcc11feab8b" style="width: 100.0%; height: 100.0%;">Country : Ireland<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3359555.3359563">Leveraging laziness, browsing-pattern aware stacked models for sequential accommodation learning to rank</a><br></div>`)[0];
            popup_5fbf52b1f5654d33962d27fd768a0617.setContent(html_ad746d516daf41cea4573bcc11feab8b);
        

        circle_marker_da666494610d4aea9e01b55f5d3ff6ae.bindPopup(popup_5fbf52b1f5654d33962d27fd768a0617)
        ;

        
    
    
            var circle_marker_45b1b9a39a2745758d3846db8933d399 = L.circleMarker(
                [9.6000359, 7.9999721],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_c5db6e09c22f4d65b9922e968a15b37f = L.popup({"maxWidth": "100%"});

        
            var html_7cd42a1405df4d01b424d84c3de3bd7f = $(`<div id="html_7cd42a1405df4d01b424d84c3de3bd7f" style="width: 100.0%; height: 100.0%;">Country : Nigeria<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11192-020-03642-y">Research paper recommender system based on public contextual metadata</a><br></div>`)[0];
            popup_c5db6e09c22f4d65b9922e968a15b37f.setContent(html_7cd42a1405df4d01b424d84c3de3bd7f);
        

        circle_marker_45b1b9a39a2745758d3846db8933d399.bindPopup(popup_c5db6e09c22f4d65b9922e968a15b37f)
        ;

        
    
    
            var circle_marker_c0bf6afe0b4b40629778df9a6a1f4737 = L.circleMarker(
                [4.5693754, 102.2656823],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8b21c3490b1f4326834ee1ba009fbef8 = L.popup({"maxWidth": "100%"});

        
            var html_b12c4ac9999e462c94d36e3b563c0594 = $(`<div id="html_b12c4ac9999e462c94d36e3b563c0594" style="width: 100.0%; height: 100.0%;">Country : Malaysia<br>                         Paper : <a href="https://link.springer.com/article/10.1007/s11192-020-03642-y">Research paper recommender system based on public contextual metadata</a><br></div>`)[0];
            popup_8b21c3490b1f4326834ee1ba009fbef8.setContent(html_b12c4ac9999e462c94d36e3b563c0594);
        

        circle_marker_c0bf6afe0b4b40629778df9a6a1f4737.bindPopup(popup_8b21c3490b1f4326834ee1ba009fbef8)
        ;

        
    
    
            var circle_marker_e2f6cd7607c64c9393d298ccb551e39d = L.circleMarker(
                [25.6242618, 42.3528328],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_39ca8a20f99a4d3da79b50614fde8d4e = L.popup({"maxWidth": "100%"});

        
            var html_3cf0637a7d1246f887f760b3abca89cc = $(`<div id="html_3cf0637a7d1246f887f760b3abca89cc" style="width: 100.0%; height: 100.0%;">Country : Saudi Arabia<br>                         Paper : <a href="https://www.mdpi.com/2076-3417/9/22/4866">Aroma Release of Olfactory Displays Based on Audio-Visual Content</a><br></div>`)[0];
            popup_39ca8a20f99a4d3da79b50614fde8d4e.setContent(html_3cf0637a7d1246f887f760b3abca89cc);
        

        circle_marker_e2f6cd7607c64c9393d298ccb551e39d.bindPopup(popup_39ca8a20f99a4d3da79b50614fde8d4e)
        ;

        
    
    
            var circle_marker_df0566819371438ab7aaf079713e0074 = L.circleMarker(
                [33.0955793, 44.1749775],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_f73ba854ab964dcbaa04fb4a5b0bbf97 = L.popup({"maxWidth": "100%"});

        
            var html_06bc159af511435c9cc389838f8d8a42 = $(`<div id="html_06bc159af511435c9cc389838f8d8a42" style="width: 100.0%; height: 100.0%;">Country : Iraq<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/9052658/">A New Deep CNN Model for Environmental Sound Classification</a><br></div>`)[0];
            popup_f73ba854ab964dcbaa04fb4a5b0bbf97.setContent(html_06bc159af511435c9cc389838f8d8a42);
        

        circle_marker_df0566819371438ab7aaf079713e0074.bindPopup(popup_f73ba854ab964dcbaa04fb4a5b0bbf97)
        ;

        
    
    
            var circle_marker_0d8ccb2bb30347dfbd7d942558ceb38e = L.circleMarker(
                [45.8133113, 14.4808369],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_954a1932153d4e0c84045be9d236f68b = L.popup({"maxWidth": "100%"});

        
            var html_77478ccf7d534547b104229bafb20bc7 = $(`<div id="html_77478ccf7d534547b104229bafb20bc7" style="width: 100.0%; height: 100.0%;">Country : Slovenia<br>                         Paper : <a href="https://dl.acm.org/doi/abs/10.1145/3301275.3302266">Prediction of music pairwise preferences from facial expressions</a><br></div>`)[0];
            popup_954a1932153d4e0c84045be9d236f68b.setContent(html_77478ccf7d534547b104229bafb20bc7);
        

        circle_marker_0d8ccb2bb30347dfbd7d942558ceb38e.bindPopup(popup_954a1932153d4e0c84045be9d236f68b)
        ;

        
    
    
            var circle_marker_f4141c10301044dfade6a2d31f2ce539 = L.circleMarker(
                [59.6749712, 14.5208584],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_8e28c082319a487b9c4d5c5db81bd385 = L.popup({"maxWidth": "100%"});

        
            var html_206e4301e98b4954ac04bcaa2ebebb63 = $(`<div id="html_206e4301e98b4954ac04bcaa2ebebb63" style="width: 100.0%; height: 100.0%;">Country : Sweden<br>                         Paper : <a href="https://books.google.com/books?hl=de&lr=&id=6jvEDwAAQBAJ&oi=fnd&pg=PA223&ots=FUoGquoOv0&sig=3RyQmnd8MD1YU2VZq8Kbbhl_ezU">User Awareness in Music Recommender Systems</a><br></div>`)[0];
            popup_8e28c082319a487b9c4d5c5db81bd385.setContent(html_206e4301e98b4954ac04bcaa2ebebb63);
        

        circle_marker_f4141c10301044dfade6a2d31f2ce539.bindPopup(popup_8e28c082319a487b9c4d5c5db81bd385)
        ;

        
    
    
            var circle_marker_596356dcf24d467c9bdeaca1fda6224b = L.circleMarker(
                [-31.7613365, -71.3187697],
                {"bubblingMouseEvents": true, "color": "#3388ff", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#3388ff", "fillOpacity": 0.2, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(marker_cluster_7a7253add80b4b16a3615265ee4b1de0);
        
    
        var popup_420727227eb54bf484b2a892a1aea427 = L.popup({"maxWidth": "100%"});

        
            var html_9fc93854dc374f29bb3d7b578729ceb3 = $(`<div id="html_9fc93854dc374f29bb3d7b578729ceb3" style="width: 100.0%; height: 100.0%;">Country : Chile<br>                         Paper : <a href="https://ieeexplore.ieee.org/abstract/document/8979409/">Artificial Generation of Partial Discharge Sources Through an Algorithm Based on Deep Convolutional Generative Adversarial Networks</a><br></div>`)[0];
            popup_420727227eb54bf484b2a892a1aea427.setContent(html_9fc93854dc374f29bb3d7b578729ceb3);
        

        circle_marker_596356dcf24d467c9bdeaca1fda6224b.bindPopup(popup_420727227eb54bf484b2a892a1aea427)
        ;

        
    
</script>
